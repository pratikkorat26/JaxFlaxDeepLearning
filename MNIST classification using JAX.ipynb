{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d20e8df8-db15-4098-ab84-c14c69de780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, value_and_grad\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from flax.training import train_state\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e903e05f-0db3-49e9-8046-14a06425b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cae77cda-1cc8-4080-8688-f9441c910dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, buffer_size):\n",
    "    train_ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
    "    test_ds = tfds.load('mnist', split='test', shuffle_files = True)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        image, label = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "        image = tf.cast(image, dtype = tf.float32) / 255.0\n",
    "\n",
    "        return {\"image\": image, \"label\" : label}\n",
    "\n",
    "    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size = batch_size)\n",
    "    train_ds = train_ds.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size = batch_size)\n",
    "    test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a92fe25a-4e8c-4763-bd45-4c7ff78dd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data(batch_size = BATCH_SIZE, buffer_size = BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b6667b34-75bf-4bcb-80b8-f59658e38c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 20:19:41.256568: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "exmp = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccc023-d5d8-45ca-ba5f-34a148b8c898",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4d0861cf-1243-493e-9b6e-b0ba59f6448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClasifier(nn.Module):\n",
    "    in_channels: int\n",
    "    num_classes: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.conv1 = nn.Conv(features = self.in_channels, kernel_size = (3,3), strides = (1,1), padding = \"SAME\")\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "\n",
    "        self.conv2 = nn.Conv(features = self.in_channels * 2, kernel_size = (3,3), strides = (2,2), padding = \"SAME\")\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "\n",
    "        self.out = nn.Dense(features = self.num_classes)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        batch_size, _, _, _ = x.shape\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = nn.relu(x)\n",
    "        x = jnp.mean(x, axis = (1, 2))\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a8ac1dab-2e83-45da-bd39-f12e5a47e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClasifier(in_channels = 32, num_classes = 10)\n",
    "rng_key = jax.random.key(10)\n",
    "variables = model.init(rngs = rng_key, x = exmp[\"image\"], train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a38cd8c9-1a63-4f68-967d-32456fed09c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                              CNNClasifier Summary                              \u001b[0m\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│       │ CNNClasifier │ train: True   │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │               │\n",
      "│       │              │ x:            │               │       │               │\n",
      "│       │              │ \"<tf.Tensor:  │               │       │               │\n",
      "│       │              │ shape=(128,   │               │       │               │\n",
      "│       │              │ 28, 28, 1),   │               │       │               │\n",
      "│       │              │ dtype=float3… │               │       │               │\n",
      "│       │              │ numpy=\\narra… │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n \\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ [0.],\\n   │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ ...,\\n    │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n    \\ │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\    │               │       │               │\n",
      "│       │              │   \\n\\n        │               │       │               │\n",
      "│       │              │ ...,\\n\\n\\n    │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n\\    │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ [0.],\\n   │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]]]],      │               │       │               │\n",
      "│       │              │ dtype=float3… │               │       │               │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│ conv1 │ Conv         │ \"<tf.Tensor:  │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │ bias:         │\n",
      "│       │              │ shape=(128,   │               │       │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│       │              │ 28, 28, 1),   │               │       │ kernel:       │\n",
      "│       │              │ dtype=float3… │               │       │ \u001b[2mfloat32\u001b[0m[3,3,… │\n",
      "│       │              │ numpy=\\narra… │               │       │               │\n",
      "│       │              │ \\             │               │       │ \u001b[1m320 \u001b[0m\u001b[1;2m(1.3 KB)\u001b[0m  │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n    \\  │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ [0.],\\n   │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ ...,\\n    │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n    \\ │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\    │               │       │               │\n",
      "│       │              │   \\n\\n        │               │       │               │\n",
      "│       │              │ ...,\\n\\n\\n    │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n\\    │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ ...,\\n\\n      │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\     │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]]],\\n\\n\\n │               │       │               │\n",
      "│       │              │ [[[0.],\\n     │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\ [0.],\\n   │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ \\             │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n   \\   │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ ...,\\n\\       │               │       │               │\n",
      "│       │              │   \\n          │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n      │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.]],\\n\\n    │               │       │               │\n",
      "│       │              │ [[0.],\\n\\     │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ ...,\\n        │               │       │               │\n",
      "│       │              │ [0.],\\n       │               │       │               │\n",
      "│       │              │ [0.],\\n\\      │               │       │               │\n",
      "│       │              │   \\           │               │       │               │\n",
      "│       │              │ [0.]]]],      │               │       │               │\n",
      "│       │              │ dtype=float3… │               │       │               │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│ norm1 │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[128,… │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │ bias:         │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│       │              │               │               │       │ scale:        │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│       │              │               │               │       │               │\n",
      "│       │              │               │               │       │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│ conv2 │ Conv         │ \u001b[2mfloat32\u001b[0m[128,… │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │ bias:         │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│       │              │               │               │       │ kernel:       │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[3,3,… │\n",
      "│       │              │               │               │       │               │\n",
      "│       │              │               │               │       │ \u001b[1m18,496 \u001b[0m\u001b[1;2m(74.0 \u001b[0m │\n",
      "│       │              │               │               │       │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│ norm2 │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[128,… │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │ bias:         │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│       │              │               │               │       │ scale:        │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│       │              │               │               │       │               │\n",
      "│       │              │               │               │       │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│ out   │ Dense        │ \u001b[2mfloat32\u001b[0m[128,… │ \u001b[2mfloat32\u001b[0m[128,… │ 0     │ bias:         │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[10]   │\n",
      "│       │              │               │               │       │ kernel:       │\n",
      "│       │              │               │               │       │ \u001b[2mfloat32\u001b[0m[64,1… │\n",
      "│       │              │               │               │       │               │\n",
      "│       │              │               │               │       │ \u001b[1m650 \u001b[0m\u001b[1;2m(2.6 KB)\u001b[0m  │\n",
      "├───────┼──────────────┼───────────────┼───────────────┼───────┼───────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mTotal\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m19,658 \u001b[0m\u001b[1;2m(78.6 \u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m       \u001b[0m│\u001b[1m              \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m       \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mKB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────┴──────────────┴───────────────┴───────────────┴───────┴───────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 19,658 \u001b[0m\u001b[1;2m(78.6 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.tabulate(rngs = rng_key, x = exmp[\"image\"], train = True, compute_flops = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bfabcf71-a016-4e70-af98-251cf3d545ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.apply(variables, exmp[\"image\"], train = True)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c6da71c1-0d3c-42af-a66b-e8341ed622a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3528588, dtype=float32)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "53457568-8261-4362-b015-02d0abab1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits : unnormalized probability distribution\n",
    "    labels : one hot encoding of labels \n",
    "    \"\"\"\n",
    "    labels = nn.one_hot(labels, num_classes = 10)\n",
    "    loss = optax.losses.softmax_cross_entropy(logits = logits, labels = labels)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    logits : unnormalized probability distribution\n",
    "    labels : one hot encoding of labels \n",
    "    \"\"\"\n",
    "\n",
    "    loss = cross_entropy_loss(logits, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    \n",
    "    return {'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "051a6122-18b5-427f-99c3-5a56bb37bf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3528588, dtype=float32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(out, exmp[\"label\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "00a27392-1463-4425-95d8-b815e89f09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is the time to create train state for the model training\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Dict\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "tx = optax.lamb(learning_rate = LEARNING_RATE)\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=tx,\n",
    "    batch_stats=variables.get('batch_stats', {})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d1c85fce-f3d9-43bb-bd10-d6a582232d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing train step\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, batch[\"image\"], train = True)\n",
    "        loss = cross_entropy_loss(logits, batch[\"label\"])\n",
    "        \n",
    "        return loss, logits\n",
    "        \n",
    "    grad_fn = value_and_grad(loss_fn, has_aux = True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    \n",
    "    state = state.apply_gradients(grads = grads)\n",
    "    \n",
    "    metrics = compute_metrics(logits, batch[\"label\"])\n",
    "\n",
    "    return state, loss, metrics\n",
    "\n",
    "def train_model(state, train_ds, test_ds, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "        tr_acc = 0\n",
    "        for i, batch in enumerate(train_ds):\n",
    "            image = batch[\"image\"].numpy()\n",
    "            label = batch[\"label\"].numpy()\n",
    "            \n",
    "            batch = {\n",
    "                \"image\" : jnp.array(image),\n",
    "                \"label\" : jnp.array(label)\n",
    "                 \n",
    "            }\n",
    "            state, loss, metrics = train_step(state, batch)\n",
    "            tr_loss += loss\n",
    "            tr_acc += metrics[\"accuracy\"]\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i}, Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        tr_loss = tr_loss / len(train_ds)\n",
    "        tr_acc = tr_acc / len(train_ds)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Train Accuracy: {tr_acc:.4f}, Test Loss: {tr_loss:.04f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        vl_loss = 0\n",
    "        vl_acc = 0\n",
    "        for batch in test_ds:\n",
    "            image = batch[\"image\"].numpy()\n",
    "            label = batch[\"label\"].numpy()\n",
    "            \n",
    "            batch = {\n",
    "                \"image\" : jnp.array(image),\n",
    "                \"label\" : jnp.array(label)\n",
    "                 \n",
    "            }\n",
    "            val_logits = model.apply({'params': state.params}, batch['image'], True)\n",
    "            val_metrics = compute_metrics(val_logits, batch['label'])\n",
    "            \n",
    "            vl_loss += val_metrics[\"loss\"]\n",
    "            vl_acc += val_metrics[\"accuracy\"]\n",
    "            \n",
    "        vl_loss = vl_loss / len(test_ds)\n",
    "        vl_acc = vl_acc / len(test_ds)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Validation Accuracy: {vl_acc:.4f}, Validation Loss: {vl_loss:.04f}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "17719edc-cfd8-4e6c-9508-e7ec5ecaa3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 2.3794, Accuracy: 0.0938\n",
      "Epoch 1, Batch 10, Loss: 2.3296, Accuracy: 0.0781\n",
      "Epoch 1, Batch 20, Loss: 2.2975, Accuracy: 0.1484\n",
      "Epoch 1, Batch 30, Loss: 2.2882, Accuracy: 0.1953\n",
      "Epoch 1, Batch 40, Loss: 2.2655, Accuracy: 0.2109\n",
      "Epoch 1, Batch 50, Loss: 2.2719, Accuracy: 0.1250\n",
      "Epoch 1, Batch 60, Loss: 2.2521, Accuracy: 0.2031\n",
      "Epoch 1, Batch 70, Loss: 2.2256, Accuracy: 0.2266\n",
      "Epoch 1, Batch 80, Loss: 2.2304, Accuracy: 0.1719\n",
      "Epoch 1, Batch 90, Loss: 2.2160, Accuracy: 0.1875\n",
      "Epoch 1, Batch 100, Loss: 2.2029, Accuracy: 0.2422\n",
      "Epoch 1, Batch 110, Loss: 2.2235, Accuracy: 0.2031\n",
      "Epoch 1, Batch 120, Loss: 2.2384, Accuracy: 0.1172\n",
      "Epoch 1, Batch 130, Loss: 2.1823, Accuracy: 0.2188\n",
      "Epoch 1, Batch 140, Loss: 2.1589, Accuracy: 0.2031\n",
      "Epoch 1, Batch 150, Loss: 2.1431, Accuracy: 0.2031\n",
      "Epoch 1, Batch 160, Loss: 2.1471, Accuracy: 0.2656\n",
      "Epoch 1, Batch 170, Loss: 2.1018, Accuracy: 0.2344\n",
      "Epoch 1, Batch 180, Loss: 2.1376, Accuracy: 0.1953\n",
      "Epoch 1, Batch 190, Loss: 2.1133, Accuracy: 0.2031\n",
      "Epoch 1, Batch 200, Loss: 2.1444, Accuracy: 0.2031\n",
      "Epoch 1, Batch 210, Loss: 2.1359, Accuracy: 0.2266\n",
      "Epoch 1, Batch 220, Loss: 2.1079, Accuracy: 0.2656\n",
      "Epoch 1, Batch 230, Loss: 2.0607, Accuracy: 0.2891\n",
      "Epoch 1, Batch 240, Loss: 2.0783, Accuracy: 0.2422\n",
      "Epoch 1, Batch 250, Loss: 2.0196, Accuracy: 0.2656\n",
      "Epoch 1, Batch 260, Loss: 2.0381, Accuracy: 0.2734\n",
      "Epoch 1, Batch 270, Loss: 1.9962, Accuracy: 0.3516\n",
      "Epoch 1, Batch 280, Loss: 2.0384, Accuracy: 0.2891\n",
      "Epoch 1, Batch 290, Loss: 2.0883, Accuracy: 0.2109\n",
      "Epoch 1, Batch 300, Loss: 2.0306, Accuracy: 0.2812\n",
      "Epoch 1, Batch 310, Loss: 2.0550, Accuracy: 0.2266\n",
      "Epoch 1, Batch 320, Loss: 2.0493, Accuracy: 0.2656\n",
      "Epoch 1, Batch 330, Loss: 2.0061, Accuracy: 0.2812\n",
      "Epoch 1, Batch 340, Loss: 1.9977, Accuracy: 0.3203\n",
      "Epoch 1, Batch 350, Loss: 1.9865, Accuracy: 0.3047\n",
      "Epoch 1, Batch 360, Loss: 2.0191, Accuracy: 0.3047\n",
      "Epoch 1, Batch 370, Loss: 1.9683, Accuracy: 0.3125\n",
      "Epoch 1, Batch 380, Loss: 2.0006, Accuracy: 0.2891\n",
      "Epoch 1, Batch 390, Loss: 2.0142, Accuracy: 0.2578\n",
      "Epoch 1, Batch 400, Loss: 1.9608, Accuracy: 0.3203\n",
      "Epoch 1, Batch 410, Loss: 1.9466, Accuracy: 0.3203\n",
      "Epoch 1, Batch 420, Loss: 2.0098, Accuracy: 0.2969\n",
      "Epoch 1, Batch 430, Loss: 1.9300, Accuracy: 0.2891\n",
      "Epoch 1, Batch 440, Loss: 2.0042, Accuracy: 0.3203\n",
      "Epoch 1, Batch 450, Loss: 1.9120, Accuracy: 0.3516\n",
      "Epoch 1, Batch 460, Loss: 1.9067, Accuracy: 0.3125\n",
      "Epoch 1, Train Accuracy: 0.2545, Test Loss: 2.0928\n",
      "Epoch 1, Validation Accuracy: 0.3522, Validation Loss: 1.8965\n",
      "Epoch 2, Batch 0, Loss: 1.8385, Accuracy: 0.3828\n",
      "Epoch 2, Batch 10, Loss: 1.8464, Accuracy: 0.3828\n",
      "Epoch 2, Batch 20, Loss: 1.8882, Accuracy: 0.3281\n",
      "Epoch 2, Batch 30, Loss: 1.9588, Accuracy: 0.2656\n",
      "Epoch 2, Batch 40, Loss: 1.8668, Accuracy: 0.3672\n",
      "Epoch 2, Batch 50, Loss: 1.8989, Accuracy: 0.3359\n",
      "Epoch 2, Batch 60, Loss: 1.8950, Accuracy: 0.3359\n",
      "Epoch 2, Batch 70, Loss: 1.8676, Accuracy: 0.3516\n",
      "Epoch 2, Batch 80, Loss: 1.8628, Accuracy: 0.3359\n",
      "Epoch 2, Batch 90, Loss: 1.8478, Accuracy: 0.4062\n",
      "Epoch 2, Batch 100, Loss: 1.8297, Accuracy: 0.3516\n",
      "Epoch 2, Batch 110, Loss: 1.8067, Accuracy: 0.3828\n",
      "Epoch 2, Batch 120, Loss: 1.7833, Accuracy: 0.3750\n",
      "Epoch 2, Batch 130, Loss: 1.7353, Accuracy: 0.4531\n",
      "Epoch 2, Batch 140, Loss: 1.8046, Accuracy: 0.4141\n",
      "Epoch 2, Batch 150, Loss: 1.7444, Accuracy: 0.4062\n",
      "Epoch 2, Batch 160, Loss: 1.7014, Accuracy: 0.4609\n",
      "Epoch 2, Batch 170, Loss: 1.7139, Accuracy: 0.4219\n",
      "Epoch 2, Batch 180, Loss: 1.7118, Accuracy: 0.4922\n",
      "Epoch 2, Batch 190, Loss: 1.7050, Accuracy: 0.4688\n",
      "Epoch 2, Batch 200, Loss: 1.6577, Accuracy: 0.4688\n",
      "Epoch 2, Batch 210, Loss: 1.6753, Accuracy: 0.3984\n",
      "Epoch 2, Batch 220, Loss: 1.6783, Accuracy: 0.3984\n",
      "Epoch 2, Batch 230, Loss: 1.7010, Accuracy: 0.4062\n",
      "Epoch 2, Batch 240, Loss: 1.5920, Accuracy: 0.4844\n",
      "Epoch 2, Batch 250, Loss: 1.6643, Accuracy: 0.4844\n",
      "Epoch 2, Batch 260, Loss: 1.5501, Accuracy: 0.5703\n",
      "Epoch 2, Batch 270, Loss: 1.6414, Accuracy: 0.5078\n",
      "Epoch 2, Batch 280, Loss: 1.7009, Accuracy: 0.3828\n",
      "Epoch 2, Batch 290, Loss: 1.6601, Accuracy: 0.5312\n",
      "Epoch 2, Batch 300, Loss: 1.5577, Accuracy: 0.4531\n",
      "Epoch 2, Batch 310, Loss: 1.6953, Accuracy: 0.4375\n",
      "Epoch 2, Batch 320, Loss: 1.6480, Accuracy: 0.4766\n",
      "Epoch 2, Batch 330, Loss: 1.5331, Accuracy: 0.5312\n",
      "Epoch 2, Batch 340, Loss: 1.5797, Accuracy: 0.4766\n",
      "Epoch 2, Batch 350, Loss: 1.6209, Accuracy: 0.4141\n",
      "Epoch 2, Batch 360, Loss: 1.5260, Accuracy: 0.4531\n",
      "Epoch 2, Batch 370, Loss: 1.6294, Accuracy: 0.4297\n",
      "Epoch 2, Batch 380, Loss: 1.6652, Accuracy: 0.3984\n",
      "Epoch 2, Batch 390, Loss: 1.6297, Accuracy: 0.4922\n",
      "Epoch 2, Batch 400, Loss: 1.5742, Accuracy: 0.4844\n",
      "Epoch 2, Batch 410, Loss: 1.5337, Accuracy: 0.4766\n",
      "Epoch 2, Batch 420, Loss: 1.5473, Accuracy: 0.4609\n",
      "Epoch 2, Batch 430, Loss: 1.4318, Accuracy: 0.5703\n",
      "Epoch 2, Batch 440, Loss: 1.4649, Accuracy: 0.5312\n",
      "Epoch 2, Batch 450, Loss: 1.4395, Accuracy: 0.5312\n",
      "Epoch 2, Batch 460, Loss: 1.4620, Accuracy: 0.5000\n",
      "Epoch 2, Train Accuracy: 0.4227, Test Loss: 1.6961\n",
      "Epoch 2, Validation Accuracy: 0.5182, Validation Loss: 1.4933\n",
      "Epoch 3, Batch 0, Loss: 1.4976, Accuracy: 0.5312\n",
      "Epoch 3, Batch 10, Loss: 1.5290, Accuracy: 0.5312\n",
      "Epoch 3, Batch 20, Loss: 1.4872, Accuracy: 0.4609\n",
      "Epoch 3, Batch 30, Loss: 1.5020, Accuracy: 0.5547\n",
      "Epoch 3, Batch 40, Loss: 1.5222, Accuracy: 0.5312\n",
      "Epoch 3, Batch 50, Loss: 1.5281, Accuracy: 0.4844\n",
      "Epoch 3, Batch 60, Loss: 1.4987, Accuracy: 0.4844\n",
      "Epoch 3, Batch 70, Loss: 1.4490, Accuracy: 0.5000\n",
      "Epoch 3, Batch 80, Loss: 1.5130, Accuracy: 0.5156\n",
      "Epoch 3, Batch 90, Loss: 1.5435, Accuracy: 0.4453\n",
      "Epoch 3, Batch 100, Loss: 1.3829, Accuracy: 0.5391\n",
      "Epoch 3, Batch 110, Loss: 1.5125, Accuracy: 0.4453\n",
      "Epoch 3, Batch 120, Loss: 1.4645, Accuracy: 0.5391\n",
      "Epoch 3, Batch 130, Loss: 1.3529, Accuracy: 0.6094\n",
      "Epoch 3, Batch 140, Loss: 1.4514, Accuracy: 0.5938\n",
      "Epoch 3, Batch 150, Loss: 1.3840, Accuracy: 0.5547\n",
      "Epoch 3, Batch 160, Loss: 1.3594, Accuracy: 0.6328\n",
      "Epoch 3, Batch 170, Loss: 1.4093, Accuracy: 0.5000\n",
      "Epoch 3, Batch 180, Loss: 1.3330, Accuracy: 0.5781\n",
      "Epoch 3, Batch 190, Loss: 1.3880, Accuracy: 0.6016\n",
      "Epoch 3, Batch 200, Loss: 1.3639, Accuracy: 0.5938\n",
      "Epoch 3, Batch 210, Loss: 1.2388, Accuracy: 0.5938\n",
      "Epoch 3, Batch 220, Loss: 1.2938, Accuracy: 0.6172\n",
      "Epoch 3, Batch 230, Loss: 1.3361, Accuracy: 0.5156\n",
      "Epoch 3, Batch 240, Loss: 1.3530, Accuracy: 0.5078\n",
      "Epoch 3, Batch 250, Loss: 1.2283, Accuracy: 0.6250\n",
      "Epoch 3, Batch 260, Loss: 1.3417, Accuracy: 0.5781\n",
      "Epoch 3, Batch 270, Loss: 1.3601, Accuracy: 0.6406\n",
      "Epoch 3, Batch 280, Loss: 1.2991, Accuracy: 0.6016\n",
      "Epoch 3, Batch 290, Loss: 1.2602, Accuracy: 0.6016\n",
      "Epoch 3, Batch 300, Loss: 1.2434, Accuracy: 0.6172\n",
      "Epoch 3, Batch 310, Loss: 1.2733, Accuracy: 0.5625\n",
      "Epoch 3, Batch 320, Loss: 1.2567, Accuracy: 0.6172\n",
      "Epoch 3, Batch 330, Loss: 1.3986, Accuracy: 0.5391\n",
      "Epoch 3, Batch 340, Loss: 1.2924, Accuracy: 0.6094\n",
      "Epoch 3, Batch 350, Loss: 1.2810, Accuracy: 0.5625\n",
      "Epoch 3, Batch 360, Loss: 1.1706, Accuracy: 0.6875\n",
      "Epoch 3, Batch 370, Loss: 1.2665, Accuracy: 0.6250\n",
      "Epoch 3, Batch 380, Loss: 1.2601, Accuracy: 0.6250\n",
      "Epoch 3, Batch 390, Loss: 1.2641, Accuracy: 0.5234\n",
      "Epoch 3, Batch 400, Loss: 1.1232, Accuracy: 0.7031\n",
      "Epoch 3, Batch 410, Loss: 1.3488, Accuracy: 0.5156\n",
      "Epoch 3, Batch 420, Loss: 1.2197, Accuracy: 0.6719\n",
      "Epoch 3, Batch 430, Loss: 1.2574, Accuracy: 0.5625\n",
      "Epoch 3, Batch 440, Loss: 1.0643, Accuracy: 0.7109\n",
      "Epoch 3, Batch 450, Loss: 1.1869, Accuracy: 0.6562\n",
      "Epoch 3, Batch 460, Loss: 1.1668, Accuracy: 0.6094\n",
      "Epoch 3, Train Accuracy: 0.5671, Test Loss: 1.3456\n",
      "Epoch 3, Validation Accuracy: 0.6709, Validation Loss: 1.1424\n",
      "Epoch 4, Batch 0, Loss: 1.1767, Accuracy: 0.6016\n",
      "Epoch 4, Batch 10, Loss: 1.1374, Accuracy: 0.6250\n",
      "Epoch 4, Batch 20, Loss: 1.2573, Accuracy: 0.6172\n",
      "Epoch 4, Batch 30, Loss: 1.2786, Accuracy: 0.5156\n",
      "Epoch 4, Batch 40, Loss: 1.2113, Accuracy: 0.5781\n",
      "Epoch 4, Batch 50, Loss: 1.0382, Accuracy: 0.6953\n",
      "Epoch 4, Batch 60, Loss: 1.0907, Accuracy: 0.6953\n",
      "Epoch 4, Batch 70, Loss: 1.1162, Accuracy: 0.6719\n",
      "Epoch 4, Batch 80, Loss: 1.1365, Accuracy: 0.6797\n",
      "Epoch 4, Batch 90, Loss: 1.0882, Accuracy: 0.6797\n",
      "Epoch 4, Batch 100, Loss: 1.0922, Accuracy: 0.6719\n",
      "Epoch 4, Batch 110, Loss: 1.1717, Accuracy: 0.6406\n",
      "Epoch 4, Batch 120, Loss: 1.0689, Accuracy: 0.6562\n",
      "Epoch 4, Batch 130, Loss: 1.0649, Accuracy: 0.6719\n",
      "Epoch 4, Batch 140, Loss: 1.0586, Accuracy: 0.6797\n",
      "Epoch 4, Batch 150, Loss: 1.1328, Accuracy: 0.6797\n",
      "Epoch 4, Batch 160, Loss: 1.0934, Accuracy: 0.7031\n",
      "Epoch 4, Batch 170, Loss: 0.9763, Accuracy: 0.7344\n",
      "Epoch 4, Batch 180, Loss: 1.0370, Accuracy: 0.7109\n",
      "Epoch 4, Batch 190, Loss: 0.9846, Accuracy: 0.7500\n",
      "Epoch 4, Batch 200, Loss: 0.9409, Accuracy: 0.6875\n",
      "Epoch 4, Batch 210, Loss: 0.9399, Accuracy: 0.7656\n",
      "Epoch 4, Batch 220, Loss: 0.9773, Accuracy: 0.6953\n",
      "Epoch 4, Batch 230, Loss: 1.1479, Accuracy: 0.6953\n",
      "Epoch 4, Batch 240, Loss: 1.0009, Accuracy: 0.6719\n",
      "Epoch 4, Batch 250, Loss: 0.9890, Accuracy: 0.6406\n",
      "Epoch 4, Batch 260, Loss: 1.0645, Accuracy: 0.6172\n",
      "Epoch 4, Batch 270, Loss: 0.8949, Accuracy: 0.7812\n",
      "Epoch 4, Batch 280, Loss: 0.9708, Accuracy: 0.7422\n",
      "Epoch 4, Batch 290, Loss: 0.8718, Accuracy: 0.7109\n",
      "Epoch 4, Batch 300, Loss: 0.9360, Accuracy: 0.6953\n",
      "Epoch 4, Batch 310, Loss: 1.0204, Accuracy: 0.6328\n",
      "Epoch 4, Batch 320, Loss: 1.0179, Accuracy: 0.6797\n",
      "Epoch 4, Batch 330, Loss: 1.0028, Accuracy: 0.7188\n",
      "Epoch 4, Batch 340, Loss: 1.0044, Accuracy: 0.6484\n",
      "Epoch 4, Batch 350, Loss: 0.9768, Accuracy: 0.6953\n",
      "Epoch 4, Batch 360, Loss: 0.8642, Accuracy: 0.7031\n",
      "Epoch 4, Batch 370, Loss: 1.0147, Accuracy: 0.7109\n",
      "Epoch 4, Batch 380, Loss: 0.9050, Accuracy: 0.7578\n",
      "Epoch 4, Batch 390, Loss: 1.0105, Accuracy: 0.7266\n",
      "Epoch 4, Batch 400, Loss: 0.8144, Accuracy: 0.7969\n",
      "Epoch 4, Batch 410, Loss: 1.0213, Accuracy: 0.6641\n",
      "Epoch 4, Batch 420, Loss: 0.9231, Accuracy: 0.6875\n",
      "Epoch 4, Batch 430, Loss: 0.8420, Accuracy: 0.7734\n",
      "Epoch 4, Batch 440, Loss: 0.8321, Accuracy: 0.7812\n",
      "Epoch 4, Batch 450, Loss: 0.8908, Accuracy: 0.6719\n",
      "Epoch 4, Batch 460, Loss: 0.8446, Accuracy: 0.7734\n",
      "Epoch 4, Train Accuracy: 0.6938, Test Loss: 1.0091\n",
      "Epoch 4, Validation Accuracy: 0.7334, Validation Loss: 0.8608\n",
      "Epoch 5, Batch 0, Loss: 0.8922, Accuracy: 0.7109\n",
      "Epoch 5, Batch 10, Loss: 0.7964, Accuracy: 0.7656\n",
      "Epoch 5, Batch 20, Loss: 0.7990, Accuracy: 0.7891\n",
      "Epoch 5, Batch 30, Loss: 0.8357, Accuracy: 0.7422\n",
      "Epoch 5, Batch 40, Loss: 0.8256, Accuracy: 0.7422\n",
      "Epoch 5, Batch 50, Loss: 0.8746, Accuracy: 0.7656\n",
      "Epoch 5, Batch 60, Loss: 0.7868, Accuracy: 0.7500\n",
      "Epoch 5, Batch 70, Loss: 0.8471, Accuracy: 0.7656\n",
      "Epoch 5, Batch 80, Loss: 0.7626, Accuracy: 0.7812\n",
      "Epoch 5, Batch 90, Loss: 0.6933, Accuracy: 0.8203\n",
      "Epoch 5, Batch 100, Loss: 0.8569, Accuracy: 0.7188\n",
      "Epoch 5, Batch 110, Loss: 0.8305, Accuracy: 0.7969\n",
      "Epoch 5, Batch 120, Loss: 0.9251, Accuracy: 0.6875\n",
      "Epoch 5, Batch 130, Loss: 0.8262, Accuracy: 0.7188\n",
      "Epoch 5, Batch 140, Loss: 0.8063, Accuracy: 0.7578\n",
      "Epoch 5, Batch 150, Loss: 0.7570, Accuracy: 0.7891\n",
      "Epoch 5, Batch 160, Loss: 0.9052, Accuracy: 0.7266\n",
      "Epoch 5, Batch 170, Loss: 0.7737, Accuracy: 0.7656\n",
      "Epoch 5, Batch 180, Loss: 0.6505, Accuracy: 0.7891\n",
      "Epoch 5, Batch 190, Loss: 0.7439, Accuracy: 0.7578\n",
      "Epoch 5, Batch 200, Loss: 0.8517, Accuracy: 0.7422\n",
      "Epoch 5, Batch 210, Loss: 0.8801, Accuracy: 0.7656\n",
      "Epoch 5, Batch 220, Loss: 0.8006, Accuracy: 0.7422\n",
      "Epoch 5, Batch 230, Loss: 0.7684, Accuracy: 0.7891\n",
      "Epoch 5, Batch 240, Loss: 0.7017, Accuracy: 0.7969\n",
      "Epoch 5, Batch 250, Loss: 0.7784, Accuracy: 0.7734\n",
      "Epoch 5, Batch 260, Loss: 0.9296, Accuracy: 0.6953\n",
      "Epoch 5, Batch 270, Loss: 0.7657, Accuracy: 0.7500\n",
      "Epoch 5, Batch 280, Loss: 0.7083, Accuracy: 0.8125\n",
      "Epoch 5, Batch 290, Loss: 0.7360, Accuracy: 0.7734\n",
      "Epoch 5, Batch 300, Loss: 0.7017, Accuracy: 0.7969\n",
      "Epoch 5, Batch 310, Loss: 0.7123, Accuracy: 0.7969\n",
      "Epoch 5, Batch 320, Loss: 0.5925, Accuracy: 0.8047\n",
      "Epoch 5, Batch 330, Loss: 0.5408, Accuracy: 0.8359\n",
      "Epoch 5, Batch 340, Loss: 0.8122, Accuracy: 0.7891\n",
      "Epoch 5, Batch 350, Loss: 0.5793, Accuracy: 0.8125\n",
      "Epoch 5, Batch 360, Loss: 0.7496, Accuracy: 0.8047\n",
      "Epoch 5, Batch 370, Loss: 0.6479, Accuracy: 0.8047\n",
      "Epoch 5, Batch 380, Loss: 0.7234, Accuracy: 0.7812\n",
      "Epoch 5, Batch 390, Loss: 0.7580, Accuracy: 0.7344\n",
      "Epoch 5, Batch 400, Loss: 0.7407, Accuracy: 0.7656\n",
      "Epoch 5, Batch 410, Loss: 0.7478, Accuracy: 0.7656\n",
      "Epoch 5, Batch 420, Loss: 0.6696, Accuracy: 0.7734\n",
      "Epoch 5, Batch 430, Loss: 0.7349, Accuracy: 0.7734\n",
      "Epoch 5, Batch 440, Loss: 0.6232, Accuracy: 0.7969\n",
      "Epoch 5, Batch 450, Loss: 0.6800, Accuracy: 0.7578\n",
      "Epoch 5, Batch 460, Loss: 0.7659, Accuracy: 0.7891\n",
      "Epoch 5, Train Accuracy: 0.7746, Test Loss: 0.7556\n",
      "Epoch 5, Validation Accuracy: 0.8029, Validation Loss: 0.6506\n",
      "Epoch 6, Batch 0, Loss: 0.8402, Accuracy: 0.7344\n",
      "Epoch 6, Batch 10, Loss: 0.6285, Accuracy: 0.8125\n",
      "Epoch 6, Batch 20, Loss: 0.6910, Accuracy: 0.7969\n",
      "Epoch 6, Batch 30, Loss: 0.6143, Accuracy: 0.7812\n",
      "Epoch 6, Batch 40, Loss: 0.5963, Accuracy: 0.8516\n",
      "Epoch 6, Batch 50, Loss: 0.5867, Accuracy: 0.8281\n",
      "Epoch 6, Batch 60, Loss: 0.5449, Accuracy: 0.8125\n",
      "Epoch 6, Batch 70, Loss: 0.6126, Accuracy: 0.7812\n",
      "Epoch 6, Batch 80, Loss: 0.6904, Accuracy: 0.7969\n",
      "Epoch 6, Batch 90, Loss: 0.4720, Accuracy: 0.8750\n",
      "Epoch 6, Batch 100, Loss: 0.6124, Accuracy: 0.8828\n",
      "Epoch 6, Batch 110, Loss: 0.6735, Accuracy: 0.8125\n",
      "Epoch 6, Batch 120, Loss: 0.6963, Accuracy: 0.8047\n",
      "Epoch 6, Batch 130, Loss: 0.6015, Accuracy: 0.8203\n",
      "Epoch 6, Batch 140, Loss: 0.6281, Accuracy: 0.8281\n",
      "Epoch 6, Batch 150, Loss: 0.5063, Accuracy: 0.8672\n",
      "Epoch 6, Batch 160, Loss: 0.5587, Accuracy: 0.8047\n",
      "Epoch 6, Batch 170, Loss: 0.6397, Accuracy: 0.8047\n",
      "Epoch 6, Batch 180, Loss: 0.7201, Accuracy: 0.7891\n",
      "Epoch 6, Batch 190, Loss: 0.6156, Accuracy: 0.8281\n",
      "Epoch 6, Batch 200, Loss: 0.4785, Accuracy: 0.8359\n",
      "Epoch 6, Batch 210, Loss: 0.8739, Accuracy: 0.7109\n",
      "Epoch 6, Batch 220, Loss: 0.4856, Accuracy: 0.8594\n",
      "Epoch 6, Batch 230, Loss: 0.6458, Accuracy: 0.8359\n",
      "Epoch 6, Batch 240, Loss: 0.5302, Accuracy: 0.8750\n",
      "Epoch 6, Batch 250, Loss: 0.6118, Accuracy: 0.8281\n",
      "Epoch 6, Batch 260, Loss: 0.5015, Accuracy: 0.8438\n",
      "Epoch 6, Batch 270, Loss: 0.5324, Accuracy: 0.8281\n",
      "Epoch 6, Batch 280, Loss: 0.6265, Accuracy: 0.7422\n",
      "Epoch 6, Batch 290, Loss: 0.6375, Accuracy: 0.7969\n",
      "Epoch 6, Batch 300, Loss: 0.5574, Accuracy: 0.8359\n",
      "Epoch 6, Batch 310, Loss: 0.4824, Accuracy: 0.8828\n",
      "Epoch 6, Batch 320, Loss: 0.6789, Accuracy: 0.7734\n",
      "Epoch 6, Batch 330, Loss: 0.6819, Accuracy: 0.8125\n",
      "Epoch 6, Batch 340, Loss: 0.5585, Accuracy: 0.8203\n",
      "Epoch 6, Batch 350, Loss: 0.5765, Accuracy: 0.8203\n",
      "Epoch 6, Batch 360, Loss: 0.4372, Accuracy: 0.8672\n",
      "Epoch 6, Batch 370, Loss: 0.5469, Accuracy: 0.8203\n",
      "Epoch 6, Batch 380, Loss: 0.5277, Accuracy: 0.8203\n",
      "Epoch 6, Batch 390, Loss: 0.5878, Accuracy: 0.8438\n",
      "Epoch 6, Batch 400, Loss: 0.5769, Accuracy: 0.8125\n",
      "Epoch 6, Batch 410, Loss: 0.4746, Accuracy: 0.8906\n",
      "Epoch 6, Batch 420, Loss: 0.5020, Accuracy: 0.8906\n",
      "Epoch 6, Batch 430, Loss: 0.4971, Accuracy: 0.8281\n",
      "Epoch 6, Batch 440, Loss: 0.4386, Accuracy: 0.8828\n",
      "Epoch 6, Batch 450, Loss: 0.5130, Accuracy: 0.8516\n",
      "Epoch 6, Batch 460, Loss: 0.6581, Accuracy: 0.7344\n",
      "Epoch 6, Train Accuracy: 0.8260, Test Loss: 0.5755\n",
      "Epoch 6, Validation Accuracy: 0.8457, Validation Loss: 0.4967\n",
      "Epoch 7, Batch 0, Loss: 0.4040, Accuracy: 0.9062\n",
      "Epoch 7, Batch 10, Loss: 0.5404, Accuracy: 0.7969\n",
      "Epoch 7, Batch 20, Loss: 0.5190, Accuracy: 0.8359\n",
      "Epoch 7, Batch 30, Loss: 0.4602, Accuracy: 0.8047\n",
      "Epoch 7, Batch 40, Loss: 0.3432, Accuracy: 0.8750\n",
      "Epoch 7, Batch 50, Loss: 0.4204, Accuracy: 0.8750\n",
      "Epoch 7, Batch 60, Loss: 0.5874, Accuracy: 0.8125\n",
      "Epoch 7, Batch 70, Loss: 0.4107, Accuracy: 0.8906\n",
      "Epoch 7, Batch 80, Loss: 0.5629, Accuracy: 0.8438\n",
      "Epoch 7, Batch 90, Loss: 0.4621, Accuracy: 0.8906\n",
      "Epoch 7, Batch 100, Loss: 0.6306, Accuracy: 0.8281\n",
      "Epoch 7, Batch 110, Loss: 0.5431, Accuracy: 0.7969\n",
      "Epoch 7, Batch 120, Loss: 0.6364, Accuracy: 0.8359\n",
      "Epoch 7, Batch 130, Loss: 0.5275, Accuracy: 0.8438\n",
      "Epoch 7, Batch 140, Loss: 0.4836, Accuracy: 0.8672\n",
      "Epoch 7, Batch 150, Loss: 0.4612, Accuracy: 0.8594\n",
      "Epoch 7, Batch 160, Loss: 0.3262, Accuracy: 0.9141\n",
      "Epoch 7, Batch 170, Loss: 0.4184, Accuracy: 0.8984\n",
      "Epoch 7, Batch 180, Loss: 0.5564, Accuracy: 0.7891\n",
      "Epoch 7, Batch 190, Loss: 0.4942, Accuracy: 0.8672\n",
      "Epoch 7, Batch 200, Loss: 0.3610, Accuracy: 0.8906\n",
      "Epoch 7, Batch 210, Loss: 0.4546, Accuracy: 0.8438\n",
      "Epoch 7, Batch 220, Loss: 0.4764, Accuracy: 0.8516\n",
      "Epoch 7, Batch 230, Loss: 0.5211, Accuracy: 0.8594\n",
      "Epoch 7, Batch 240, Loss: 0.4376, Accuracy: 0.8828\n",
      "Epoch 7, Batch 250, Loss: 0.5355, Accuracy: 0.7891\n",
      "Epoch 7, Batch 260, Loss: 0.4501, Accuracy: 0.8359\n",
      "Epoch 7, Batch 270, Loss: 0.4150, Accuracy: 0.8906\n",
      "Epoch 7, Batch 280, Loss: 0.4778, Accuracy: 0.8672\n",
      "Epoch 7, Batch 290, Loss: 0.4581, Accuracy: 0.8516\n",
      "Epoch 7, Batch 300, Loss: 0.4491, Accuracy: 0.8750\n",
      "Epoch 7, Batch 310, Loss: 0.5384, Accuracy: 0.8438\n",
      "Epoch 7, Batch 320, Loss: 0.3480, Accuracy: 0.9062\n",
      "Epoch 7, Batch 330, Loss: 0.6070, Accuracy: 0.8594\n",
      "Epoch 7, Batch 340, Loss: 0.3489, Accuracy: 0.8984\n",
      "Epoch 7, Batch 350, Loss: 0.4483, Accuracy: 0.8594\n",
      "Epoch 7, Batch 360, Loss: 0.3243, Accuracy: 0.9062\n",
      "Epoch 7, Batch 370, Loss: 0.3485, Accuracy: 0.9141\n",
      "Epoch 7, Batch 380, Loss: 0.4359, Accuracy: 0.8828\n",
      "Epoch 7, Batch 390, Loss: 0.5838, Accuracy: 0.8438\n",
      "Epoch 7, Batch 400, Loss: 0.4816, Accuracy: 0.8594\n",
      "Epoch 7, Batch 410, Loss: 0.6020, Accuracy: 0.7812\n",
      "Epoch 7, Batch 420, Loss: 0.3240, Accuracy: 0.8906\n",
      "Epoch 7, Batch 430, Loss: 0.4527, Accuracy: 0.8516\n",
      "Epoch 7, Batch 440, Loss: 0.4072, Accuracy: 0.9062\n",
      "Epoch 7, Batch 450, Loss: 0.4746, Accuracy: 0.8828\n",
      "Epoch 7, Batch 460, Loss: 0.5565, Accuracy: 0.8281\n",
      "Epoch 7, Train Accuracy: 0.8560, Test Loss: 0.4713\n",
      "Epoch 7, Validation Accuracy: 0.8811, Validation Loss: 0.4068\n",
      "Epoch 8, Batch 0, Loss: 0.4062, Accuracy: 0.8828\n",
      "Epoch 8, Batch 10, Loss: 0.3363, Accuracy: 0.9141\n",
      "Epoch 8, Batch 20, Loss: 0.3120, Accuracy: 0.9375\n",
      "Epoch 8, Batch 30, Loss: 0.5088, Accuracy: 0.8125\n",
      "Epoch 8, Batch 40, Loss: 0.6643, Accuracy: 0.7891\n",
      "Epoch 8, Batch 50, Loss: 0.3529, Accuracy: 0.8906\n",
      "Epoch 8, Batch 60, Loss: 0.4632, Accuracy: 0.8516\n",
      "Epoch 8, Batch 70, Loss: 0.3722, Accuracy: 0.8672\n",
      "Epoch 8, Batch 80, Loss: 0.4838, Accuracy: 0.8594\n",
      "Epoch 8, Batch 90, Loss: 0.5793, Accuracy: 0.7891\n",
      "Epoch 8, Batch 100, Loss: 0.2903, Accuracy: 0.9062\n",
      "Epoch 8, Batch 110, Loss: 0.3997, Accuracy: 0.8750\n",
      "Epoch 8, Batch 120, Loss: 0.3653, Accuracy: 0.8906\n",
      "Epoch 8, Batch 130, Loss: 0.3120, Accuracy: 0.9062\n",
      "Epoch 8, Batch 140, Loss: 0.2944, Accuracy: 0.9062\n",
      "Epoch 8, Batch 150, Loss: 0.4706, Accuracy: 0.8359\n",
      "Epoch 8, Batch 160, Loss: 0.3423, Accuracy: 0.8672\n",
      "Epoch 8, Batch 170, Loss: 0.3851, Accuracy: 0.8672\n",
      "Epoch 8, Batch 180, Loss: 0.4831, Accuracy: 0.8125\n",
      "Epoch 8, Batch 190, Loss: 0.6291, Accuracy: 0.8125\n",
      "Epoch 8, Batch 200, Loss: 0.3630, Accuracy: 0.8984\n",
      "Epoch 8, Batch 210, Loss: 0.3528, Accuracy: 0.9062\n",
      "Epoch 8, Batch 220, Loss: 0.4011, Accuracy: 0.8906\n",
      "Epoch 8, Batch 230, Loss: 0.3888, Accuracy: 0.9141\n",
      "Epoch 8, Batch 240, Loss: 0.5312, Accuracy: 0.8438\n",
      "Epoch 8, Batch 250, Loss: 0.4582, Accuracy: 0.8828\n",
      "Epoch 8, Batch 260, Loss: 0.3942, Accuracy: 0.8594\n",
      "Epoch 8, Batch 270, Loss: 0.2710, Accuracy: 0.9141\n",
      "Epoch 8, Batch 280, Loss: 0.4255, Accuracy: 0.8438\n",
      "Epoch 8, Batch 290, Loss: 0.3064, Accuracy: 0.8828\n",
      "Epoch 8, Batch 300, Loss: 0.3545, Accuracy: 0.8672\n",
      "Epoch 8, Batch 310, Loss: 0.3816, Accuracy: 0.8906\n",
      "Epoch 8, Batch 320, Loss: 0.3400, Accuracy: 0.8828\n",
      "Epoch 8, Batch 330, Loss: 0.4227, Accuracy: 0.8906\n",
      "Epoch 8, Batch 340, Loss: 0.6330, Accuracy: 0.8516\n",
      "Epoch 8, Batch 350, Loss: 0.4051, Accuracy: 0.8906\n",
      "Epoch 8, Batch 360, Loss: 0.3969, Accuracy: 0.8672\n",
      "Epoch 8, Batch 370, Loss: 0.2812, Accuracy: 0.9219\n",
      "Epoch 8, Batch 380, Loss: 0.2917, Accuracy: 0.9141\n",
      "Epoch 8, Batch 390, Loss: 0.3140, Accuracy: 0.9219\n",
      "Epoch 8, Batch 400, Loss: 0.2508, Accuracy: 0.9219\n",
      "Epoch 8, Batch 410, Loss: 0.4889, Accuracy: 0.8203\n",
      "Epoch 8, Batch 420, Loss: 0.3894, Accuracy: 0.8750\n",
      "Epoch 8, Batch 430, Loss: 0.2902, Accuracy: 0.9141\n",
      "Epoch 8, Batch 440, Loss: 0.3437, Accuracy: 0.8828\n",
      "Epoch 8, Batch 450, Loss: 0.4486, Accuracy: 0.8984\n",
      "Epoch 8, Batch 460, Loss: 0.3162, Accuracy: 0.8906\n",
      "Epoch 8, Train Accuracy: 0.8763, Test Loss: 0.4025\n",
      "Epoch 8, Validation Accuracy: 0.8947, Validation Loss: 0.3563\n",
      "Epoch 9, Batch 0, Loss: 0.4730, Accuracy: 0.8516\n",
      "Epoch 9, Batch 10, Loss: 0.3446, Accuracy: 0.8984\n",
      "Epoch 9, Batch 20, Loss: 0.3937, Accuracy: 0.8828\n",
      "Epoch 9, Batch 30, Loss: 0.2616, Accuracy: 0.9141\n",
      "Epoch 9, Batch 40, Loss: 0.5060, Accuracy: 0.8594\n",
      "Epoch 9, Batch 50, Loss: 0.4673, Accuracy: 0.8672\n",
      "Epoch 9, Batch 60, Loss: 0.3789, Accuracy: 0.8828\n",
      "Epoch 9, Batch 70, Loss: 0.3095, Accuracy: 0.8906\n",
      "Epoch 9, Batch 80, Loss: 0.3486, Accuracy: 0.9062\n",
      "Epoch 9, Batch 90, Loss: 0.4148, Accuracy: 0.8906\n",
      "Epoch 9, Batch 100, Loss: 0.3165, Accuracy: 0.8984\n",
      "Epoch 9, Batch 110, Loss: 0.3604, Accuracy: 0.9141\n",
      "Epoch 9, Batch 120, Loss: 0.5974, Accuracy: 0.8047\n",
      "Epoch 9, Batch 130, Loss: 0.4090, Accuracy: 0.8828\n",
      "Epoch 9, Batch 140, Loss: 0.4463, Accuracy: 0.8672\n",
      "Epoch 9, Batch 150, Loss: 0.3582, Accuracy: 0.8984\n",
      "Epoch 9, Batch 160, Loss: 0.2771, Accuracy: 0.9297\n",
      "Epoch 9, Batch 170, Loss: 0.4733, Accuracy: 0.8516\n",
      "Epoch 9, Batch 180, Loss: 0.4421, Accuracy: 0.8281\n",
      "Epoch 9, Batch 190, Loss: 0.3780, Accuracy: 0.8750\n",
      "Epoch 9, Batch 200, Loss: 0.4195, Accuracy: 0.8750\n",
      "Epoch 9, Batch 210, Loss: 0.3769, Accuracy: 0.8906\n",
      "Epoch 9, Batch 220, Loss: 0.3334, Accuracy: 0.9062\n",
      "Epoch 9, Batch 230, Loss: 0.2155, Accuracy: 0.9609\n",
      "Epoch 9, Batch 240, Loss: 0.3953, Accuracy: 0.8750\n",
      "Epoch 9, Batch 250, Loss: 0.4374, Accuracy: 0.8906\n",
      "Epoch 9, Batch 260, Loss: 0.2967, Accuracy: 0.8906\n",
      "Epoch 9, Batch 270, Loss: 0.3021, Accuracy: 0.9062\n",
      "Epoch 9, Batch 280, Loss: 0.4024, Accuracy: 0.8906\n",
      "Epoch 9, Batch 290, Loss: 0.3925, Accuracy: 0.9062\n",
      "Epoch 9, Batch 300, Loss: 0.2833, Accuracy: 0.9219\n",
      "Epoch 9, Batch 310, Loss: 0.3172, Accuracy: 0.9219\n",
      "Epoch 9, Batch 320, Loss: 0.3244, Accuracy: 0.8984\n",
      "Epoch 9, Batch 330, Loss: 0.2658, Accuracy: 0.9297\n",
      "Epoch 9, Batch 340, Loss: 0.2806, Accuracy: 0.9141\n",
      "Epoch 9, Batch 350, Loss: 0.4366, Accuracy: 0.8203\n",
      "Epoch 9, Batch 360, Loss: 0.2767, Accuracy: 0.9297\n",
      "Epoch 9, Batch 370, Loss: 0.4749, Accuracy: 0.8438\n",
      "Epoch 9, Batch 380, Loss: 0.3571, Accuracy: 0.8984\n",
      "Epoch 9, Batch 390, Loss: 0.3763, Accuracy: 0.8984\n",
      "Epoch 9, Batch 400, Loss: 0.2062, Accuracy: 0.9297\n",
      "Epoch 9, Batch 410, Loss: 0.4126, Accuracy: 0.9062\n",
      "Epoch 9, Batch 420, Loss: 0.3415, Accuracy: 0.8750\n",
      "Epoch 9, Batch 430, Loss: 0.3471, Accuracy: 0.8906\n",
      "Epoch 9, Batch 440, Loss: 0.3968, Accuracy: 0.8828\n",
      "Epoch 9, Batch 450, Loss: 0.3099, Accuracy: 0.9062\n",
      "Epoch 9, Batch 460, Loss: 0.3149, Accuracy: 0.8906\n",
      "Epoch 9, Train Accuracy: 0.8920, Test Loss: 0.3534\n",
      "Epoch 9, Validation Accuracy: 0.9007, Validation Loss: 0.3242\n",
      "Epoch 10, Batch 0, Loss: 0.4103, Accuracy: 0.8984\n",
      "Epoch 10, Batch 10, Loss: 0.2140, Accuracy: 0.9375\n",
      "Epoch 10, Batch 20, Loss: 0.3019, Accuracy: 0.8906\n",
      "Epoch 10, Batch 30, Loss: 0.2711, Accuracy: 0.9062\n",
      "Epoch 10, Batch 40, Loss: 0.3938, Accuracy: 0.8594\n",
      "Epoch 10, Batch 50, Loss: 0.3711, Accuracy: 0.8750\n",
      "Epoch 10, Batch 60, Loss: 0.2190, Accuracy: 0.9297\n",
      "Epoch 10, Batch 70, Loss: 0.3695, Accuracy: 0.8984\n",
      "Epoch 10, Batch 80, Loss: 0.3291, Accuracy: 0.8828\n",
      "Epoch 10, Batch 90, Loss: 0.3507, Accuracy: 0.8750\n",
      "Epoch 10, Batch 100, Loss: 0.4266, Accuracy: 0.8828\n",
      "Epoch 10, Batch 110, Loss: 0.2478, Accuracy: 0.9141\n",
      "Epoch 10, Batch 120, Loss: 0.3288, Accuracy: 0.9219\n",
      "Epoch 10, Batch 130, Loss: 0.3253, Accuracy: 0.8984\n",
      "Epoch 10, Batch 140, Loss: 0.1826, Accuracy: 0.9609\n",
      "Epoch 10, Batch 150, Loss: 0.3077, Accuracy: 0.9297\n",
      "Epoch 10, Batch 160, Loss: 0.5216, Accuracy: 0.8516\n",
      "Epoch 10, Batch 170, Loss: 0.3290, Accuracy: 0.9531\n",
      "Epoch 10, Batch 180, Loss: 0.1807, Accuracy: 0.9688\n",
      "Epoch 10, Batch 190, Loss: 0.2229, Accuracy: 0.9453\n",
      "Epoch 10, Batch 200, Loss: 0.1965, Accuracy: 0.9531\n",
      "Epoch 10, Batch 210, Loss: 0.3797, Accuracy: 0.8828\n",
      "Epoch 10, Batch 220, Loss: 0.3609, Accuracy: 0.9219\n",
      "Epoch 10, Batch 230, Loss: 0.2642, Accuracy: 0.9141\n",
      "Epoch 10, Batch 240, Loss: 0.3860, Accuracy: 0.8516\n",
      "Epoch 10, Batch 250, Loss: 0.2314, Accuracy: 0.9219\n",
      "Epoch 10, Batch 260, Loss: 0.3562, Accuracy: 0.8672\n",
      "Epoch 10, Batch 270, Loss: 0.2416, Accuracy: 0.9219\n",
      "Epoch 10, Batch 280, Loss: 0.5235, Accuracy: 0.8438\n",
      "Epoch 10, Batch 290, Loss: 0.2924, Accuracy: 0.8984\n",
      "Epoch 10, Batch 300, Loss: 0.3371, Accuracy: 0.8984\n",
      "Epoch 10, Batch 310, Loss: 0.1779, Accuracy: 0.9609\n",
      "Epoch 10, Batch 320, Loss: 0.2339, Accuracy: 0.9219\n",
      "Epoch 10, Batch 330, Loss: 0.3879, Accuracy: 0.8750\n",
      "Epoch 10, Batch 340, Loss: 0.2920, Accuracy: 0.9141\n",
      "Epoch 10, Batch 350, Loss: 0.3136, Accuracy: 0.8906\n",
      "Epoch 10, Batch 360, Loss: 0.2467, Accuracy: 0.9141\n",
      "Epoch 10, Batch 370, Loss: 0.2797, Accuracy: 0.9062\n",
      "Epoch 10, Batch 380, Loss: 0.3281, Accuracy: 0.8906\n",
      "Epoch 10, Batch 390, Loss: 0.3352, Accuracy: 0.8672\n",
      "Epoch 10, Batch 400, Loss: 0.4011, Accuracy: 0.8672\n",
      "Epoch 10, Batch 410, Loss: 0.2956, Accuracy: 0.8984\n",
      "Epoch 10, Batch 420, Loss: 0.2525, Accuracy: 0.9297\n",
      "Epoch 10, Batch 430, Loss: 0.3427, Accuracy: 0.8906\n",
      "Epoch 10, Batch 440, Loss: 0.2658, Accuracy: 0.9219\n",
      "Epoch 10, Batch 450, Loss: 0.1618, Accuracy: 0.9531\n",
      "Epoch 10, Batch 460, Loss: 0.3512, Accuracy: 0.8984\n",
      "Epoch 10, Train Accuracy: 0.9024, Test Loss: 0.3183\n",
      "Epoch 10, Validation Accuracy: 0.9174, Validation Loss: 0.2725\n",
      "Epoch 11, Batch 0, Loss: 0.1733, Accuracy: 0.9375\n",
      "Epoch 11, Batch 10, Loss: 0.2836, Accuracy: 0.9141\n",
      "Epoch 11, Batch 20, Loss: 0.2875, Accuracy: 0.8984\n",
      "Epoch 11, Batch 30, Loss: 0.2190, Accuracy: 0.9297\n",
      "Epoch 11, Batch 40, Loss: 0.3375, Accuracy: 0.8672\n",
      "Epoch 11, Batch 50, Loss: 0.3296, Accuracy: 0.9141\n",
      "Epoch 11, Batch 60, Loss: 0.2929, Accuracy: 0.8984\n",
      "Epoch 11, Batch 70, Loss: 0.3344, Accuracy: 0.9062\n",
      "Epoch 11, Batch 80, Loss: 0.4071, Accuracy: 0.8672\n",
      "Epoch 11, Batch 90, Loss: 0.3342, Accuracy: 0.9062\n",
      "Epoch 11, Batch 100, Loss: 0.2226, Accuracy: 0.9453\n",
      "Epoch 11, Batch 110, Loss: 0.2852, Accuracy: 0.9219\n",
      "Epoch 11, Batch 120, Loss: 0.2162, Accuracy: 0.9141\n",
      "Epoch 11, Batch 130, Loss: 0.1854, Accuracy: 0.9375\n",
      "Epoch 11, Batch 140, Loss: 0.3354, Accuracy: 0.8906\n",
      "Epoch 11, Batch 150, Loss: 0.3130, Accuracy: 0.9062\n",
      "Epoch 11, Batch 160, Loss: 0.3122, Accuracy: 0.9062\n",
      "Epoch 11, Batch 170, Loss: 0.2076, Accuracy: 0.9297\n",
      "Epoch 11, Batch 180, Loss: 0.2359, Accuracy: 0.9297\n",
      "Epoch 11, Batch 190, Loss: 0.3516, Accuracy: 0.9141\n",
      "Epoch 11, Batch 200, Loss: 0.2539, Accuracy: 0.9375\n",
      "Epoch 11, Batch 210, Loss: 0.3686, Accuracy: 0.8672\n",
      "Epoch 11, Batch 220, Loss: 0.3165, Accuracy: 0.8906\n",
      "Epoch 11, Batch 230, Loss: 0.2368, Accuracy: 0.9141\n",
      "Epoch 11, Batch 240, Loss: 0.3402, Accuracy: 0.8828\n",
      "Epoch 11, Batch 250, Loss: 0.2434, Accuracy: 0.9375\n",
      "Epoch 11, Batch 260, Loss: 0.3005, Accuracy: 0.8906\n",
      "Epoch 11, Batch 270, Loss: 0.2087, Accuracy: 0.9453\n",
      "Epoch 11, Batch 280, Loss: 0.2015, Accuracy: 0.9375\n",
      "Epoch 11, Batch 290, Loss: 0.2737, Accuracy: 0.9219\n",
      "Epoch 11, Batch 300, Loss: 0.2648, Accuracy: 0.9453\n",
      "Epoch 11, Batch 310, Loss: 0.3230, Accuracy: 0.8984\n",
      "Epoch 11, Batch 320, Loss: 0.3941, Accuracy: 0.8828\n",
      "Epoch 11, Batch 330, Loss: 0.2884, Accuracy: 0.9062\n",
      "Epoch 11, Batch 340, Loss: 0.2118, Accuracy: 0.9297\n",
      "Epoch 11, Batch 350, Loss: 0.3530, Accuracy: 0.8906\n",
      "Epoch 11, Batch 360, Loss: 0.3184, Accuracy: 0.9062\n",
      "Epoch 11, Batch 370, Loss: 0.3655, Accuracy: 0.9141\n",
      "Epoch 11, Batch 380, Loss: 0.2712, Accuracy: 0.8906\n",
      "Epoch 11, Batch 390, Loss: 0.1815, Accuracy: 0.9375\n",
      "Epoch 11, Batch 400, Loss: 0.3165, Accuracy: 0.8750\n",
      "Epoch 11, Batch 410, Loss: 0.2440, Accuracy: 0.9297\n",
      "Epoch 11, Batch 420, Loss: 0.2341, Accuracy: 0.9297\n",
      "Epoch 11, Batch 430, Loss: 0.2303, Accuracy: 0.9531\n",
      "Epoch 11, Batch 440, Loss: 0.2545, Accuracy: 0.9219\n",
      "Epoch 11, Batch 450, Loss: 0.2961, Accuracy: 0.9375\n",
      "Epoch 11, Batch 460, Loss: 0.3351, Accuracy: 0.8906\n",
      "Epoch 11, Train Accuracy: 0.9109, Test Loss: 0.2916\n",
      "Epoch 11, Validation Accuracy: 0.9219, Validation Loss: 0.2640\n",
      "Epoch 12, Batch 0, Loss: 0.2388, Accuracy: 0.9141\n",
      "Epoch 12, Batch 10, Loss: 0.3216, Accuracy: 0.9062\n",
      "Epoch 12, Batch 20, Loss: 0.4234, Accuracy: 0.8516\n",
      "Epoch 12, Batch 30, Loss: 0.1982, Accuracy: 0.9375\n",
      "Epoch 12, Batch 40, Loss: 0.2165, Accuracy: 0.9375\n",
      "Epoch 12, Batch 50, Loss: 0.2436, Accuracy: 0.9453\n",
      "Epoch 12, Batch 60, Loss: 0.3252, Accuracy: 0.8906\n",
      "Epoch 12, Batch 70, Loss: 0.4593, Accuracy: 0.8828\n",
      "Epoch 12, Batch 80, Loss: 0.4153, Accuracy: 0.8672\n",
      "Epoch 12, Batch 90, Loss: 0.4920, Accuracy: 0.8828\n",
      "Epoch 12, Batch 100, Loss: 0.2100, Accuracy: 0.9453\n",
      "Epoch 12, Batch 110, Loss: 0.1332, Accuracy: 0.9688\n",
      "Epoch 12, Batch 120, Loss: 0.2896, Accuracy: 0.9141\n",
      "Epoch 12, Batch 130, Loss: 0.2597, Accuracy: 0.9219\n",
      "Epoch 12, Batch 140, Loss: 0.2067, Accuracy: 0.9219\n",
      "Epoch 12, Batch 150, Loss: 0.2457, Accuracy: 0.9297\n",
      "Epoch 12, Batch 160, Loss: 0.2513, Accuracy: 0.9141\n",
      "Epoch 12, Batch 170, Loss: 0.2608, Accuracy: 0.9062\n",
      "Epoch 12, Batch 180, Loss: 0.2461, Accuracy: 0.9297\n",
      "Epoch 12, Batch 190, Loss: 0.2409, Accuracy: 0.8984\n",
      "Epoch 12, Batch 200, Loss: 0.3108, Accuracy: 0.9219\n",
      "Epoch 12, Batch 210, Loss: 0.2989, Accuracy: 0.9141\n",
      "Epoch 12, Batch 220, Loss: 0.3142, Accuracy: 0.8750\n",
      "Epoch 12, Batch 230, Loss: 0.3258, Accuracy: 0.9375\n",
      "Epoch 12, Batch 240, Loss: 0.2098, Accuracy: 0.9453\n",
      "Epoch 12, Batch 250, Loss: 0.3779, Accuracy: 0.8672\n",
      "Epoch 12, Batch 260, Loss: 0.3023, Accuracy: 0.9062\n",
      "Epoch 12, Batch 270, Loss: 0.2470, Accuracy: 0.9141\n",
      "Epoch 12, Batch 280, Loss: 0.3194, Accuracy: 0.9062\n",
      "Epoch 12, Batch 290, Loss: 0.3989, Accuracy: 0.8359\n",
      "Epoch 12, Batch 300, Loss: 0.1781, Accuracy: 0.9688\n",
      "Epoch 12, Batch 310, Loss: 0.3437, Accuracy: 0.8594\n",
      "Epoch 12, Batch 320, Loss: 0.3223, Accuracy: 0.9141\n",
      "Epoch 12, Batch 330, Loss: 0.2808, Accuracy: 0.9219\n",
      "Epoch 12, Batch 340, Loss: 0.2721, Accuracy: 0.9141\n",
      "Epoch 12, Batch 350, Loss: 0.2704, Accuracy: 0.9062\n",
      "Epoch 12, Batch 360, Loss: 0.2823, Accuracy: 0.9219\n",
      "Epoch 12, Batch 370, Loss: 0.4512, Accuracy: 0.8984\n",
      "Epoch 12, Batch 380, Loss: 0.3696, Accuracy: 0.8750\n",
      "Epoch 12, Batch 390, Loss: 0.2732, Accuracy: 0.8828\n",
      "Epoch 12, Batch 400, Loss: 0.2959, Accuracy: 0.8984\n",
      "Epoch 12, Batch 410, Loss: 0.3875, Accuracy: 0.8984\n",
      "Epoch 12, Batch 420, Loss: 0.2983, Accuracy: 0.9062\n",
      "Epoch 12, Batch 430, Loss: 0.3055, Accuracy: 0.9297\n",
      "Epoch 12, Batch 440, Loss: 0.4194, Accuracy: 0.8984\n",
      "Epoch 12, Batch 450, Loss: 0.1439, Accuracy: 0.9766\n",
      "Epoch 12, Batch 460, Loss: 0.3289, Accuracy: 0.8984\n",
      "Epoch 12, Train Accuracy: 0.9163, Test Loss: 0.2748\n",
      "Epoch 12, Validation Accuracy: 0.9210, Validation Loss: 0.2616\n",
      "Epoch 13, Batch 0, Loss: 0.3360, Accuracy: 0.8906\n",
      "Epoch 13, Batch 10, Loss: 0.2715, Accuracy: 0.8906\n",
      "Epoch 13, Batch 20, Loss: 0.3801, Accuracy: 0.8594\n",
      "Epoch 13, Batch 30, Loss: 0.2683, Accuracy: 0.9219\n",
      "Epoch 13, Batch 40, Loss: 0.2413, Accuracy: 0.9297\n",
      "Epoch 13, Batch 50, Loss: 0.1774, Accuracy: 0.9766\n",
      "Epoch 13, Batch 60, Loss: 0.2452, Accuracy: 0.9219\n",
      "Epoch 13, Batch 70, Loss: 0.2114, Accuracy: 0.9297\n",
      "Epoch 13, Batch 80, Loss: 0.1484, Accuracy: 0.9766\n",
      "Epoch 13, Batch 90, Loss: 0.2904, Accuracy: 0.8594\n",
      "Epoch 13, Batch 100, Loss: 0.2967, Accuracy: 0.8984\n",
      "Epoch 13, Batch 110, Loss: 0.2848, Accuracy: 0.9219\n",
      "Epoch 13, Batch 120, Loss: 0.3034, Accuracy: 0.9375\n",
      "Epoch 13, Batch 130, Loss: 0.2276, Accuracy: 0.9219\n",
      "Epoch 13, Batch 140, Loss: 0.3306, Accuracy: 0.8984\n",
      "Epoch 13, Batch 150, Loss: 0.2468, Accuracy: 0.9219\n",
      "Epoch 13, Batch 160, Loss: 0.2637, Accuracy: 0.9219\n",
      "Epoch 13, Batch 170, Loss: 0.1190, Accuracy: 0.9766\n",
      "Epoch 13, Batch 180, Loss: 0.1940, Accuracy: 0.9531\n",
      "Epoch 13, Batch 190, Loss: 0.2006, Accuracy: 0.9453\n",
      "Epoch 13, Batch 200, Loss: 0.3458, Accuracy: 0.9062\n",
      "Epoch 13, Batch 210, Loss: 0.2350, Accuracy: 0.8984\n",
      "Epoch 13, Batch 220, Loss: 0.2114, Accuracy: 0.9531\n",
      "Epoch 13, Batch 230, Loss: 0.4614, Accuracy: 0.8828\n",
      "Epoch 13, Batch 240, Loss: 0.2721, Accuracy: 0.9062\n",
      "Epoch 13, Batch 250, Loss: 0.2933, Accuracy: 0.9219\n",
      "Epoch 13, Batch 260, Loss: 0.2499, Accuracy: 0.9062\n",
      "Epoch 13, Batch 270, Loss: 0.3803, Accuracy: 0.8750\n",
      "Epoch 13, Batch 280, Loss: 0.2848, Accuracy: 0.9219\n",
      "Epoch 13, Batch 290, Loss: 0.2962, Accuracy: 0.9297\n",
      "Epoch 13, Batch 300, Loss: 0.3288, Accuracy: 0.8906\n",
      "Epoch 13, Batch 310, Loss: 0.1177, Accuracy: 0.9766\n",
      "Epoch 13, Batch 320, Loss: 0.2719, Accuracy: 0.9062\n",
      "Epoch 13, Batch 330, Loss: 0.2064, Accuracy: 0.9141\n",
      "Epoch 13, Batch 340, Loss: 0.2950, Accuracy: 0.9141\n",
      "Epoch 13, Batch 350, Loss: 0.2017, Accuracy: 0.9141\n",
      "Epoch 13, Batch 360, Loss: 0.3796, Accuracy: 0.8828\n",
      "Epoch 13, Batch 370, Loss: 0.3444, Accuracy: 0.8594\n",
      "Epoch 13, Batch 380, Loss: 0.3049, Accuracy: 0.9062\n",
      "Epoch 13, Batch 390, Loss: 0.2223, Accuracy: 0.9375\n",
      "Epoch 13, Batch 400, Loss: 0.2639, Accuracy: 0.9062\n",
      "Epoch 13, Batch 410, Loss: 0.3122, Accuracy: 0.9141\n",
      "Epoch 13, Batch 420, Loss: 0.2081, Accuracy: 0.9141\n",
      "Epoch 13, Batch 430, Loss: 0.1941, Accuracy: 0.9609\n",
      "Epoch 13, Batch 440, Loss: 0.4059, Accuracy: 0.8984\n",
      "Epoch 13, Batch 450, Loss: 0.2742, Accuracy: 0.9297\n",
      "Epoch 13, Batch 460, Loss: 0.3024, Accuracy: 0.9062\n",
      "Epoch 13, Train Accuracy: 0.9210, Test Loss: 0.2588\n",
      "Epoch 13, Validation Accuracy: 0.9348, Validation Loss: 0.2271\n",
      "Epoch 14, Batch 0, Loss: 0.3168, Accuracy: 0.8828\n",
      "Epoch 14, Batch 10, Loss: 0.1923, Accuracy: 0.9219\n",
      "Epoch 14, Batch 20, Loss: 0.2900, Accuracy: 0.9062\n",
      "Epoch 14, Batch 30, Loss: 0.2619, Accuracy: 0.9141\n",
      "Epoch 14, Batch 40, Loss: 0.3557, Accuracy: 0.9141\n",
      "Epoch 14, Batch 50, Loss: 0.2770, Accuracy: 0.8906\n",
      "Epoch 14, Batch 60, Loss: 0.2529, Accuracy: 0.9453\n",
      "Epoch 14, Batch 70, Loss: 0.2756, Accuracy: 0.9141\n",
      "Epoch 14, Batch 80, Loss: 0.2783, Accuracy: 0.8906\n",
      "Epoch 14, Batch 90, Loss: 0.2452, Accuracy: 0.9141\n",
      "Epoch 14, Batch 100, Loss: 0.3751, Accuracy: 0.8828\n",
      "Epoch 14, Batch 110, Loss: 0.2513, Accuracy: 0.9219\n",
      "Epoch 14, Batch 120, Loss: 0.2730, Accuracy: 0.8984\n",
      "Epoch 14, Batch 130, Loss: 0.1516, Accuracy: 0.9531\n",
      "Epoch 14, Batch 140, Loss: 0.2091, Accuracy: 0.9531\n",
      "Epoch 14, Batch 150, Loss: 0.1949, Accuracy: 0.9375\n",
      "Epoch 14, Batch 160, Loss: 0.2162, Accuracy: 0.9297\n",
      "Epoch 14, Batch 170, Loss: 0.4213, Accuracy: 0.8750\n",
      "Epoch 14, Batch 180, Loss: 0.2742, Accuracy: 0.8984\n",
      "Epoch 14, Batch 190, Loss: 0.2137, Accuracy: 0.9688\n",
      "Epoch 14, Batch 200, Loss: 0.2384, Accuracy: 0.9375\n",
      "Epoch 14, Batch 210, Loss: 0.1340, Accuracy: 0.9609\n",
      "Epoch 14, Batch 220, Loss: 0.2263, Accuracy: 0.9219\n",
      "Epoch 14, Batch 230, Loss: 0.1953, Accuracy: 0.9219\n",
      "Epoch 14, Batch 240, Loss: 0.1633, Accuracy: 0.9297\n",
      "Epoch 14, Batch 250, Loss: 0.2014, Accuracy: 0.9453\n",
      "Epoch 14, Batch 260, Loss: 0.2485, Accuracy: 0.9297\n",
      "Epoch 14, Batch 270, Loss: 0.3889, Accuracy: 0.8906\n",
      "Epoch 14, Batch 280, Loss: 0.2036, Accuracy: 0.9375\n",
      "Epoch 14, Batch 290, Loss: 0.2384, Accuracy: 0.9453\n",
      "Epoch 14, Batch 300, Loss: 0.1955, Accuracy: 0.9297\n",
      "Epoch 14, Batch 310, Loss: 0.1928, Accuracy: 0.9453\n",
      "Epoch 14, Batch 320, Loss: 0.3313, Accuracy: 0.9062\n",
      "Epoch 14, Batch 330, Loss: 0.2603, Accuracy: 0.9062\n",
      "Epoch 14, Batch 340, Loss: 0.2501, Accuracy: 0.9219\n",
      "Epoch 14, Batch 350, Loss: 0.4320, Accuracy: 0.8750\n",
      "Epoch 14, Batch 360, Loss: 0.2503, Accuracy: 0.9062\n",
      "Epoch 14, Batch 370, Loss: 0.1303, Accuracy: 0.9453\n",
      "Epoch 14, Batch 380, Loss: 0.2009, Accuracy: 0.9453\n",
      "Epoch 14, Batch 390, Loss: 0.3419, Accuracy: 0.8906\n",
      "Epoch 14, Batch 400, Loss: 0.3929, Accuracy: 0.8906\n",
      "Epoch 14, Batch 410, Loss: 0.1448, Accuracy: 0.9531\n",
      "Epoch 14, Batch 420, Loss: 0.1777, Accuracy: 0.9219\n",
      "Epoch 14, Batch 430, Loss: 0.2228, Accuracy: 0.9375\n",
      "Epoch 14, Batch 440, Loss: 0.1866, Accuracy: 0.9375\n",
      "Epoch 14, Batch 450, Loss: 0.2447, Accuracy: 0.8984\n",
      "Epoch 14, Batch 460, Loss: 0.2078, Accuracy: 0.9375\n",
      "Epoch 14, Train Accuracy: 0.9244, Test Loss: 0.2445\n",
      "Epoch 14, Validation Accuracy: 0.9309, Validation Loss: 0.2360\n",
      "Epoch 15, Batch 0, Loss: 0.2696, Accuracy: 0.9219\n",
      "Epoch 15, Batch 10, Loss: 0.2645, Accuracy: 0.9141\n",
      "Epoch 15, Batch 20, Loss: 0.2087, Accuracy: 0.9375\n",
      "Epoch 15, Batch 30, Loss: 0.2025, Accuracy: 0.9297\n",
      "Epoch 15, Batch 40, Loss: 0.2634, Accuracy: 0.9062\n",
      "Epoch 15, Batch 50, Loss: 0.2581, Accuracy: 0.9297\n",
      "Epoch 15, Batch 60, Loss: 0.2636, Accuracy: 0.8906\n",
      "Epoch 15, Batch 70, Loss: 0.3492, Accuracy: 0.9141\n",
      "Epoch 15, Batch 80, Loss: 0.2289, Accuracy: 0.9297\n",
      "Epoch 15, Batch 90, Loss: 0.2407, Accuracy: 0.9531\n",
      "Epoch 15, Batch 100, Loss: 0.3573, Accuracy: 0.8672\n",
      "Epoch 15, Batch 110, Loss: 0.2280, Accuracy: 0.9297\n",
      "Epoch 15, Batch 120, Loss: 0.2134, Accuracy: 0.9688\n",
      "Epoch 15, Batch 130, Loss: 0.2102, Accuracy: 0.9375\n",
      "Epoch 15, Batch 140, Loss: 0.4297, Accuracy: 0.8750\n",
      "Epoch 15, Batch 150, Loss: 0.2142, Accuracy: 0.9375\n",
      "Epoch 15, Batch 160, Loss: 0.2585, Accuracy: 0.9062\n",
      "Epoch 15, Batch 170, Loss: 0.1895, Accuracy: 0.9688\n",
      "Epoch 15, Batch 180, Loss: 0.1487, Accuracy: 0.9453\n",
      "Epoch 15, Batch 190, Loss: 0.2949, Accuracy: 0.9297\n",
      "Epoch 15, Batch 200, Loss: 0.1847, Accuracy: 0.9453\n",
      "Epoch 15, Batch 210, Loss: 0.1764, Accuracy: 0.9453\n",
      "Epoch 15, Batch 220, Loss: 0.2547, Accuracy: 0.9219\n",
      "Epoch 15, Batch 230, Loss: 0.1360, Accuracy: 0.9766\n",
      "Epoch 15, Batch 240, Loss: 0.1594, Accuracy: 0.9531\n",
      "Epoch 15, Batch 250, Loss: 0.2071, Accuracy: 0.9531\n",
      "Epoch 15, Batch 260, Loss: 0.2612, Accuracy: 0.9141\n",
      "Epoch 15, Batch 270, Loss: 0.2688, Accuracy: 0.9062\n",
      "Epoch 15, Batch 280, Loss: 0.2968, Accuracy: 0.9219\n",
      "Epoch 15, Batch 290, Loss: 0.2738, Accuracy: 0.9297\n",
      "Epoch 15, Batch 300, Loss: 0.1382, Accuracy: 0.9531\n",
      "Epoch 15, Batch 310, Loss: 0.1828, Accuracy: 0.9297\n",
      "Epoch 15, Batch 320, Loss: 0.2474, Accuracy: 0.9219\n",
      "Epoch 15, Batch 330, Loss: 0.2062, Accuracy: 0.9219\n",
      "Epoch 15, Batch 340, Loss: 0.3084, Accuracy: 0.9141\n",
      "Epoch 15, Batch 350, Loss: 0.2236, Accuracy: 0.9219\n",
      "Epoch 15, Batch 360, Loss: 0.2117, Accuracy: 0.9453\n",
      "Epoch 15, Batch 370, Loss: 0.3432, Accuracy: 0.8984\n",
      "Epoch 15, Batch 380, Loss: 0.2082, Accuracy: 0.9062\n",
      "Epoch 15, Batch 390, Loss: 0.3880, Accuracy: 0.8984\n",
      "Epoch 15, Batch 400, Loss: 0.1837, Accuracy: 0.9141\n",
      "Epoch 15, Batch 410, Loss: 0.1818, Accuracy: 0.9453\n",
      "Epoch 15, Batch 420, Loss: 0.3046, Accuracy: 0.9062\n",
      "Epoch 15, Batch 430, Loss: 0.2144, Accuracy: 0.9062\n",
      "Epoch 15, Batch 440, Loss: 0.2617, Accuracy: 0.9219\n",
      "Epoch 15, Batch 450, Loss: 0.2236, Accuracy: 0.9219\n",
      "Epoch 15, Batch 460, Loss: 0.2448, Accuracy: 0.9219\n",
      "Epoch 15, Train Accuracy: 0.9283, Test Loss: 0.2330\n",
      "Epoch 15, Validation Accuracy: 0.9279, Validation Loss: 0.2386\n",
      "Epoch 16, Batch 0, Loss: 0.2136, Accuracy: 0.9297\n",
      "Epoch 16, Batch 10, Loss: 0.2417, Accuracy: 0.9141\n",
      "Epoch 16, Batch 20, Loss: 0.1897, Accuracy: 0.9531\n",
      "Epoch 16, Batch 30, Loss: 0.3568, Accuracy: 0.8594\n",
      "Epoch 16, Batch 40, Loss: 0.2369, Accuracy: 0.9297\n",
      "Epoch 16, Batch 50, Loss: 0.3041, Accuracy: 0.8828\n",
      "Epoch 16, Batch 60, Loss: 0.1348, Accuracy: 0.9688\n",
      "Epoch 16, Batch 70, Loss: 0.1783, Accuracy: 0.9297\n",
      "Epoch 16, Batch 80, Loss: 0.2121, Accuracy: 0.9297\n",
      "Epoch 16, Batch 90, Loss: 0.2229, Accuracy: 0.9297\n",
      "Epoch 16, Batch 100, Loss: 0.2702, Accuracy: 0.8906\n",
      "Epoch 16, Batch 110, Loss: 0.1629, Accuracy: 0.9531\n",
      "Epoch 16, Batch 120, Loss: 0.2280, Accuracy: 0.9297\n",
      "Epoch 16, Batch 130, Loss: 0.1830, Accuracy: 0.9297\n",
      "Epoch 16, Batch 140, Loss: 0.1481, Accuracy: 0.9453\n",
      "Epoch 16, Batch 150, Loss: 0.2160, Accuracy: 0.9375\n",
      "Epoch 16, Batch 160, Loss: 0.2317, Accuracy: 0.9141\n",
      "Epoch 16, Batch 170, Loss: 0.1917, Accuracy: 0.9297\n",
      "Epoch 16, Batch 180, Loss: 0.2923, Accuracy: 0.8750\n",
      "Epoch 16, Batch 190, Loss: 0.1771, Accuracy: 0.9375\n",
      "Epoch 16, Batch 200, Loss: 0.1680, Accuracy: 0.9609\n",
      "Epoch 16, Batch 210, Loss: 0.1755, Accuracy: 0.9141\n",
      "Epoch 16, Batch 220, Loss: 0.1221, Accuracy: 0.9688\n",
      "Epoch 16, Batch 230, Loss: 0.1364, Accuracy: 0.9531\n",
      "Epoch 16, Batch 240, Loss: 0.1453, Accuracy: 0.9531\n",
      "Epoch 16, Batch 250, Loss: 0.1691, Accuracy: 0.9531\n",
      "Epoch 16, Batch 260, Loss: 0.2175, Accuracy: 0.9453\n",
      "Epoch 16, Batch 270, Loss: 0.2313, Accuracy: 0.9531\n",
      "Epoch 16, Batch 280, Loss: 0.3683, Accuracy: 0.8828\n",
      "Epoch 16, Batch 290, Loss: 0.2111, Accuracy: 0.9453\n",
      "Epoch 16, Batch 300, Loss: 0.3089, Accuracy: 0.8984\n",
      "Epoch 16, Batch 310, Loss: 0.2464, Accuracy: 0.9219\n",
      "Epoch 16, Batch 320, Loss: 0.3111, Accuracy: 0.9062\n",
      "Epoch 16, Batch 330, Loss: 0.2461, Accuracy: 0.9453\n",
      "Epoch 16, Batch 340, Loss: 0.2953, Accuracy: 0.9375\n",
      "Epoch 16, Batch 350, Loss: 0.1597, Accuracy: 0.9453\n",
      "Epoch 16, Batch 360, Loss: 0.2184, Accuracy: 0.9297\n",
      "Epoch 16, Batch 370, Loss: 0.2934, Accuracy: 0.9219\n",
      "Epoch 16, Batch 380, Loss: 0.1308, Accuracy: 0.9609\n",
      "Epoch 16, Batch 390, Loss: 0.1699, Accuracy: 0.9453\n",
      "Epoch 16, Batch 400, Loss: 0.1700, Accuracy: 0.9609\n",
      "Epoch 16, Batch 410, Loss: 0.2464, Accuracy: 0.9531\n",
      "Epoch 16, Batch 420, Loss: 0.2463, Accuracy: 0.9453\n",
      "Epoch 16, Batch 430, Loss: 0.2157, Accuracy: 0.9297\n",
      "Epoch 16, Batch 440, Loss: 0.2667, Accuracy: 0.9141\n",
      "Epoch 16, Batch 450, Loss: 0.1804, Accuracy: 0.9297\n",
      "Epoch 16, Batch 460, Loss: 0.1718, Accuracy: 0.9453\n",
      "Epoch 16, Train Accuracy: 0.9316, Test Loss: 0.2234\n",
      "Epoch 16, Validation Accuracy: 0.9444, Validation Loss: 0.1955\n",
      "Epoch 17, Batch 0, Loss: 0.2788, Accuracy: 0.9141\n",
      "Epoch 17, Batch 10, Loss: 0.4341, Accuracy: 0.9141\n",
      "Epoch 17, Batch 20, Loss: 0.2164, Accuracy: 0.9062\n",
      "Epoch 17, Batch 30, Loss: 0.1934, Accuracy: 0.9297\n",
      "Epoch 17, Batch 40, Loss: 0.1468, Accuracy: 0.9609\n",
      "Epoch 17, Batch 50, Loss: 0.1533, Accuracy: 0.9453\n",
      "Epoch 17, Batch 60, Loss: 0.2108, Accuracy: 0.9531\n",
      "Epoch 17, Batch 70, Loss: 0.1725, Accuracy: 0.9453\n",
      "Epoch 17, Batch 80, Loss: 0.3455, Accuracy: 0.8906\n",
      "Epoch 17, Batch 90, Loss: 0.1477, Accuracy: 0.9609\n",
      "Epoch 17, Batch 100, Loss: 0.2305, Accuracy: 0.9219\n",
      "Epoch 17, Batch 110, Loss: 0.1831, Accuracy: 0.9375\n",
      "Epoch 17, Batch 120, Loss: 0.1865, Accuracy: 0.9531\n",
      "Epoch 17, Batch 130, Loss: 0.1997, Accuracy: 0.9219\n",
      "Epoch 17, Batch 140, Loss: 0.1670, Accuracy: 0.9453\n",
      "Epoch 17, Batch 150, Loss: 0.1926, Accuracy: 0.9453\n",
      "Epoch 17, Batch 160, Loss: 0.2333, Accuracy: 0.9375\n",
      "Epoch 17, Batch 170, Loss: 0.3368, Accuracy: 0.8906\n",
      "Epoch 17, Batch 180, Loss: 0.1619, Accuracy: 0.9375\n",
      "Epoch 17, Batch 190, Loss: 0.1604, Accuracy: 0.9609\n",
      "Epoch 17, Batch 200, Loss: 0.4006, Accuracy: 0.8672\n",
      "Epoch 17, Batch 210, Loss: 0.2228, Accuracy: 0.9531\n",
      "Epoch 17, Batch 220, Loss: 0.1723, Accuracy: 0.9453\n",
      "Epoch 17, Batch 230, Loss: 0.1852, Accuracy: 0.9531\n",
      "Epoch 17, Batch 240, Loss: 0.1758, Accuracy: 0.9297\n",
      "Epoch 17, Batch 250, Loss: 0.1531, Accuracy: 0.9688\n",
      "Epoch 17, Batch 260, Loss: 0.1910, Accuracy: 0.9219\n",
      "Epoch 17, Batch 270, Loss: 0.1894, Accuracy: 0.9453\n",
      "Epoch 17, Batch 280, Loss: 0.2146, Accuracy: 0.9219\n",
      "Epoch 17, Batch 290, Loss: 0.2080, Accuracy: 0.9297\n",
      "Epoch 17, Batch 300, Loss: 0.1176, Accuracy: 0.9531\n",
      "Epoch 17, Batch 310, Loss: 0.1385, Accuracy: 0.9609\n",
      "Epoch 17, Batch 320, Loss: 0.2786, Accuracy: 0.9219\n",
      "Epoch 17, Batch 330, Loss: 0.3462, Accuracy: 0.9062\n",
      "Epoch 17, Batch 340, Loss: 0.1860, Accuracy: 0.9531\n",
      "Epoch 17, Batch 350, Loss: 0.1826, Accuracy: 0.9297\n",
      "Epoch 17, Batch 360, Loss: 0.2520, Accuracy: 0.9297\n",
      "Epoch 17, Batch 370, Loss: 0.2467, Accuracy: 0.9453\n",
      "Epoch 17, Batch 380, Loss: 0.2231, Accuracy: 0.9219\n",
      "Epoch 17, Batch 390, Loss: 0.1852, Accuracy: 0.9219\n",
      "Epoch 17, Batch 400, Loss: 0.2104, Accuracy: 0.9453\n",
      "Epoch 17, Batch 410, Loss: 0.1998, Accuracy: 0.9219\n",
      "Epoch 17, Batch 420, Loss: 0.2353, Accuracy: 0.9141\n",
      "Epoch 17, Batch 430, Loss: 0.1868, Accuracy: 0.9531\n",
      "Epoch 17, Batch 440, Loss: 0.1965, Accuracy: 0.9375\n",
      "Epoch 17, Batch 450, Loss: 0.1014, Accuracy: 0.9688\n",
      "Epoch 17, Batch 460, Loss: 0.2794, Accuracy: 0.9062\n",
      "Epoch 17, Train Accuracy: 0.9313, Test Loss: 0.2203\n",
      "Epoch 17, Validation Accuracy: 0.9390, Validation Loss: 0.2042\n",
      "Epoch 18, Batch 0, Loss: 0.1785, Accuracy: 0.9609\n",
      "Epoch 18, Batch 10, Loss: 0.2149, Accuracy: 0.9375\n",
      "Epoch 18, Batch 20, Loss: 0.3825, Accuracy: 0.8828\n",
      "Epoch 18, Batch 30, Loss: 0.1938, Accuracy: 0.9531\n",
      "Epoch 18, Batch 40, Loss: 0.2500, Accuracy: 0.9219\n",
      "Epoch 18, Batch 50, Loss: 0.4098, Accuracy: 0.8828\n",
      "Epoch 18, Batch 60, Loss: 0.1720, Accuracy: 0.9297\n",
      "Epoch 18, Batch 70, Loss: 0.2014, Accuracy: 0.9297\n",
      "Epoch 18, Batch 80, Loss: 0.1595, Accuracy: 0.9609\n",
      "Epoch 18, Batch 90, Loss: 0.2608, Accuracy: 0.9297\n",
      "Epoch 18, Batch 100, Loss: 0.1909, Accuracy: 0.9453\n",
      "Epoch 18, Batch 110, Loss: 0.1338, Accuracy: 0.9609\n",
      "Epoch 18, Batch 120, Loss: 0.2287, Accuracy: 0.9453\n",
      "Epoch 18, Batch 130, Loss: 0.3303, Accuracy: 0.8906\n",
      "Epoch 18, Batch 140, Loss: 0.1740, Accuracy: 0.9453\n",
      "Epoch 18, Batch 150, Loss: 0.1433, Accuracy: 0.9688\n",
      "Epoch 18, Batch 160, Loss: 0.3343, Accuracy: 0.9297\n",
      "Epoch 18, Batch 170, Loss: 0.2178, Accuracy: 0.9375\n",
      "Epoch 18, Batch 180, Loss: 0.2393, Accuracy: 0.9219\n",
      "Epoch 18, Batch 190, Loss: 0.2358, Accuracy: 0.9062\n",
      "Epoch 18, Batch 200, Loss: 0.1485, Accuracy: 0.9375\n",
      "Epoch 18, Batch 210, Loss: 0.3366, Accuracy: 0.9219\n",
      "Epoch 18, Batch 220, Loss: 0.2128, Accuracy: 0.9297\n",
      "Epoch 18, Batch 230, Loss: 0.2188, Accuracy: 0.9219\n",
      "Epoch 18, Batch 240, Loss: 0.1763, Accuracy: 0.9531\n",
      "Epoch 18, Batch 250, Loss: 0.1940, Accuracy: 0.9453\n",
      "Epoch 18, Batch 260, Loss: 0.1662, Accuracy: 0.9375\n",
      "Epoch 18, Batch 270, Loss: 0.2930, Accuracy: 0.9062\n",
      "Epoch 18, Batch 280, Loss: 0.2436, Accuracy: 0.9297\n",
      "Epoch 18, Batch 290, Loss: 0.2880, Accuracy: 0.9297\n",
      "Epoch 18, Batch 300, Loss: 0.1763, Accuracy: 0.9531\n",
      "Epoch 18, Batch 310, Loss: 0.2894, Accuracy: 0.9141\n",
      "Epoch 18, Batch 320, Loss: 0.1626, Accuracy: 0.9531\n",
      "Epoch 18, Batch 330, Loss: 0.2065, Accuracy: 0.9453\n",
      "Epoch 18, Batch 340, Loss: 0.1358, Accuracy: 0.9609\n",
      "Epoch 18, Batch 350, Loss: 0.2423, Accuracy: 0.9297\n",
      "Epoch 18, Batch 360, Loss: 0.2240, Accuracy: 0.9297\n",
      "Epoch 18, Batch 370, Loss: 0.1315, Accuracy: 0.9531\n",
      "Epoch 18, Batch 380, Loss: 0.2133, Accuracy: 0.9375\n",
      "Epoch 18, Batch 390, Loss: 0.1978, Accuracy: 0.9375\n",
      "Epoch 18, Batch 400, Loss: 0.1805, Accuracy: 0.9609\n",
      "Epoch 18, Batch 410, Loss: 0.2184, Accuracy: 0.9219\n",
      "Epoch 18, Batch 420, Loss: 0.1349, Accuracy: 0.9453\n",
      "Epoch 18, Batch 430, Loss: 0.1246, Accuracy: 0.9531\n",
      "Epoch 18, Batch 440, Loss: 0.2661, Accuracy: 0.9141\n",
      "Epoch 18, Batch 450, Loss: 0.2076, Accuracy: 0.9375\n",
      "Epoch 18, Batch 460, Loss: 0.1572, Accuracy: 0.9453\n",
      "Epoch 18, Train Accuracy: 0.9369, Test Loss: 0.2078\n",
      "Epoch 18, Validation Accuracy: 0.9330, Validation Loss: 0.2158\n",
      "Epoch 19, Batch 0, Loss: 0.2773, Accuracy: 0.9453\n",
      "Epoch 19, Batch 10, Loss: 0.1740, Accuracy: 0.9141\n",
      "Epoch 19, Batch 20, Loss: 0.1812, Accuracy: 0.9297\n",
      "Epoch 19, Batch 30, Loss: 0.2316, Accuracy: 0.9219\n",
      "Epoch 19, Batch 40, Loss: 0.2230, Accuracy: 0.9297\n",
      "Epoch 19, Batch 50, Loss: 0.1384, Accuracy: 0.9375\n",
      "Epoch 19, Batch 60, Loss: 0.2440, Accuracy: 0.9141\n",
      "Epoch 19, Batch 70, Loss: 0.2987, Accuracy: 0.9141\n",
      "Epoch 19, Batch 80, Loss: 0.2892, Accuracy: 0.8906\n",
      "Epoch 19, Batch 90, Loss: 0.2034, Accuracy: 0.9141\n",
      "Epoch 19, Batch 100, Loss: 0.1061, Accuracy: 0.9609\n",
      "Epoch 19, Batch 110, Loss: 0.1698, Accuracy: 0.9766\n",
      "Epoch 19, Batch 120, Loss: 0.1550, Accuracy: 0.9453\n",
      "Epoch 19, Batch 130, Loss: 0.1443, Accuracy: 0.9531\n",
      "Epoch 19, Batch 140, Loss: 0.2183, Accuracy: 0.9297\n",
      "Epoch 19, Batch 150, Loss: 0.1879, Accuracy: 0.9531\n",
      "Epoch 19, Batch 160, Loss: 0.1918, Accuracy: 0.9297\n",
      "Epoch 19, Batch 170, Loss: 0.1789, Accuracy: 0.9375\n",
      "Epoch 19, Batch 180, Loss: 0.2023, Accuracy: 0.9297\n",
      "Epoch 19, Batch 190, Loss: 0.1743, Accuracy: 0.9375\n",
      "Epoch 19, Batch 200, Loss: 0.2329, Accuracy: 0.9297\n",
      "Epoch 19, Batch 210, Loss: 0.1283, Accuracy: 0.9766\n",
      "Epoch 19, Batch 220, Loss: 0.2207, Accuracy: 0.9141\n",
      "Epoch 19, Batch 230, Loss: 0.2473, Accuracy: 0.9141\n",
      "Epoch 19, Batch 240, Loss: 0.2411, Accuracy: 0.9375\n",
      "Epoch 19, Batch 250, Loss: 0.2778, Accuracy: 0.9062\n",
      "Epoch 19, Batch 260, Loss: 0.1532, Accuracy: 0.9531\n",
      "Epoch 19, Batch 270, Loss: 0.1449, Accuracy: 0.9766\n",
      "Epoch 19, Batch 280, Loss: 0.1437, Accuracy: 0.9688\n",
      "Epoch 19, Batch 290, Loss: 0.2604, Accuracy: 0.9297\n",
      "Epoch 19, Batch 300, Loss: 0.3427, Accuracy: 0.9219\n",
      "Epoch 19, Batch 310, Loss: 0.1938, Accuracy: 0.9297\n",
      "Epoch 19, Batch 320, Loss: 0.3070, Accuracy: 0.8750\n",
      "Epoch 19, Batch 330, Loss: 0.1663, Accuracy: 0.9375\n",
      "Epoch 19, Batch 340, Loss: 0.3234, Accuracy: 0.8906\n",
      "Epoch 19, Batch 350, Loss: 0.1743, Accuracy: 0.9297\n",
      "Epoch 19, Batch 360, Loss: 0.2509, Accuracy: 0.9297\n",
      "Epoch 19, Batch 370, Loss: 0.3127, Accuracy: 0.8984\n",
      "Epoch 19, Batch 380, Loss: 0.1551, Accuracy: 0.9453\n",
      "Epoch 19, Batch 390, Loss: 0.3613, Accuracy: 0.9297\n",
      "Epoch 19, Batch 400, Loss: 0.1682, Accuracy: 0.9609\n",
      "Epoch 19, Batch 410, Loss: 0.2574, Accuracy: 0.9375\n",
      "Epoch 19, Batch 420, Loss: 0.2302, Accuracy: 0.9531\n",
      "Epoch 19, Batch 430, Loss: 0.2032, Accuracy: 0.9375\n",
      "Epoch 19, Batch 440, Loss: 0.3147, Accuracy: 0.8984\n",
      "Epoch 19, Batch 450, Loss: 0.2108, Accuracy: 0.9453\n",
      "Epoch 19, Batch 460, Loss: 0.2778, Accuracy: 0.9297\n",
      "Epoch 19, Train Accuracy: 0.9377, Test Loss: 0.2057\n",
      "Epoch 19, Validation Accuracy: 0.9461, Validation Loss: 0.1875\n",
      "Epoch 20, Batch 0, Loss: 0.2663, Accuracy: 0.9062\n",
      "Epoch 20, Batch 10, Loss: 0.3218, Accuracy: 0.8984\n",
      "Epoch 20, Batch 20, Loss: 0.2523, Accuracy: 0.9141\n",
      "Epoch 20, Batch 30, Loss: 0.1650, Accuracy: 0.9453\n",
      "Epoch 20, Batch 40, Loss: 0.1644, Accuracy: 0.9609\n",
      "Epoch 20, Batch 50, Loss: 0.2294, Accuracy: 0.9375\n",
      "Epoch 20, Batch 60, Loss: 0.2250, Accuracy: 0.9453\n",
      "Epoch 20, Batch 70, Loss: 0.1664, Accuracy: 0.9297\n",
      "Epoch 20, Batch 80, Loss: 0.1754, Accuracy: 0.9531\n",
      "Epoch 20, Batch 90, Loss: 0.0985, Accuracy: 0.9453\n",
      "Epoch 20, Batch 100, Loss: 0.3107, Accuracy: 0.8984\n",
      "Epoch 20, Batch 110, Loss: 0.1182, Accuracy: 0.9688\n",
      "Epoch 20, Batch 120, Loss: 0.1268, Accuracy: 0.9531\n",
      "Epoch 20, Batch 130, Loss: 0.2023, Accuracy: 0.9453\n",
      "Epoch 20, Batch 140, Loss: 0.1459, Accuracy: 0.9688\n",
      "Epoch 20, Batch 150, Loss: 0.1282, Accuracy: 0.9531\n",
      "Epoch 20, Batch 160, Loss: 0.2519, Accuracy: 0.9062\n",
      "Epoch 20, Batch 170, Loss: 0.2609, Accuracy: 0.9375\n",
      "Epoch 20, Batch 180, Loss: 0.1640, Accuracy: 0.9375\n",
      "Epoch 20, Batch 190, Loss: 0.1683, Accuracy: 0.9531\n",
      "Epoch 20, Batch 200, Loss: 0.1459, Accuracy: 0.9531\n",
      "Epoch 20, Batch 210, Loss: 0.1457, Accuracy: 0.9688\n",
      "Epoch 20, Batch 220, Loss: 0.1656, Accuracy: 0.9375\n",
      "Epoch 20, Batch 230, Loss: 0.0882, Accuracy: 0.9688\n",
      "Epoch 20, Batch 240, Loss: 0.1595, Accuracy: 0.9453\n",
      "Epoch 20, Batch 250, Loss: 0.2232, Accuracy: 0.9375\n",
      "Epoch 20, Batch 260, Loss: 0.2497, Accuracy: 0.9531\n",
      "Epoch 20, Batch 270, Loss: 0.2080, Accuracy: 0.9531\n",
      "Epoch 20, Batch 280, Loss: 0.0891, Accuracy: 0.9766\n",
      "Epoch 20, Batch 290, Loss: 0.2239, Accuracy: 0.9375\n",
      "Epoch 20, Batch 300, Loss: 0.2183, Accuracy: 0.9453\n",
      "Epoch 20, Batch 310, Loss: 0.1600, Accuracy: 0.9453\n",
      "Epoch 20, Batch 320, Loss: 0.2249, Accuracy: 0.9297\n",
      "Epoch 20, Batch 330, Loss: 0.1592, Accuracy: 0.9609\n",
      "Epoch 20, Batch 340, Loss: 0.2902, Accuracy: 0.8906\n",
      "Epoch 20, Batch 350, Loss: 0.1527, Accuracy: 0.9531\n",
      "Epoch 20, Batch 360, Loss: 0.1894, Accuracy: 0.9297\n",
      "Epoch 20, Batch 370, Loss: 0.0979, Accuracy: 0.9844\n",
      "Epoch 20, Batch 380, Loss: 0.1515, Accuracy: 0.9688\n",
      "Epoch 20, Batch 390, Loss: 0.2823, Accuracy: 0.8906\n",
      "Epoch 20, Batch 400, Loss: 0.2225, Accuracy: 0.9375\n",
      "Epoch 20, Batch 410, Loss: 0.3440, Accuracy: 0.8828\n",
      "Epoch 20, Batch 420, Loss: 0.1587, Accuracy: 0.9453\n",
      "Epoch 20, Batch 430, Loss: 0.1567, Accuracy: 0.9453\n",
      "Epoch 20, Batch 440, Loss: 0.1987, Accuracy: 0.9297\n",
      "Epoch 20, Batch 450, Loss: 0.1783, Accuracy: 0.9531\n",
      "Epoch 20, Batch 460, Loss: 0.1536, Accuracy: 0.9531\n",
      "Epoch 20, Train Accuracy: 0.9400, Test Loss: 0.1981\n",
      "Epoch 20, Validation Accuracy: 0.9425, Validation Loss: 0.2021\n",
      "Epoch 21, Batch 0, Loss: 0.1260, Accuracy: 0.9531\n",
      "Epoch 21, Batch 10, Loss: 0.2364, Accuracy: 0.9297\n",
      "Epoch 21, Batch 20, Loss: 0.2644, Accuracy: 0.9219\n",
      "Epoch 21, Batch 30, Loss: 0.2237, Accuracy: 0.9531\n",
      "Epoch 21, Batch 40, Loss: 0.1490, Accuracy: 0.9688\n",
      "Epoch 21, Batch 50, Loss: 0.1391, Accuracy: 0.9531\n",
      "Epoch 21, Batch 60, Loss: 0.3004, Accuracy: 0.9219\n",
      "Epoch 21, Batch 70, Loss: 0.1583, Accuracy: 0.9609\n",
      "Epoch 21, Batch 80, Loss: 0.2083, Accuracy: 0.9219\n",
      "Epoch 21, Batch 90, Loss: 0.0897, Accuracy: 0.9766\n",
      "Epoch 21, Batch 100, Loss: 0.1552, Accuracy: 0.9375\n",
      "Epoch 21, Batch 110, Loss: 0.1686, Accuracy: 0.9453\n",
      "Epoch 21, Batch 120, Loss: 0.2386, Accuracy: 0.9375\n",
      "Epoch 21, Batch 130, Loss: 0.1807, Accuracy: 0.9531\n",
      "Epoch 21, Batch 140, Loss: 0.2229, Accuracy: 0.9375\n",
      "Epoch 21, Batch 150, Loss: 0.1577, Accuracy: 0.9375\n",
      "Epoch 21, Batch 160, Loss: 0.1000, Accuracy: 0.9688\n",
      "Epoch 21, Batch 170, Loss: 0.1628, Accuracy: 0.9219\n",
      "Epoch 21, Batch 180, Loss: 0.1755, Accuracy: 0.9688\n",
      "Epoch 21, Batch 190, Loss: 0.1808, Accuracy: 0.9297\n",
      "Epoch 21, Batch 200, Loss: 0.1336, Accuracy: 0.9688\n",
      "Epoch 21, Batch 210, Loss: 0.1223, Accuracy: 0.9688\n",
      "Epoch 21, Batch 220, Loss: 0.3116, Accuracy: 0.9141\n",
      "Epoch 21, Batch 230, Loss: 0.2849, Accuracy: 0.9141\n",
      "Epoch 21, Batch 240, Loss: 0.2484, Accuracy: 0.9375\n",
      "Epoch 21, Batch 250, Loss: 0.1860, Accuracy: 0.9297\n",
      "Epoch 21, Batch 260, Loss: 0.1013, Accuracy: 0.9844\n",
      "Epoch 21, Batch 270, Loss: 0.1530, Accuracy: 0.9531\n",
      "Epoch 21, Batch 280, Loss: 0.2037, Accuracy: 0.9297\n",
      "Epoch 21, Batch 290, Loss: 0.1716, Accuracy: 0.9609\n",
      "Epoch 21, Batch 300, Loss: 0.1806, Accuracy: 0.9297\n",
      "Epoch 21, Batch 310, Loss: 0.3209, Accuracy: 0.9062\n",
      "Epoch 21, Batch 320, Loss: 0.1595, Accuracy: 0.9531\n",
      "Epoch 21, Batch 330, Loss: 0.1787, Accuracy: 0.9453\n",
      "Epoch 21, Batch 340, Loss: 0.1664, Accuracy: 0.9453\n",
      "Epoch 21, Batch 350, Loss: 0.3498, Accuracy: 0.9219\n",
      "Epoch 21, Batch 360, Loss: 0.2029, Accuracy: 0.9375\n",
      "Epoch 21, Batch 370, Loss: 0.1478, Accuracy: 0.9531\n",
      "Epoch 21, Batch 380, Loss: 0.2070, Accuracy: 0.9219\n",
      "Epoch 21, Batch 390, Loss: 0.2212, Accuracy: 0.9297\n",
      "Epoch 21, Batch 400, Loss: 0.2455, Accuracy: 0.9297\n",
      "Epoch 21, Batch 410, Loss: 0.2136, Accuracy: 0.9375\n",
      "Epoch 21, Batch 420, Loss: 0.3021, Accuracy: 0.8984\n",
      "Epoch 21, Batch 430, Loss: 0.2923, Accuracy: 0.9375\n",
      "Epoch 21, Batch 440, Loss: 0.2147, Accuracy: 0.9141\n",
      "Epoch 21, Batch 450, Loss: 0.1947, Accuracy: 0.9141\n",
      "Epoch 21, Batch 460, Loss: 0.1417, Accuracy: 0.9375\n",
      "Epoch 21, Train Accuracy: 0.9412, Test Loss: 0.1913\n",
      "Epoch 21, Validation Accuracy: 0.9406, Validation Loss: 0.1929\n",
      "Epoch 22, Batch 0, Loss: 0.1595, Accuracy: 0.9297\n",
      "Epoch 22, Batch 10, Loss: 0.1580, Accuracy: 0.9531\n",
      "Epoch 22, Batch 20, Loss: 0.2182, Accuracy: 0.9375\n",
      "Epoch 22, Batch 30, Loss: 0.1831, Accuracy: 0.9375\n",
      "Epoch 22, Batch 40, Loss: 0.2765, Accuracy: 0.9219\n",
      "Epoch 22, Batch 50, Loss: 0.1384, Accuracy: 0.9688\n",
      "Epoch 22, Batch 60, Loss: 0.1934, Accuracy: 0.9531\n",
      "Epoch 22, Batch 70, Loss: 0.2353, Accuracy: 0.9609\n",
      "Epoch 22, Batch 80, Loss: 0.0890, Accuracy: 0.9609\n",
      "Epoch 22, Batch 90, Loss: 0.1546, Accuracy: 0.9609\n",
      "Epoch 22, Batch 100, Loss: 0.2706, Accuracy: 0.8906\n",
      "Epoch 22, Batch 110, Loss: 0.1198, Accuracy: 0.9609\n",
      "Epoch 22, Batch 120, Loss: 0.1874, Accuracy: 0.9375\n",
      "Epoch 22, Batch 130, Loss: 0.2959, Accuracy: 0.9141\n",
      "Epoch 22, Batch 140, Loss: 0.3199, Accuracy: 0.9219\n",
      "Epoch 22, Batch 150, Loss: 0.3577, Accuracy: 0.9062\n",
      "Epoch 22, Batch 160, Loss: 0.2390, Accuracy: 0.9453\n",
      "Epoch 22, Batch 170, Loss: 0.2757, Accuracy: 0.9219\n",
      "Epoch 22, Batch 180, Loss: 0.1855, Accuracy: 0.9688\n",
      "Epoch 22, Batch 190, Loss: 0.2821, Accuracy: 0.9297\n",
      "Epoch 22, Batch 200, Loss: 0.1363, Accuracy: 0.9609\n",
      "Epoch 22, Batch 210, Loss: 0.1542, Accuracy: 0.9453\n",
      "Epoch 22, Batch 220, Loss: 0.0610, Accuracy: 0.9688\n",
      "Epoch 22, Batch 230, Loss: 0.1836, Accuracy: 0.9375\n",
      "Epoch 22, Batch 240, Loss: 0.2562, Accuracy: 0.9219\n",
      "Epoch 22, Batch 250, Loss: 0.1594, Accuracy: 0.9531\n",
      "Epoch 22, Batch 260, Loss: 0.2285, Accuracy: 0.9141\n",
      "Epoch 22, Batch 270, Loss: 0.1988, Accuracy: 0.9453\n",
      "Epoch 22, Batch 280, Loss: 0.1861, Accuracy: 0.9375\n",
      "Epoch 22, Batch 290, Loss: 0.1791, Accuracy: 0.9219\n",
      "Epoch 22, Batch 300, Loss: 0.2219, Accuracy: 0.9375\n",
      "Epoch 22, Batch 310, Loss: 0.3092, Accuracy: 0.8984\n",
      "Epoch 22, Batch 320, Loss: 0.2128, Accuracy: 0.9453\n",
      "Epoch 22, Batch 330, Loss: 0.2257, Accuracy: 0.9453\n",
      "Epoch 22, Batch 340, Loss: 0.2432, Accuracy: 0.9375\n",
      "Epoch 22, Batch 350, Loss: 0.2971, Accuracy: 0.9375\n",
      "Epoch 22, Batch 360, Loss: 0.2219, Accuracy: 0.9297\n",
      "Epoch 22, Batch 370, Loss: 0.1092, Accuracy: 0.9609\n",
      "Epoch 22, Batch 380, Loss: 0.2192, Accuracy: 0.9453\n",
      "Epoch 22, Batch 390, Loss: 0.1623, Accuracy: 0.9531\n",
      "Epoch 22, Batch 400, Loss: 0.2197, Accuracy: 0.9297\n",
      "Epoch 22, Batch 410, Loss: 0.1115, Accuracy: 0.9844\n",
      "Epoch 22, Batch 420, Loss: 0.1818, Accuracy: 0.9531\n",
      "Epoch 22, Batch 430, Loss: 0.1410, Accuracy: 0.9453\n",
      "Epoch 22, Batch 440, Loss: 0.2264, Accuracy: 0.9375\n",
      "Epoch 22, Batch 450, Loss: 0.1267, Accuracy: 0.9688\n",
      "Epoch 22, Batch 460, Loss: 0.1356, Accuracy: 0.9609\n",
      "Epoch 22, Train Accuracy: 0.9428, Test Loss: 0.1878\n",
      "Epoch 22, Validation Accuracy: 0.9440, Validation Loss: 0.1878\n",
      "Epoch 23, Batch 0, Loss: 0.1339, Accuracy: 0.9766\n",
      "Epoch 23, Batch 10, Loss: 0.1920, Accuracy: 0.9609\n",
      "Epoch 23, Batch 20, Loss: 0.1461, Accuracy: 0.9531\n",
      "Epoch 23, Batch 30, Loss: 0.1570, Accuracy: 0.9375\n",
      "Epoch 23, Batch 40, Loss: 0.1453, Accuracy: 0.9453\n",
      "Epoch 23, Batch 50, Loss: 0.2043, Accuracy: 0.9531\n",
      "Epoch 23, Batch 60, Loss: 0.2270, Accuracy: 0.9141\n",
      "Epoch 23, Batch 70, Loss: 0.1617, Accuracy: 0.9609\n",
      "Epoch 23, Batch 80, Loss: 0.2167, Accuracy: 0.9375\n",
      "Epoch 23, Batch 90, Loss: 0.1962, Accuracy: 0.9531\n",
      "Epoch 23, Batch 100, Loss: 0.1804, Accuracy: 0.9375\n",
      "Epoch 23, Batch 110, Loss: 0.3308, Accuracy: 0.9297\n",
      "Epoch 23, Batch 120, Loss: 0.1531, Accuracy: 0.9375\n",
      "Epoch 23, Batch 130, Loss: 0.1273, Accuracy: 0.9609\n",
      "Epoch 23, Batch 140, Loss: 0.3600, Accuracy: 0.9531\n",
      "Epoch 23, Batch 150, Loss: 0.0901, Accuracy: 0.9922\n",
      "Epoch 23, Batch 160, Loss: 0.2044, Accuracy: 0.9375\n",
      "Epoch 23, Batch 170, Loss: 0.1916, Accuracy: 0.9297\n",
      "Epoch 23, Batch 180, Loss: 0.1631, Accuracy: 0.9453\n",
      "Epoch 23, Batch 190, Loss: 0.1756, Accuracy: 0.9375\n",
      "Epoch 23, Batch 200, Loss: 0.2143, Accuracy: 0.9375\n",
      "Epoch 23, Batch 210, Loss: 0.1737, Accuracy: 0.9375\n",
      "Epoch 23, Batch 220, Loss: 0.1186, Accuracy: 0.9688\n",
      "Epoch 23, Batch 230, Loss: 0.1590, Accuracy: 0.9453\n",
      "Epoch 23, Batch 240, Loss: 0.1089, Accuracy: 0.9609\n",
      "Epoch 23, Batch 250, Loss: 0.2387, Accuracy: 0.9141\n",
      "Epoch 23, Batch 260, Loss: 0.2596, Accuracy: 0.9141\n",
      "Epoch 23, Batch 270, Loss: 0.2444, Accuracy: 0.9375\n",
      "Epoch 23, Batch 280, Loss: 0.2241, Accuracy: 0.9375\n",
      "Epoch 23, Batch 290, Loss: 0.1515, Accuracy: 0.9531\n",
      "Epoch 23, Batch 300, Loss: 0.1598, Accuracy: 0.9531\n",
      "Epoch 23, Batch 310, Loss: 0.2257, Accuracy: 0.9062\n",
      "Epoch 23, Batch 320, Loss: 0.1538, Accuracy: 0.9609\n",
      "Epoch 23, Batch 330, Loss: 0.1296, Accuracy: 0.9531\n",
      "Epoch 23, Batch 340, Loss: 0.1848, Accuracy: 0.9688\n",
      "Epoch 23, Batch 350, Loss: 0.2254, Accuracy: 0.9453\n",
      "Epoch 23, Batch 360, Loss: 0.2027, Accuracy: 0.9375\n",
      "Epoch 23, Batch 370, Loss: 0.1061, Accuracy: 0.9531\n",
      "Epoch 23, Batch 380, Loss: 0.1054, Accuracy: 0.9688\n",
      "Epoch 23, Batch 390, Loss: 0.3453, Accuracy: 0.8828\n",
      "Epoch 23, Batch 400, Loss: 0.0994, Accuracy: 0.9688\n",
      "Epoch 23, Batch 410, Loss: 0.0957, Accuracy: 0.9609\n",
      "Epoch 23, Batch 420, Loss: 0.2161, Accuracy: 0.9297\n",
      "Epoch 23, Batch 430, Loss: 0.1619, Accuracy: 0.9297\n",
      "Epoch 23, Batch 440, Loss: 0.1681, Accuracy: 0.9688\n",
      "Epoch 23, Batch 450, Loss: 0.1742, Accuracy: 0.9297\n",
      "Epoch 23, Batch 460, Loss: 0.1183, Accuracy: 0.9531\n",
      "Epoch 23, Train Accuracy: 0.9432, Test Loss: 0.1839\n",
      "Epoch 23, Validation Accuracy: 0.9439, Validation Loss: 0.1868\n",
      "Epoch 24, Batch 0, Loss: 0.1582, Accuracy: 0.9531\n",
      "Epoch 24, Batch 10, Loss: 0.3006, Accuracy: 0.9297\n",
      "Epoch 24, Batch 20, Loss: 0.1835, Accuracy: 0.9375\n",
      "Epoch 24, Batch 30, Loss: 0.1398, Accuracy: 0.9688\n",
      "Epoch 24, Batch 40, Loss: 0.2817, Accuracy: 0.9141\n",
      "Epoch 24, Batch 50, Loss: 0.1647, Accuracy: 0.9375\n",
      "Epoch 24, Batch 60, Loss: 0.1440, Accuracy: 0.9688\n",
      "Epoch 24, Batch 70, Loss: 0.0932, Accuracy: 0.9688\n",
      "Epoch 24, Batch 80, Loss: 0.1778, Accuracy: 0.9531\n",
      "Epoch 24, Batch 90, Loss: 0.1642, Accuracy: 0.9531\n",
      "Epoch 24, Batch 100, Loss: 0.1800, Accuracy: 0.9453\n",
      "Epoch 24, Batch 110, Loss: 0.1937, Accuracy: 0.9219\n",
      "Epoch 24, Batch 120, Loss: 0.1770, Accuracy: 0.9297\n",
      "Epoch 24, Batch 130, Loss: 0.1262, Accuracy: 0.9688\n",
      "Epoch 24, Batch 140, Loss: 0.1340, Accuracy: 0.9531\n",
      "Epoch 24, Batch 150, Loss: 0.1785, Accuracy: 0.9297\n",
      "Epoch 24, Batch 160, Loss: 0.1448, Accuracy: 0.9609\n",
      "Epoch 24, Batch 170, Loss: 0.1402, Accuracy: 0.9453\n",
      "Epoch 24, Batch 180, Loss: 0.1153, Accuracy: 0.9688\n",
      "Epoch 24, Batch 190, Loss: 0.1826, Accuracy: 0.9531\n",
      "Epoch 24, Batch 200, Loss: 0.3568, Accuracy: 0.9531\n",
      "Epoch 24, Batch 210, Loss: 0.2838, Accuracy: 0.9141\n",
      "Epoch 24, Batch 220, Loss: 0.2009, Accuracy: 0.9375\n",
      "Epoch 24, Batch 230, Loss: 0.1670, Accuracy: 0.9375\n",
      "Epoch 24, Batch 240, Loss: 0.1977, Accuracy: 0.9375\n",
      "Epoch 24, Batch 250, Loss: 0.2419, Accuracy: 0.9141\n",
      "Epoch 24, Batch 260, Loss: 0.2938, Accuracy: 0.9141\n",
      "Epoch 24, Batch 270, Loss: 0.2288, Accuracy: 0.9297\n",
      "Epoch 24, Batch 280, Loss: 0.1954, Accuracy: 0.9531\n",
      "Epoch 24, Batch 290, Loss: 0.3033, Accuracy: 0.8750\n",
      "Epoch 24, Batch 300, Loss: 0.1668, Accuracy: 0.9531\n",
      "Epoch 24, Batch 310, Loss: 0.1565, Accuracy: 0.9531\n",
      "Epoch 24, Batch 320, Loss: 0.3172, Accuracy: 0.9141\n",
      "Epoch 24, Batch 330, Loss: 0.1445, Accuracy: 0.9688\n",
      "Epoch 24, Batch 340, Loss: 0.1556, Accuracy: 0.9297\n",
      "Epoch 24, Batch 350, Loss: 0.1712, Accuracy: 0.9219\n",
      "Epoch 24, Batch 360, Loss: 0.1727, Accuracy: 0.9609\n",
      "Epoch 24, Batch 370, Loss: 0.2690, Accuracy: 0.9219\n",
      "Epoch 24, Batch 380, Loss: 0.3111, Accuracy: 0.9062\n",
      "Epoch 24, Batch 390, Loss: 0.1905, Accuracy: 0.9453\n",
      "Epoch 24, Batch 400, Loss: 0.0946, Accuracy: 0.9766\n",
      "Epoch 24, Batch 410, Loss: 0.1906, Accuracy: 0.9453\n",
      "Epoch 24, Batch 420, Loss: 0.1552, Accuracy: 0.9609\n",
      "Epoch 24, Batch 430, Loss: 0.1336, Accuracy: 0.9453\n",
      "Epoch 24, Batch 440, Loss: 0.2192, Accuracy: 0.9375\n",
      "Epoch 24, Batch 450, Loss: 0.1901, Accuracy: 0.9688\n",
      "Epoch 24, Batch 460, Loss: 0.1061, Accuracy: 0.9688\n",
      "Epoch 24, Train Accuracy: 0.9449, Test Loss: 0.1809\n",
      "Epoch 24, Validation Accuracy: 0.9464, Validation Loss: 0.1757\n",
      "Epoch 25, Batch 0, Loss: 0.1524, Accuracy: 0.9609\n",
      "Epoch 25, Batch 10, Loss: 0.1368, Accuracy: 0.9609\n",
      "Epoch 25, Batch 20, Loss: 0.1578, Accuracy: 0.9375\n",
      "Epoch 25, Batch 30, Loss: 0.1241, Accuracy: 0.9453\n",
      "Epoch 25, Batch 40, Loss: 0.1094, Accuracy: 0.9766\n",
      "Epoch 25, Batch 50, Loss: 0.4098, Accuracy: 0.9141\n",
      "Epoch 25, Batch 60, Loss: 0.1723, Accuracy: 0.9453\n",
      "Epoch 25, Batch 70, Loss: 0.1039, Accuracy: 0.9609\n",
      "Epoch 25, Batch 80, Loss: 0.1923, Accuracy: 0.9609\n",
      "Epoch 25, Batch 90, Loss: 0.1110, Accuracy: 0.9688\n",
      "Epoch 25, Batch 100, Loss: 0.1311, Accuracy: 0.9531\n",
      "Epoch 25, Batch 110, Loss: 0.1717, Accuracy: 0.9375\n",
      "Epoch 25, Batch 120, Loss: 0.0961, Accuracy: 0.9766\n",
      "Epoch 25, Batch 130, Loss: 0.0927, Accuracy: 0.9844\n",
      "Epoch 25, Batch 140, Loss: 0.1116, Accuracy: 0.9531\n",
      "Epoch 25, Batch 150, Loss: 0.1993, Accuracy: 0.9453\n",
      "Epoch 25, Batch 160, Loss: 0.1681, Accuracy: 0.9453\n",
      "Epoch 25, Batch 170, Loss: 0.1668, Accuracy: 0.9375\n",
      "Epoch 25, Batch 180, Loss: 0.1080, Accuracy: 0.9766\n",
      "Epoch 25, Batch 190, Loss: 0.2404, Accuracy: 0.9141\n",
      "Epoch 25, Batch 200, Loss: 0.1818, Accuracy: 0.9609\n",
      "Epoch 25, Batch 210, Loss: 0.1758, Accuracy: 0.9453\n",
      "Epoch 25, Batch 220, Loss: 0.2031, Accuracy: 0.9141\n",
      "Epoch 25, Batch 230, Loss: 0.1761, Accuracy: 0.9453\n",
      "Epoch 25, Batch 240, Loss: 0.1236, Accuracy: 0.9688\n",
      "Epoch 25, Batch 250, Loss: 0.1809, Accuracy: 0.9609\n",
      "Epoch 25, Batch 260, Loss: 0.1667, Accuracy: 0.9531\n",
      "Epoch 25, Batch 270, Loss: 0.1527, Accuracy: 0.9531\n",
      "Epoch 25, Batch 280, Loss: 0.2492, Accuracy: 0.9297\n",
      "Epoch 25, Batch 290, Loss: 0.1670, Accuracy: 0.9531\n",
      "Epoch 25, Batch 300, Loss: 0.1078, Accuracy: 0.9688\n",
      "Epoch 25, Batch 310, Loss: 0.0960, Accuracy: 0.9609\n",
      "Epoch 25, Batch 320, Loss: 0.1522, Accuracy: 0.9453\n",
      "Epoch 25, Batch 330, Loss: 0.3169, Accuracy: 0.9453\n",
      "Epoch 25, Batch 340, Loss: 0.2501, Accuracy: 0.9297\n",
      "Epoch 25, Batch 350, Loss: 0.1756, Accuracy: 0.9375\n",
      "Epoch 25, Batch 360, Loss: 0.1787, Accuracy: 0.9453\n",
      "Epoch 25, Batch 370, Loss: 0.2124, Accuracy: 0.9531\n",
      "Epoch 25, Batch 380, Loss: 0.2569, Accuracy: 0.9453\n",
      "Epoch 25, Batch 390, Loss: 0.1068, Accuracy: 0.9766\n",
      "Epoch 25, Batch 400, Loss: 0.1717, Accuracy: 0.9375\n",
      "Epoch 25, Batch 410, Loss: 0.1969, Accuracy: 0.9141\n",
      "Epoch 25, Batch 420, Loss: 0.1748, Accuracy: 0.9453\n",
      "Epoch 25, Batch 430, Loss: 0.1594, Accuracy: 0.9531\n",
      "Epoch 25, Batch 440, Loss: 0.1909, Accuracy: 0.9688\n",
      "Epoch 25, Batch 450, Loss: 0.1819, Accuracy: 0.9453\n",
      "Epoch 25, Batch 460, Loss: 0.1626, Accuracy: 0.9453\n",
      "Epoch 25, Train Accuracy: 0.9464, Test Loss: 0.1749\n",
      "Epoch 25, Validation Accuracy: 0.9443, Validation Loss: 0.1899\n",
      "Epoch 26, Batch 0, Loss: 0.2774, Accuracy: 0.9375\n",
      "Epoch 26, Batch 10, Loss: 0.2191, Accuracy: 0.9062\n",
      "Epoch 26, Batch 20, Loss: 0.1140, Accuracy: 0.9688\n",
      "Epoch 26, Batch 30, Loss: 0.2000, Accuracy: 0.9297\n",
      "Epoch 26, Batch 40, Loss: 0.1659, Accuracy: 0.9375\n",
      "Epoch 26, Batch 50, Loss: 0.1188, Accuracy: 0.9766\n",
      "Epoch 26, Batch 60, Loss: 0.1577, Accuracy: 0.9609\n",
      "Epoch 26, Batch 70, Loss: 0.2638, Accuracy: 0.9141\n",
      "Epoch 26, Batch 80, Loss: 0.1878, Accuracy: 0.9297\n",
      "Epoch 26, Batch 90, Loss: 0.1511, Accuracy: 0.9453\n",
      "Epoch 26, Batch 100, Loss: 0.2473, Accuracy: 0.9297\n",
      "Epoch 26, Batch 110, Loss: 0.2605, Accuracy: 0.9297\n",
      "Epoch 26, Batch 120, Loss: 0.1332, Accuracy: 0.9688\n",
      "Epoch 26, Batch 130, Loss: 0.0859, Accuracy: 0.9688\n",
      "Epoch 26, Batch 140, Loss: 0.1235, Accuracy: 0.9609\n",
      "Epoch 26, Batch 150, Loss: 0.1610, Accuracy: 0.9609\n",
      "Epoch 26, Batch 160, Loss: 0.1921, Accuracy: 0.9141\n",
      "Epoch 26, Batch 170, Loss: 0.2390, Accuracy: 0.9609\n",
      "Epoch 26, Batch 180, Loss: 0.1243, Accuracy: 0.9531\n",
      "Epoch 26, Batch 190, Loss: 0.1908, Accuracy: 0.9453\n",
      "Epoch 26, Batch 200, Loss: 0.1852, Accuracy: 0.9297\n",
      "Epoch 26, Batch 210, Loss: 0.1779, Accuracy: 0.9297\n",
      "Epoch 26, Batch 220, Loss: 0.1113, Accuracy: 0.9688\n",
      "Epoch 26, Batch 230, Loss: 0.1445, Accuracy: 0.9375\n",
      "Epoch 26, Batch 240, Loss: 0.2552, Accuracy: 0.9453\n",
      "Epoch 26, Batch 250, Loss: 0.1641, Accuracy: 0.9453\n",
      "Epoch 26, Batch 260, Loss: 0.1199, Accuracy: 0.9766\n",
      "Epoch 26, Batch 270, Loss: 0.1264, Accuracy: 0.9609\n",
      "Epoch 26, Batch 280, Loss: 0.1186, Accuracy: 0.9531\n",
      "Epoch 26, Batch 290, Loss: 0.1214, Accuracy: 0.9531\n",
      "Epoch 26, Batch 300, Loss: 0.1868, Accuracy: 0.9453\n",
      "Epoch 26, Batch 310, Loss: 0.0950, Accuracy: 0.9688\n",
      "Epoch 26, Batch 320, Loss: 0.1624, Accuracy: 0.9531\n",
      "Epoch 26, Batch 330, Loss: 0.1713, Accuracy: 0.9531\n",
      "Epoch 26, Batch 340, Loss: 0.0879, Accuracy: 0.9766\n",
      "Epoch 26, Batch 350, Loss: 0.1398, Accuracy: 0.9688\n",
      "Epoch 26, Batch 360, Loss: 0.1623, Accuracy: 0.9453\n",
      "Epoch 26, Batch 370, Loss: 0.2334, Accuracy: 0.9453\n",
      "Epoch 26, Batch 380, Loss: 0.1595, Accuracy: 0.9297\n",
      "Epoch 26, Batch 390, Loss: 0.1501, Accuracy: 0.9453\n",
      "Epoch 26, Batch 400, Loss: 0.1966, Accuracy: 0.9375\n",
      "Epoch 26, Batch 410, Loss: 0.2898, Accuracy: 0.8906\n",
      "Epoch 26, Batch 420, Loss: 0.1292, Accuracy: 0.9766\n",
      "Epoch 26, Batch 430, Loss: 0.0802, Accuracy: 0.9766\n",
      "Epoch 26, Batch 440, Loss: 0.0556, Accuracy: 0.9922\n",
      "Epoch 26, Batch 450, Loss: 0.1732, Accuracy: 0.9609\n",
      "Epoch 26, Batch 460, Loss: 0.2098, Accuracy: 0.9219\n",
      "Epoch 26, Train Accuracy: 0.9467, Test Loss: 0.1744\n",
      "Epoch 26, Validation Accuracy: 0.9474, Validation Loss: 0.1809\n",
      "Epoch 27, Batch 0, Loss: 0.2231, Accuracy: 0.9375\n",
      "Epoch 27, Batch 10, Loss: 0.1363, Accuracy: 0.9609\n",
      "Epoch 27, Batch 20, Loss: 0.0797, Accuracy: 0.9766\n",
      "Epoch 27, Batch 30, Loss: 0.0756, Accuracy: 0.9844\n",
      "Epoch 27, Batch 40, Loss: 0.1414, Accuracy: 0.9609\n",
      "Epoch 27, Batch 50, Loss: 0.1675, Accuracy: 0.9531\n",
      "Epoch 27, Batch 60, Loss: 0.1231, Accuracy: 0.9609\n",
      "Epoch 27, Batch 70, Loss: 0.1351, Accuracy: 0.9531\n",
      "Epoch 27, Batch 80, Loss: 0.2535, Accuracy: 0.9062\n",
      "Epoch 27, Batch 90, Loss: 0.1729, Accuracy: 0.9453\n",
      "Epoch 27, Batch 100, Loss: 0.1710, Accuracy: 0.9297\n",
      "Epoch 27, Batch 110, Loss: 0.1296, Accuracy: 0.9453\n",
      "Epoch 27, Batch 120, Loss: 0.1650, Accuracy: 0.9297\n",
      "Epoch 27, Batch 130, Loss: 0.1028, Accuracy: 0.9609\n",
      "Epoch 27, Batch 140, Loss: 0.2586, Accuracy: 0.9141\n",
      "Epoch 27, Batch 150, Loss: 0.2441, Accuracy: 0.9219\n",
      "Epoch 27, Batch 160, Loss: 0.1857, Accuracy: 0.9609\n",
      "Epoch 27, Batch 170, Loss: 0.1392, Accuracy: 0.9531\n",
      "Epoch 27, Batch 180, Loss: 0.1866, Accuracy: 0.9375\n",
      "Epoch 27, Batch 190, Loss: 0.2314, Accuracy: 0.9297\n",
      "Epoch 27, Batch 200, Loss: 0.1843, Accuracy: 0.9453\n",
      "Epoch 27, Batch 210, Loss: 0.1668, Accuracy: 0.9531\n",
      "Epoch 27, Batch 220, Loss: 0.1397, Accuracy: 0.9375\n",
      "Epoch 27, Batch 230, Loss: 0.0699, Accuracy: 0.9766\n",
      "Epoch 27, Batch 240, Loss: 0.1744, Accuracy: 0.9375\n",
      "Epoch 27, Batch 250, Loss: 0.3232, Accuracy: 0.9219\n",
      "Epoch 27, Batch 260, Loss: 0.1771, Accuracy: 0.9531\n",
      "Epoch 27, Batch 270, Loss: 0.1152, Accuracy: 0.9688\n",
      "Epoch 27, Batch 280, Loss: 0.1649, Accuracy: 0.9531\n",
      "Epoch 27, Batch 290, Loss: 0.1362, Accuracy: 0.9531\n",
      "Epoch 27, Batch 300, Loss: 0.2008, Accuracy: 0.9531\n",
      "Epoch 27, Batch 310, Loss: 0.0668, Accuracy: 0.9844\n",
      "Epoch 27, Batch 320, Loss: 0.2546, Accuracy: 0.9219\n",
      "Epoch 27, Batch 330, Loss: 0.1157, Accuracy: 0.9688\n",
      "Epoch 27, Batch 340, Loss: 0.0675, Accuracy: 0.9844\n",
      "Epoch 27, Batch 350, Loss: 0.2200, Accuracy: 0.9297\n",
      "Epoch 27, Batch 360, Loss: 0.1191, Accuracy: 0.9531\n",
      "Epoch 27, Batch 370, Loss: 0.1615, Accuracy: 0.9453\n",
      "Epoch 27, Batch 380, Loss: 0.2232, Accuracy: 0.9297\n",
      "Epoch 27, Batch 390, Loss: 0.2769, Accuracy: 0.9375\n",
      "Epoch 27, Batch 400, Loss: 0.0922, Accuracy: 0.9766\n",
      "Epoch 27, Batch 410, Loss: 0.2728, Accuracy: 0.9219\n",
      "Epoch 27, Batch 420, Loss: 0.2206, Accuracy: 0.9453\n",
      "Epoch 27, Batch 430, Loss: 0.1282, Accuracy: 0.9375\n",
      "Epoch 27, Batch 440, Loss: 0.1034, Accuracy: 0.9766\n",
      "Epoch 27, Batch 450, Loss: 0.0888, Accuracy: 0.9844\n",
      "Epoch 27, Batch 460, Loss: 0.3159, Accuracy: 0.9375\n",
      "Epoch 27, Train Accuracy: 0.9498, Test Loss: 0.1671\n",
      "Epoch 27, Validation Accuracy: 0.9456, Validation Loss: 0.1816\n",
      "Epoch 28, Batch 0, Loss: 0.3178, Accuracy: 0.9062\n",
      "Epoch 28, Batch 10, Loss: 0.2339, Accuracy: 0.9453\n",
      "Epoch 28, Batch 20, Loss: 0.1813, Accuracy: 0.9375\n",
      "Epoch 28, Batch 30, Loss: 0.1400, Accuracy: 0.9688\n",
      "Epoch 28, Batch 40, Loss: 0.1422, Accuracy: 0.9453\n",
      "Epoch 28, Batch 50, Loss: 0.1135, Accuracy: 0.9531\n",
      "Epoch 28, Batch 60, Loss: 0.1801, Accuracy: 0.9531\n",
      "Epoch 28, Batch 70, Loss: 0.0753, Accuracy: 0.9766\n",
      "Epoch 28, Batch 80, Loss: 0.2170, Accuracy: 0.9141\n",
      "Epoch 28, Batch 90, Loss: 0.1151, Accuracy: 0.9766\n",
      "Epoch 28, Batch 100, Loss: 0.1401, Accuracy: 0.9766\n",
      "Epoch 28, Batch 110, Loss: 0.2056, Accuracy: 0.9375\n",
      "Epoch 28, Batch 120, Loss: 0.1938, Accuracy: 0.9453\n",
      "Epoch 28, Batch 130, Loss: 0.0643, Accuracy: 0.9922\n",
      "Epoch 28, Batch 140, Loss: 0.2952, Accuracy: 0.9062\n",
      "Epoch 28, Batch 150, Loss: 0.1422, Accuracy: 0.9375\n",
      "Epoch 28, Batch 160, Loss: 0.1500, Accuracy: 0.9531\n",
      "Epoch 28, Batch 170, Loss: 0.3682, Accuracy: 0.9375\n",
      "Epoch 28, Batch 180, Loss: 0.1810, Accuracy: 0.9453\n",
      "Epoch 28, Batch 190, Loss: 0.2514, Accuracy: 0.9062\n",
      "Epoch 28, Batch 200, Loss: 0.1162, Accuracy: 0.9688\n",
      "Epoch 28, Batch 210, Loss: 0.1544, Accuracy: 0.9531\n",
      "Epoch 28, Batch 220, Loss: 0.1501, Accuracy: 0.9141\n",
      "Epoch 28, Batch 230, Loss: 0.1069, Accuracy: 0.9688\n",
      "Epoch 28, Batch 240, Loss: 0.1440, Accuracy: 0.9453\n",
      "Epoch 28, Batch 250, Loss: 0.1444, Accuracy: 0.9609\n",
      "Epoch 28, Batch 260, Loss: 0.2091, Accuracy: 0.9219\n",
      "Epoch 28, Batch 270, Loss: 0.1380, Accuracy: 0.9375\n",
      "Epoch 28, Batch 280, Loss: 0.1615, Accuracy: 0.9297\n",
      "Epoch 28, Batch 290, Loss: 0.3136, Accuracy: 0.9375\n",
      "Epoch 28, Batch 300, Loss: 0.1241, Accuracy: 0.9688\n",
      "Epoch 28, Batch 310, Loss: 0.1458, Accuracy: 0.9531\n",
      "Epoch 28, Batch 320, Loss: 0.1516, Accuracy: 0.9453\n",
      "Epoch 28, Batch 330, Loss: 0.1547, Accuracy: 0.9453\n",
      "Epoch 28, Batch 340, Loss: 0.1129, Accuracy: 0.9688\n",
      "Epoch 28, Batch 350, Loss: 0.1198, Accuracy: 0.9688\n",
      "Epoch 28, Batch 360, Loss: 0.2159, Accuracy: 0.9297\n",
      "Epoch 28, Batch 370, Loss: 0.2040, Accuracy: 0.9453\n",
      "Epoch 28, Batch 380, Loss: 0.1292, Accuracy: 0.9531\n",
      "Epoch 28, Batch 390, Loss: 0.1290, Accuracy: 0.9531\n",
      "Epoch 28, Batch 400, Loss: 0.1444, Accuracy: 0.9531\n",
      "Epoch 28, Batch 410, Loss: 0.2379, Accuracy: 0.9531\n",
      "Epoch 28, Batch 420, Loss: 0.1267, Accuracy: 0.9688\n",
      "Epoch 28, Batch 430, Loss: 0.1818, Accuracy: 0.9297\n",
      "Epoch 28, Batch 440, Loss: 0.1635, Accuracy: 0.9453\n",
      "Epoch 28, Batch 450, Loss: 0.1711, Accuracy: 0.9609\n",
      "Epoch 28, Batch 460, Loss: 0.1224, Accuracy: 0.9531\n",
      "Epoch 28, Train Accuracy: 0.9489, Test Loss: 0.1674\n",
      "Epoch 28, Validation Accuracy: 0.9555, Validation Loss: 0.1576\n",
      "Epoch 29, Batch 0, Loss: 0.1096, Accuracy: 0.9453\n",
      "Epoch 29, Batch 10, Loss: 0.2221, Accuracy: 0.9531\n",
      "Epoch 29, Batch 20, Loss: 0.2059, Accuracy: 0.9219\n",
      "Epoch 29, Batch 30, Loss: 0.0986, Accuracy: 0.9766\n",
      "Epoch 29, Batch 40, Loss: 0.0949, Accuracy: 0.9531\n",
      "Epoch 29, Batch 50, Loss: 0.1854, Accuracy: 0.9375\n",
      "Epoch 29, Batch 60, Loss: 0.1795, Accuracy: 0.9609\n",
      "Epoch 29, Batch 70, Loss: 0.1449, Accuracy: 0.9531\n",
      "Epoch 29, Batch 80, Loss: 0.0859, Accuracy: 0.9766\n",
      "Epoch 29, Batch 90, Loss: 0.1901, Accuracy: 0.9453\n",
      "Epoch 29, Batch 100, Loss: 0.1347, Accuracy: 0.9453\n",
      "Epoch 29, Batch 110, Loss: 0.1339, Accuracy: 0.9766\n",
      "Epoch 29, Batch 120, Loss: 0.1089, Accuracy: 0.9688\n",
      "Epoch 29, Batch 130, Loss: 0.0770, Accuracy: 0.9766\n",
      "Epoch 29, Batch 140, Loss: 0.0574, Accuracy: 1.0000\n",
      "Epoch 29, Batch 150, Loss: 0.1537, Accuracy: 0.9531\n",
      "Epoch 29, Batch 160, Loss: 0.1832, Accuracy: 0.9531\n",
      "Epoch 29, Batch 170, Loss: 0.1133, Accuracy: 0.9609\n",
      "Epoch 29, Batch 180, Loss: 0.1585, Accuracy: 0.9609\n",
      "Epoch 29, Batch 190, Loss: 0.1505, Accuracy: 0.9297\n",
      "Epoch 29, Batch 200, Loss: 0.1686, Accuracy: 0.9609\n",
      "Epoch 29, Batch 210, Loss: 0.1663, Accuracy: 0.9453\n",
      "Epoch 29, Batch 220, Loss: 0.1451, Accuracy: 0.9453\n",
      "Epoch 29, Batch 230, Loss: 0.0820, Accuracy: 0.9688\n",
      "Epoch 29, Batch 240, Loss: 0.0898, Accuracy: 0.9609\n",
      "Epoch 29, Batch 250, Loss: 0.1538, Accuracy: 0.9609\n",
      "Epoch 29, Batch 260, Loss: 0.0936, Accuracy: 0.9688\n",
      "Epoch 29, Batch 270, Loss: 0.2512, Accuracy: 0.9219\n",
      "Epoch 29, Batch 280, Loss: 0.1488, Accuracy: 0.9688\n",
      "Epoch 29, Batch 290, Loss: 0.1043, Accuracy: 0.9688\n",
      "Epoch 29, Batch 300, Loss: 0.0975, Accuracy: 0.9688\n",
      "Epoch 29, Batch 310, Loss: 0.1993, Accuracy: 0.9453\n",
      "Epoch 29, Batch 320, Loss: 0.2152, Accuracy: 0.9219\n",
      "Epoch 29, Batch 330, Loss: 0.1180, Accuracy: 0.9688\n",
      "Epoch 29, Batch 340, Loss: 0.1317, Accuracy: 0.9688\n",
      "Epoch 29, Batch 350, Loss: 0.1044, Accuracy: 0.9531\n",
      "Epoch 29, Batch 360, Loss: 0.1564, Accuracy: 0.9531\n",
      "Epoch 29, Batch 370, Loss: 0.1189, Accuracy: 0.9688\n",
      "Epoch 29, Batch 380, Loss: 0.2376, Accuracy: 0.9531\n",
      "Epoch 29, Batch 390, Loss: 0.1916, Accuracy: 0.9688\n",
      "Epoch 29, Batch 400, Loss: 0.2717, Accuracy: 0.8984\n",
      "Epoch 29, Batch 410, Loss: 0.3139, Accuracy: 0.9062\n",
      "Epoch 29, Batch 420, Loss: 0.1562, Accuracy: 0.9375\n",
      "Epoch 29, Batch 430, Loss: 0.1449, Accuracy: 0.9688\n",
      "Epoch 29, Batch 440, Loss: 0.1024, Accuracy: 0.9609\n",
      "Epoch 29, Batch 450, Loss: 0.2941, Accuracy: 0.9219\n",
      "Epoch 29, Batch 460, Loss: 0.1902, Accuracy: 0.9297\n",
      "Epoch 29, Train Accuracy: 0.9511, Test Loss: 0.1596\n",
      "Epoch 29, Validation Accuracy: 0.9524, Validation Loss: 0.1697\n",
      "Epoch 30, Batch 0, Loss: 0.1468, Accuracy: 0.9609\n",
      "Epoch 30, Batch 10, Loss: 0.1739, Accuracy: 0.9375\n",
      "Epoch 30, Batch 20, Loss: 0.1729, Accuracy: 0.9375\n",
      "Epoch 30, Batch 30, Loss: 0.1810, Accuracy: 0.9688\n",
      "Epoch 30, Batch 40, Loss: 0.1239, Accuracy: 0.9766\n",
      "Epoch 30, Batch 50, Loss: 0.1081, Accuracy: 0.9688\n",
      "Epoch 30, Batch 60, Loss: 0.1805, Accuracy: 0.9453\n",
      "Epoch 30, Batch 70, Loss: 0.1658, Accuracy: 0.9297\n",
      "Epoch 30, Batch 80, Loss: 0.2707, Accuracy: 0.9609\n",
      "Epoch 30, Batch 90, Loss: 0.1979, Accuracy: 0.9219\n",
      "Epoch 30, Batch 100, Loss: 0.2480, Accuracy: 0.9062\n",
      "Epoch 30, Batch 110, Loss: 0.1605, Accuracy: 0.9531\n",
      "Epoch 30, Batch 120, Loss: 0.0981, Accuracy: 0.9688\n",
      "Epoch 30, Batch 130, Loss: 0.2508, Accuracy: 0.9141\n",
      "Epoch 30, Batch 140, Loss: 0.0975, Accuracy: 0.9609\n",
      "Epoch 30, Batch 150, Loss: 0.1238, Accuracy: 0.9609\n",
      "Epoch 30, Batch 160, Loss: 0.1562, Accuracy: 0.9531\n",
      "Epoch 30, Batch 170, Loss: 0.1447, Accuracy: 0.9688\n",
      "Epoch 30, Batch 180, Loss: 0.0921, Accuracy: 0.9609\n",
      "Epoch 30, Batch 190, Loss: 0.1342, Accuracy: 0.9453\n",
      "Epoch 30, Batch 200, Loss: 0.2217, Accuracy: 0.9219\n",
      "Epoch 30, Batch 210, Loss: 0.1602, Accuracy: 0.9531\n",
      "Epoch 30, Batch 220, Loss: 0.1442, Accuracy: 0.9375\n",
      "Epoch 30, Batch 230, Loss: 0.1610, Accuracy: 0.9453\n",
      "Epoch 30, Batch 240, Loss: 0.1260, Accuracy: 0.9609\n",
      "Epoch 30, Batch 250, Loss: 0.1521, Accuracy: 0.9531\n",
      "Epoch 30, Batch 260, Loss: 0.1599, Accuracy: 0.9531\n",
      "Epoch 30, Batch 270, Loss: 0.1625, Accuracy: 0.9531\n",
      "Epoch 30, Batch 280, Loss: 0.1472, Accuracy: 0.9609\n",
      "Epoch 30, Batch 290, Loss: 0.1882, Accuracy: 0.9531\n",
      "Epoch 30, Batch 300, Loss: 0.1864, Accuracy: 0.9375\n",
      "Epoch 30, Batch 310, Loss: 0.1394, Accuracy: 0.9531\n",
      "Epoch 30, Batch 320, Loss: 0.0759, Accuracy: 0.9844\n",
      "Epoch 30, Batch 330, Loss: 0.3451, Accuracy: 0.9062\n",
      "Epoch 30, Batch 340, Loss: 0.1168, Accuracy: 0.9688\n",
      "Epoch 30, Batch 350, Loss: 0.1561, Accuracy: 0.9453\n",
      "Epoch 30, Batch 360, Loss: 0.1738, Accuracy: 0.9453\n",
      "Epoch 30, Batch 370, Loss: 0.2207, Accuracy: 0.9531\n",
      "Epoch 30, Batch 380, Loss: 0.1152, Accuracy: 0.9609\n",
      "Epoch 30, Batch 390, Loss: 0.1766, Accuracy: 0.9219\n",
      "Epoch 30, Batch 400, Loss: 0.0497, Accuracy: 1.0000\n",
      "Epoch 30, Batch 410, Loss: 0.1096, Accuracy: 0.9766\n",
      "Epoch 30, Batch 420, Loss: 0.1373, Accuracy: 0.9297\n",
      "Epoch 30, Batch 430, Loss: 0.1331, Accuracy: 0.9531\n",
      "Epoch 30, Batch 440, Loss: 0.1371, Accuracy: 0.9688\n",
      "Epoch 30, Batch 450, Loss: 0.2618, Accuracy: 0.9375\n",
      "Epoch 30, Batch 460, Loss: 0.1586, Accuracy: 0.9531\n",
      "Epoch 30, Train Accuracy: 0.9512, Test Loss: 0.1591\n",
      "Epoch 30, Validation Accuracy: 0.9503, Validation Loss: 0.1659\n",
      "Epoch 31, Batch 0, Loss: 0.0904, Accuracy: 0.9688\n",
      "Epoch 31, Batch 10, Loss: 0.1363, Accuracy: 0.9453\n",
      "Epoch 31, Batch 20, Loss: 0.1909, Accuracy: 0.9297\n",
      "Epoch 31, Batch 30, Loss: 0.1375, Accuracy: 0.9688\n",
      "Epoch 31, Batch 40, Loss: 0.1608, Accuracy: 0.9531\n",
      "Epoch 31, Batch 50, Loss: 0.1478, Accuracy: 0.9375\n",
      "Epoch 31, Batch 60, Loss: 0.1277, Accuracy: 0.9688\n",
      "Epoch 31, Batch 70, Loss: 0.2285, Accuracy: 0.9297\n",
      "Epoch 31, Batch 80, Loss: 0.1297, Accuracy: 0.9609\n",
      "Epoch 31, Batch 90, Loss: 0.2084, Accuracy: 0.9375\n",
      "Epoch 31, Batch 100, Loss: 0.2295, Accuracy: 0.9297\n",
      "Epoch 31, Batch 110, Loss: 0.1257, Accuracy: 0.9297\n",
      "Epoch 31, Batch 120, Loss: 0.0929, Accuracy: 0.9766\n",
      "Epoch 31, Batch 130, Loss: 0.1574, Accuracy: 0.9688\n",
      "Epoch 31, Batch 140, Loss: 0.1019, Accuracy: 0.9609\n",
      "Epoch 31, Batch 150, Loss: 0.1480, Accuracy: 0.9609\n",
      "Epoch 31, Batch 160, Loss: 0.3027, Accuracy: 0.9141\n",
      "Epoch 31, Batch 170, Loss: 0.1334, Accuracy: 0.9375\n",
      "Epoch 31, Batch 180, Loss: 0.1820, Accuracy: 0.9297\n",
      "Epoch 31, Batch 190, Loss: 0.1331, Accuracy: 0.9609\n",
      "Epoch 31, Batch 200, Loss: 0.3169, Accuracy: 0.8828\n",
      "Epoch 31, Batch 210, Loss: 0.1089, Accuracy: 0.9531\n",
      "Epoch 31, Batch 220, Loss: 0.0437, Accuracy: 0.9844\n",
      "Epoch 31, Batch 230, Loss: 0.1424, Accuracy: 0.9531\n",
      "Epoch 31, Batch 240, Loss: 0.2490, Accuracy: 0.9219\n",
      "Epoch 31, Batch 250, Loss: 0.1364, Accuracy: 0.9688\n",
      "Epoch 31, Batch 260, Loss: 0.2754, Accuracy: 0.9219\n",
      "Epoch 31, Batch 270, Loss: 0.1364, Accuracy: 0.9688\n",
      "Epoch 31, Batch 280, Loss: 0.1397, Accuracy: 0.9609\n",
      "Epoch 31, Batch 290, Loss: 0.1286, Accuracy: 0.9609\n",
      "Epoch 31, Batch 300, Loss: 0.1812, Accuracy: 0.9453\n",
      "Epoch 31, Batch 310, Loss: 0.1275, Accuracy: 0.9531\n",
      "Epoch 31, Batch 320, Loss: 0.0749, Accuracy: 0.9688\n",
      "Epoch 31, Batch 330, Loss: 0.1586, Accuracy: 0.9531\n",
      "Epoch 31, Batch 340, Loss: 0.2334, Accuracy: 0.9453\n",
      "Epoch 31, Batch 350, Loss: 0.3182, Accuracy: 0.8906\n",
      "Epoch 31, Batch 360, Loss: 0.0800, Accuracy: 0.9688\n",
      "Epoch 31, Batch 370, Loss: 0.1302, Accuracy: 0.9609\n",
      "Epoch 31, Batch 380, Loss: 0.1207, Accuracy: 0.9609\n",
      "Epoch 31, Batch 390, Loss: 0.1646, Accuracy: 0.9297\n",
      "Epoch 31, Batch 400, Loss: 0.0497, Accuracy: 0.9922\n",
      "Epoch 31, Batch 410, Loss: 0.1003, Accuracy: 0.9688\n",
      "Epoch 31, Batch 420, Loss: 0.2238, Accuracy: 0.9141\n",
      "Epoch 31, Batch 430, Loss: 0.0842, Accuracy: 0.9766\n",
      "Epoch 31, Batch 440, Loss: 0.1370, Accuracy: 0.9531\n",
      "Epoch 31, Batch 450, Loss: 0.1602, Accuracy: 0.9297\n",
      "Epoch 31, Batch 460, Loss: 0.1646, Accuracy: 0.9297\n",
      "Epoch 31, Train Accuracy: 0.9516, Test Loss: 0.1565\n",
      "Epoch 31, Validation Accuracy: 0.9538, Validation Loss: 0.1607\n",
      "Epoch 32, Batch 0, Loss: 0.0950, Accuracy: 0.9609\n",
      "Epoch 32, Batch 10, Loss: 0.1432, Accuracy: 0.9609\n",
      "Epoch 32, Batch 20, Loss: 0.1516, Accuracy: 0.9609\n",
      "Epoch 32, Batch 30, Loss: 0.1679, Accuracy: 0.9531\n",
      "Epoch 32, Batch 40, Loss: 0.1913, Accuracy: 0.9531\n",
      "Epoch 32, Batch 50, Loss: 0.2241, Accuracy: 0.9297\n",
      "Epoch 32, Batch 60, Loss: 0.0754, Accuracy: 0.9688\n",
      "Epoch 32, Batch 70, Loss: 0.2178, Accuracy: 0.9297\n",
      "Epoch 32, Batch 80, Loss: 0.2601, Accuracy: 0.9297\n",
      "Epoch 32, Batch 90, Loss: 0.2160, Accuracy: 0.9688\n",
      "Epoch 32, Batch 100, Loss: 0.0747, Accuracy: 0.9766\n",
      "Epoch 32, Batch 110, Loss: 0.1119, Accuracy: 0.9688\n",
      "Epoch 32, Batch 120, Loss: 0.1111, Accuracy: 0.9609\n",
      "Epoch 32, Batch 130, Loss: 0.2387, Accuracy: 0.9453\n",
      "Epoch 32, Batch 140, Loss: 0.0594, Accuracy: 0.9922\n",
      "Epoch 32, Batch 150, Loss: 0.1908, Accuracy: 0.9375\n",
      "Epoch 32, Batch 160, Loss: 0.1083, Accuracy: 0.9609\n",
      "Epoch 32, Batch 170, Loss: 0.1163, Accuracy: 0.9688\n",
      "Epoch 32, Batch 180, Loss: 0.1913, Accuracy: 0.9531\n",
      "Epoch 32, Batch 190, Loss: 0.1982, Accuracy: 0.9297\n",
      "Epoch 32, Batch 200, Loss: 0.1435, Accuracy: 0.9531\n",
      "Epoch 32, Batch 210, Loss: 0.1767, Accuracy: 0.9375\n",
      "Epoch 32, Batch 220, Loss: 0.1239, Accuracy: 0.9688\n",
      "Epoch 32, Batch 230, Loss: 0.2126, Accuracy: 0.9375\n",
      "Epoch 32, Batch 240, Loss: 0.2140, Accuracy: 0.9219\n",
      "Epoch 32, Batch 250, Loss: 0.1929, Accuracy: 0.9453\n",
      "Epoch 32, Batch 260, Loss: 0.1456, Accuracy: 0.9453\n",
      "Epoch 32, Batch 270, Loss: 0.3067, Accuracy: 0.9141\n",
      "Epoch 32, Batch 280, Loss: 0.1336, Accuracy: 0.9609\n",
      "Epoch 32, Batch 290, Loss: 0.1361, Accuracy: 0.9375\n",
      "Epoch 32, Batch 300, Loss: 0.1488, Accuracy: 0.9609\n",
      "Epoch 32, Batch 310, Loss: 0.2330, Accuracy: 0.9453\n",
      "Epoch 32, Batch 320, Loss: 0.1909, Accuracy: 0.9453\n",
      "Epoch 32, Batch 330, Loss: 0.1411, Accuracy: 0.9531\n",
      "Epoch 32, Batch 340, Loss: 0.0527, Accuracy: 0.9922\n",
      "Epoch 32, Batch 350, Loss: 0.1791, Accuracy: 0.9062\n",
      "Epoch 32, Batch 360, Loss: 0.0824, Accuracy: 0.9766\n",
      "Epoch 32, Batch 370, Loss: 0.1237, Accuracy: 0.9688\n",
      "Epoch 32, Batch 380, Loss: 0.1909, Accuracy: 0.9531\n",
      "Epoch 32, Batch 390, Loss: 0.1749, Accuracy: 0.9375\n",
      "Epoch 32, Batch 400, Loss: 0.1196, Accuracy: 0.9609\n",
      "Epoch 32, Batch 410, Loss: 0.1656, Accuracy: 0.9609\n",
      "Epoch 32, Batch 420, Loss: 0.1264, Accuracy: 0.9688\n",
      "Epoch 32, Batch 430, Loss: 0.1087, Accuracy: 0.9453\n",
      "Epoch 32, Batch 440, Loss: 0.1237, Accuracy: 0.9609\n",
      "Epoch 32, Batch 450, Loss: 0.0945, Accuracy: 0.9844\n",
      "Epoch 32, Batch 460, Loss: 0.0496, Accuracy: 0.9844\n",
      "Epoch 32, Train Accuracy: 0.9537, Test Loss: 0.1531\n",
      "Epoch 32, Validation Accuracy: 0.9548, Validation Loss: 0.1517\n",
      "Epoch 33, Batch 0, Loss: 0.1452, Accuracy: 0.9531\n",
      "Epoch 33, Batch 10, Loss: 0.0998, Accuracy: 0.9688\n",
      "Epoch 33, Batch 20, Loss: 0.1263, Accuracy: 0.9688\n",
      "Epoch 33, Batch 30, Loss: 0.0633, Accuracy: 0.9766\n",
      "Epoch 33, Batch 40, Loss: 0.2742, Accuracy: 0.8984\n",
      "Epoch 33, Batch 50, Loss: 0.1308, Accuracy: 0.9609\n",
      "Epoch 33, Batch 60, Loss: 0.1488, Accuracy: 0.9688\n",
      "Epoch 33, Batch 70, Loss: 0.3397, Accuracy: 0.8984\n",
      "Epoch 33, Batch 80, Loss: 0.0927, Accuracy: 0.9531\n",
      "Epoch 33, Batch 90, Loss: 0.1361, Accuracy: 0.9531\n",
      "Epoch 33, Batch 100, Loss: 0.0838, Accuracy: 0.9609\n",
      "Epoch 33, Batch 110, Loss: 0.1444, Accuracy: 0.9531\n",
      "Epoch 33, Batch 120, Loss: 0.1742, Accuracy: 0.9375\n",
      "Epoch 33, Batch 130, Loss: 0.1386, Accuracy: 0.9453\n",
      "Epoch 33, Batch 140, Loss: 0.1633, Accuracy: 0.9453\n",
      "Epoch 33, Batch 150, Loss: 0.0870, Accuracy: 0.9844\n",
      "Epoch 33, Batch 160, Loss: 0.1192, Accuracy: 0.9688\n",
      "Epoch 33, Batch 170, Loss: 0.1682, Accuracy: 0.9609\n",
      "Epoch 33, Batch 180, Loss: 0.1670, Accuracy: 0.9609\n",
      "Epoch 33, Batch 190, Loss: 0.1977, Accuracy: 0.9375\n",
      "Epoch 33, Batch 200, Loss: 0.1271, Accuracy: 0.9453\n",
      "Epoch 33, Batch 210, Loss: 0.0902, Accuracy: 0.9688\n",
      "Epoch 33, Batch 220, Loss: 0.0651, Accuracy: 0.9922\n",
      "Epoch 33, Batch 230, Loss: 0.2478, Accuracy: 0.9531\n",
      "Epoch 33, Batch 240, Loss: 0.2838, Accuracy: 0.9375\n",
      "Epoch 33, Batch 250, Loss: 0.2299, Accuracy: 0.9453\n",
      "Epoch 33, Batch 260, Loss: 0.1647, Accuracy: 0.9297\n",
      "Epoch 33, Batch 270, Loss: 0.1575, Accuracy: 0.9531\n",
      "Epoch 33, Batch 280, Loss: 0.1030, Accuracy: 0.9531\n",
      "Epoch 33, Batch 290, Loss: 0.0746, Accuracy: 0.9766\n",
      "Epoch 33, Batch 300, Loss: 0.1463, Accuracy: 0.9375\n",
      "Epoch 33, Batch 310, Loss: 0.2526, Accuracy: 0.9453\n",
      "Epoch 33, Batch 320, Loss: 0.1760, Accuracy: 0.9297\n",
      "Epoch 33, Batch 330, Loss: 0.2576, Accuracy: 0.9062\n",
      "Epoch 33, Batch 340, Loss: 0.2177, Accuracy: 0.9375\n",
      "Epoch 33, Batch 350, Loss: 0.2519, Accuracy: 0.9297\n",
      "Epoch 33, Batch 360, Loss: 0.2207, Accuracy: 0.9453\n",
      "Epoch 33, Batch 370, Loss: 0.1525, Accuracy: 0.9453\n",
      "Epoch 33, Batch 380, Loss: 0.2892, Accuracy: 0.9062\n",
      "Epoch 33, Batch 390, Loss: 0.1846, Accuracy: 0.9453\n",
      "Epoch 33, Batch 400, Loss: 0.2390, Accuracy: 0.9375\n",
      "Epoch 33, Batch 410, Loss: 0.1256, Accuracy: 0.9531\n",
      "Epoch 33, Batch 420, Loss: 0.1352, Accuracy: 0.9375\n",
      "Epoch 33, Batch 430, Loss: 0.3671, Accuracy: 0.9062\n",
      "Epoch 33, Batch 440, Loss: 0.1660, Accuracy: 0.9297\n",
      "Epoch 33, Batch 450, Loss: 0.1310, Accuracy: 0.9922\n",
      "Epoch 33, Batch 460, Loss: 0.1258, Accuracy: 0.9531\n",
      "Epoch 33, Train Accuracy: 0.9544, Test Loss: 0.1501\n",
      "Epoch 33, Validation Accuracy: 0.9506, Validation Loss: 0.1623\n",
      "Epoch 34, Batch 0, Loss: 0.2131, Accuracy: 0.9453\n",
      "Epoch 34, Batch 10, Loss: 0.0836, Accuracy: 0.9766\n",
      "Epoch 34, Batch 20, Loss: 0.0792, Accuracy: 0.9844\n",
      "Epoch 34, Batch 30, Loss: 0.1192, Accuracy: 0.9609\n",
      "Epoch 34, Batch 40, Loss: 0.1277, Accuracy: 0.9688\n",
      "Epoch 34, Batch 50, Loss: 0.1458, Accuracy: 0.9297\n",
      "Epoch 34, Batch 60, Loss: 0.0922, Accuracy: 0.9609\n",
      "Epoch 34, Batch 70, Loss: 0.2384, Accuracy: 0.9297\n",
      "Epoch 34, Batch 80, Loss: 0.0634, Accuracy: 0.9844\n",
      "Epoch 34, Batch 90, Loss: 0.1498, Accuracy: 0.9531\n",
      "Epoch 34, Batch 100, Loss: 0.0604, Accuracy: 0.9688\n",
      "Epoch 34, Batch 110, Loss: 0.0851, Accuracy: 0.9531\n",
      "Epoch 34, Batch 120, Loss: 0.1974, Accuracy: 0.9453\n",
      "Epoch 34, Batch 130, Loss: 0.1061, Accuracy: 0.9609\n",
      "Epoch 34, Batch 140, Loss: 0.0814, Accuracy: 0.9844\n",
      "Epoch 34, Batch 150, Loss: 0.1022, Accuracy: 0.9609\n",
      "Epoch 34, Batch 160, Loss: 0.2627, Accuracy: 0.9141\n",
      "Epoch 34, Batch 170, Loss: 0.0893, Accuracy: 0.9531\n",
      "Epoch 34, Batch 180, Loss: 0.1292, Accuracy: 0.9688\n",
      "Epoch 34, Batch 190, Loss: 0.1359, Accuracy: 0.9531\n",
      "Epoch 34, Batch 200, Loss: 0.1893, Accuracy: 0.9375\n",
      "Epoch 34, Batch 210, Loss: 0.1208, Accuracy: 0.9531\n",
      "Epoch 34, Batch 220, Loss: 0.1055, Accuracy: 0.9531\n",
      "Epoch 34, Batch 230, Loss: 0.1460, Accuracy: 0.9531\n",
      "Epoch 34, Batch 240, Loss: 0.1357, Accuracy: 0.9531\n",
      "Epoch 34, Batch 250, Loss: 0.2044, Accuracy: 0.9375\n",
      "Epoch 34, Batch 260, Loss: 0.1846, Accuracy: 0.9297\n",
      "Epoch 34, Batch 270, Loss: 0.2565, Accuracy: 0.9297\n",
      "Epoch 34, Batch 280, Loss: 0.2352, Accuracy: 0.9453\n",
      "Epoch 34, Batch 290, Loss: 0.1911, Accuracy: 0.9297\n",
      "Epoch 34, Batch 300, Loss: 0.0766, Accuracy: 0.9766\n",
      "Epoch 34, Batch 310, Loss: 0.1391, Accuracy: 0.9609\n",
      "Epoch 34, Batch 320, Loss: 0.0681, Accuracy: 0.9766\n",
      "Epoch 34, Batch 330, Loss: 0.1388, Accuracy: 0.9688\n",
      "Epoch 34, Batch 340, Loss: 0.1149, Accuracy: 0.9609\n",
      "Epoch 34, Batch 350, Loss: 0.1589, Accuracy: 0.9609\n",
      "Epoch 34, Batch 360, Loss: 0.2792, Accuracy: 0.9297\n",
      "Epoch 34, Batch 370, Loss: 0.1925, Accuracy: 0.9219\n",
      "Epoch 34, Batch 380, Loss: 0.2024, Accuracy: 0.9609\n",
      "Epoch 34, Batch 390, Loss: 0.2041, Accuracy: 0.9688\n",
      "Epoch 34, Batch 400, Loss: 0.1336, Accuracy: 0.9609\n",
      "Epoch 34, Batch 410, Loss: 0.0352, Accuracy: 0.9922\n",
      "Epoch 34, Batch 420, Loss: 0.0855, Accuracy: 0.9609\n",
      "Epoch 34, Batch 430, Loss: 0.0689, Accuracy: 0.9688\n",
      "Epoch 34, Batch 440, Loss: 0.1024, Accuracy: 0.9688\n",
      "Epoch 34, Batch 450, Loss: 0.0901, Accuracy: 0.9688\n",
      "Epoch 34, Batch 460, Loss: 0.1237, Accuracy: 0.9531\n",
      "Epoch 34, Train Accuracy: 0.9529, Test Loss: 0.1522\n",
      "Epoch 34, Validation Accuracy: 0.9555, Validation Loss: 0.1509\n",
      "Epoch 35, Batch 0, Loss: 0.0676, Accuracy: 0.9609\n",
      "Epoch 35, Batch 10, Loss: 0.1679, Accuracy: 0.9453\n",
      "Epoch 35, Batch 20, Loss: 0.2074, Accuracy: 0.9297\n",
      "Epoch 35, Batch 30, Loss: 0.1659, Accuracy: 0.9531\n",
      "Epoch 35, Batch 40, Loss: 0.1930, Accuracy: 0.9375\n",
      "Epoch 35, Batch 50, Loss: 0.2797, Accuracy: 0.9062\n",
      "Epoch 35, Batch 60, Loss: 0.0548, Accuracy: 0.9844\n",
      "Epoch 35, Batch 70, Loss: 0.1625, Accuracy: 0.9609\n",
      "Epoch 35, Batch 80, Loss: 0.1002, Accuracy: 0.9766\n",
      "Epoch 35, Batch 90, Loss: 0.1111, Accuracy: 0.9688\n",
      "Epoch 35, Batch 100, Loss: 0.0879, Accuracy: 0.9688\n",
      "Epoch 35, Batch 110, Loss: 0.1477, Accuracy: 0.9688\n",
      "Epoch 35, Batch 120, Loss: 0.1247, Accuracy: 0.9609\n",
      "Epoch 35, Batch 130, Loss: 0.1025, Accuracy: 0.9531\n",
      "Epoch 35, Batch 140, Loss: 0.1757, Accuracy: 0.9531\n",
      "Epoch 35, Batch 150, Loss: 0.1121, Accuracy: 0.9609\n",
      "Epoch 35, Batch 160, Loss: 0.1958, Accuracy: 0.9375\n",
      "Epoch 35, Batch 170, Loss: 0.2336, Accuracy: 0.9297\n",
      "Epoch 35, Batch 180, Loss: 0.1256, Accuracy: 0.9609\n",
      "Epoch 35, Batch 190, Loss: 0.1961, Accuracy: 0.9297\n",
      "Epoch 35, Batch 200, Loss: 0.1708, Accuracy: 0.9297\n",
      "Epoch 35, Batch 210, Loss: 0.1247, Accuracy: 0.9453\n",
      "Epoch 35, Batch 220, Loss: 0.1703, Accuracy: 0.9531\n",
      "Epoch 35, Batch 230, Loss: 0.2114, Accuracy: 0.9531\n",
      "Epoch 35, Batch 240, Loss: 0.1297, Accuracy: 0.9766\n",
      "Epoch 35, Batch 250, Loss: 0.1340, Accuracy: 0.9688\n",
      "Epoch 35, Batch 260, Loss: 0.1852, Accuracy: 0.9219\n",
      "Epoch 35, Batch 270, Loss: 0.2228, Accuracy: 0.9297\n",
      "Epoch 35, Batch 280, Loss: 0.1700, Accuracy: 0.9375\n",
      "Epoch 35, Batch 290, Loss: 0.0724, Accuracy: 0.9766\n",
      "Epoch 35, Batch 300, Loss: 0.1858, Accuracy: 0.9609\n",
      "Epoch 35, Batch 310, Loss: 0.1081, Accuracy: 0.9844\n",
      "Epoch 35, Batch 320, Loss: 0.1878, Accuracy: 0.9297\n",
      "Epoch 35, Batch 330, Loss: 0.1914, Accuracy: 0.9375\n",
      "Epoch 35, Batch 340, Loss: 0.2299, Accuracy: 0.9297\n",
      "Epoch 35, Batch 350, Loss: 0.1746, Accuracy: 0.9688\n",
      "Epoch 35, Batch 360, Loss: 0.1742, Accuracy: 0.9297\n",
      "Epoch 35, Batch 370, Loss: 0.1354, Accuracy: 0.9453\n",
      "Epoch 35, Batch 380, Loss: 0.1683, Accuracy: 0.9688\n",
      "Epoch 35, Batch 390, Loss: 0.1048, Accuracy: 0.9688\n",
      "Epoch 35, Batch 400, Loss: 0.2140, Accuracy: 0.9219\n",
      "Epoch 35, Batch 410, Loss: 0.1167, Accuracy: 0.9766\n",
      "Epoch 35, Batch 420, Loss: 0.1922, Accuracy: 0.9219\n",
      "Epoch 35, Batch 430, Loss: 0.2101, Accuracy: 0.9375\n",
      "Epoch 35, Batch 440, Loss: 0.2454, Accuracy: 0.9297\n",
      "Epoch 35, Batch 450, Loss: 0.0939, Accuracy: 0.9844\n",
      "Epoch 35, Batch 460, Loss: 0.1325, Accuracy: 0.9766\n",
      "Epoch 35, Train Accuracy: 0.9559, Test Loss: 0.1452\n",
      "Epoch 35, Validation Accuracy: 0.9519, Validation Loss: 0.1617\n",
      "Epoch 36, Batch 0, Loss: 0.1382, Accuracy: 0.9844\n",
      "Epoch 36, Batch 10, Loss: 0.1034, Accuracy: 0.9531\n",
      "Epoch 36, Batch 20, Loss: 0.0804, Accuracy: 0.9609\n",
      "Epoch 36, Batch 30, Loss: 0.1127, Accuracy: 0.9688\n",
      "Epoch 36, Batch 40, Loss: 0.1701, Accuracy: 0.9453\n",
      "Epoch 36, Batch 50, Loss: 0.0654, Accuracy: 0.9766\n",
      "Epoch 36, Batch 60, Loss: 0.1394, Accuracy: 0.9531\n",
      "Epoch 36, Batch 70, Loss: 0.0476, Accuracy: 0.9922\n",
      "Epoch 36, Batch 80, Loss: 0.1595, Accuracy: 0.9609\n",
      "Epoch 36, Batch 90, Loss: 0.1281, Accuracy: 0.9609\n",
      "Epoch 36, Batch 100, Loss: 0.0794, Accuracy: 0.9766\n",
      "Epoch 36, Batch 110, Loss: 0.2685, Accuracy: 0.9141\n",
      "Epoch 36, Batch 120, Loss: 0.1027, Accuracy: 0.9688\n",
      "Epoch 36, Batch 130, Loss: 0.1568, Accuracy: 0.9609\n",
      "Epoch 36, Batch 140, Loss: 0.2075, Accuracy: 0.9531\n",
      "Epoch 36, Batch 150, Loss: 0.1667, Accuracy: 0.9453\n",
      "Epoch 36, Batch 160, Loss: 0.2354, Accuracy: 0.9141\n",
      "Epoch 36, Batch 170, Loss: 0.1956, Accuracy: 0.9219\n",
      "Epoch 36, Batch 180, Loss: 0.1816, Accuracy: 0.9531\n",
      "Epoch 36, Batch 190, Loss: 0.0802, Accuracy: 0.9688\n",
      "Epoch 36, Batch 200, Loss: 0.2052, Accuracy: 0.9375\n",
      "Epoch 36, Batch 210, Loss: 0.0716, Accuracy: 0.9766\n",
      "Epoch 36, Batch 220, Loss: 0.0931, Accuracy: 0.9688\n",
      "Epoch 36, Batch 230, Loss: 0.2113, Accuracy: 0.9219\n",
      "Epoch 36, Batch 240, Loss: 0.1005, Accuracy: 0.9766\n",
      "Epoch 36, Batch 250, Loss: 0.0568, Accuracy: 0.9922\n",
      "Epoch 36, Batch 260, Loss: 0.0933, Accuracy: 0.9766\n",
      "Epoch 36, Batch 270, Loss: 0.1634, Accuracy: 0.9453\n",
      "Epoch 36, Batch 280, Loss: 0.0626, Accuracy: 0.9609\n",
      "Epoch 36, Batch 290, Loss: 0.0790, Accuracy: 0.9766\n",
      "Epoch 36, Batch 300, Loss: 0.1324, Accuracy: 0.9609\n",
      "Epoch 36, Batch 310, Loss: 0.1856, Accuracy: 0.9453\n",
      "Epoch 36, Batch 320, Loss: 0.1626, Accuracy: 0.9375\n",
      "Epoch 36, Batch 330, Loss: 0.1697, Accuracy: 0.9453\n",
      "Epoch 36, Batch 340, Loss: 0.0759, Accuracy: 0.9766\n",
      "Epoch 36, Batch 350, Loss: 0.2039, Accuracy: 0.9531\n",
      "Epoch 36, Batch 360, Loss: 0.1815, Accuracy: 0.9531\n",
      "Epoch 36, Batch 370, Loss: 0.3232, Accuracy: 0.9141\n",
      "Epoch 36, Batch 380, Loss: 0.0518, Accuracy: 0.9922\n",
      "Epoch 36, Batch 390, Loss: 0.0876, Accuracy: 0.9688\n",
      "Epoch 36, Batch 400, Loss: 0.0895, Accuracy: 0.9766\n",
      "Epoch 36, Batch 410, Loss: 0.1323, Accuracy: 0.9453\n",
      "Epoch 36, Batch 420, Loss: 0.0569, Accuracy: 0.9922\n",
      "Epoch 36, Batch 430, Loss: 0.0951, Accuracy: 0.9688\n",
      "Epoch 36, Batch 440, Loss: 0.1274, Accuracy: 0.9531\n",
      "Epoch 36, Batch 450, Loss: 0.1683, Accuracy: 0.9375\n",
      "Epoch 36, Batch 460, Loss: 0.1277, Accuracy: 0.9453\n",
      "Epoch 36, Train Accuracy: 0.9574, Test Loss: 0.1427\n",
      "Epoch 36, Validation Accuracy: 0.9606, Validation Loss: 0.1329\n",
      "Epoch 37, Batch 0, Loss: 0.1410, Accuracy: 0.9609\n",
      "Epoch 37, Batch 10, Loss: 0.0945, Accuracy: 0.9609\n",
      "Epoch 37, Batch 20, Loss: 0.1315, Accuracy: 0.9688\n",
      "Epoch 37, Batch 30, Loss: 0.1921, Accuracy: 0.9375\n",
      "Epoch 37, Batch 40, Loss: 0.2113, Accuracy: 0.9219\n",
      "Epoch 37, Batch 50, Loss: 0.2416, Accuracy: 0.9531\n",
      "Epoch 37, Batch 60, Loss: 0.0982, Accuracy: 0.9688\n",
      "Epoch 37, Batch 70, Loss: 0.1599, Accuracy: 0.9609\n",
      "Epoch 37, Batch 80, Loss: 0.2754, Accuracy: 0.9453\n",
      "Epoch 37, Batch 90, Loss: 0.0867, Accuracy: 0.9766\n",
      "Epoch 37, Batch 100, Loss: 0.1258, Accuracy: 0.9453\n",
      "Epoch 37, Batch 110, Loss: 0.1468, Accuracy: 0.9609\n",
      "Epoch 37, Batch 120, Loss: 0.0807, Accuracy: 0.9766\n",
      "Epoch 37, Batch 130, Loss: 0.1842, Accuracy: 0.9453\n",
      "Epoch 37, Batch 140, Loss: 0.0570, Accuracy: 0.9844\n",
      "Epoch 37, Batch 150, Loss: 0.1343, Accuracy: 0.9453\n",
      "Epoch 37, Batch 160, Loss: 0.1442, Accuracy: 0.9531\n",
      "Epoch 37, Batch 170, Loss: 0.2402, Accuracy: 0.9375\n",
      "Epoch 37, Batch 180, Loss: 0.1539, Accuracy: 0.9531\n",
      "Epoch 37, Batch 190, Loss: 0.1646, Accuracy: 0.9609\n",
      "Epoch 37, Batch 200, Loss: 0.2202, Accuracy: 0.9219\n",
      "Epoch 37, Batch 210, Loss: 0.1563, Accuracy: 0.9219\n",
      "Epoch 37, Batch 220, Loss: 0.1624, Accuracy: 0.9531\n",
      "Epoch 37, Batch 230, Loss: 0.1769, Accuracy: 0.9453\n",
      "Epoch 37, Batch 240, Loss: 0.0931, Accuracy: 0.9688\n",
      "Epoch 37, Batch 250, Loss: 0.3103, Accuracy: 0.9375\n",
      "Epoch 37, Batch 260, Loss: 0.2468, Accuracy: 0.9453\n",
      "Epoch 37, Batch 270, Loss: 0.1676, Accuracy: 0.9531\n",
      "Epoch 37, Batch 280, Loss: 0.2310, Accuracy: 0.9219\n",
      "Epoch 37, Batch 290, Loss: 0.0941, Accuracy: 0.9844\n",
      "Epoch 37, Batch 300, Loss: 0.0741, Accuracy: 0.9766\n",
      "Epoch 37, Batch 310, Loss: 0.1305, Accuracy: 0.9453\n",
      "Epoch 37, Batch 320, Loss: 0.1427, Accuracy: 0.9453\n",
      "Epoch 37, Batch 330, Loss: 0.0648, Accuracy: 0.9922\n",
      "Epoch 37, Batch 340, Loss: 0.0894, Accuracy: 0.9688\n",
      "Epoch 37, Batch 350, Loss: 0.1785, Accuracy: 0.9531\n",
      "Epoch 37, Batch 360, Loss: 0.1055, Accuracy: 0.9688\n",
      "Epoch 37, Batch 370, Loss: 0.1506, Accuracy: 0.9453\n",
      "Epoch 37, Batch 380, Loss: 0.1376, Accuracy: 0.9531\n",
      "Epoch 37, Batch 390, Loss: 0.1612, Accuracy: 0.9531\n",
      "Epoch 37, Batch 400, Loss: 0.0901, Accuracy: 0.9766\n",
      "Epoch 37, Batch 410, Loss: 0.1174, Accuracy: 0.9609\n",
      "Epoch 37, Batch 420, Loss: 0.1333, Accuracy: 0.9609\n",
      "Epoch 37, Batch 430, Loss: 0.1260, Accuracy: 0.9531\n",
      "Epoch 37, Batch 440, Loss: 0.1463, Accuracy: 0.9766\n",
      "Epoch 37, Batch 450, Loss: 0.3041, Accuracy: 0.9062\n",
      "Epoch 37, Batch 460, Loss: 0.1694, Accuracy: 0.9766\n",
      "Epoch 37, Train Accuracy: 0.9551, Test Loss: 0.1448\n",
      "Epoch 37, Validation Accuracy: 0.9497, Validation Loss: 0.1630\n",
      "Epoch 38, Batch 0, Loss: 0.0903, Accuracy: 0.9688\n",
      "Epoch 38, Batch 10, Loss: 0.1487, Accuracy: 0.9766\n",
      "Epoch 38, Batch 20, Loss: 0.0652, Accuracy: 0.9688\n",
      "Epoch 38, Batch 30, Loss: 0.1340, Accuracy: 0.9531\n",
      "Epoch 38, Batch 40, Loss: 0.0860, Accuracy: 0.9609\n",
      "Epoch 38, Batch 50, Loss: 0.1199, Accuracy: 0.9609\n",
      "Epoch 38, Batch 60, Loss: 0.1653, Accuracy: 0.9531\n",
      "Epoch 38, Batch 70, Loss: 0.1513, Accuracy: 0.9609\n",
      "Epoch 38, Batch 80, Loss: 0.1128, Accuracy: 0.9609\n",
      "Epoch 38, Batch 90, Loss: 0.1523, Accuracy: 0.9453\n",
      "Epoch 38, Batch 100, Loss: 0.1682, Accuracy: 0.9531\n",
      "Epoch 38, Batch 110, Loss: 0.2098, Accuracy: 0.9453\n",
      "Epoch 38, Batch 120, Loss: 0.1870, Accuracy: 0.9531\n",
      "Epoch 38, Batch 130, Loss: 0.2399, Accuracy: 0.9297\n",
      "Epoch 38, Batch 140, Loss: 0.0899, Accuracy: 0.9688\n",
      "Epoch 38, Batch 150, Loss: 0.1054, Accuracy: 0.9609\n",
      "Epoch 38, Batch 160, Loss: 0.1070, Accuracy: 0.9766\n",
      "Epoch 38, Batch 170, Loss: 0.1923, Accuracy: 0.9375\n",
      "Epoch 38, Batch 180, Loss: 0.0922, Accuracy: 0.9688\n",
      "Epoch 38, Batch 190, Loss: 0.1539, Accuracy: 0.9531\n",
      "Epoch 38, Batch 200, Loss: 0.1211, Accuracy: 0.9609\n",
      "Epoch 38, Batch 210, Loss: 0.0908, Accuracy: 0.9688\n",
      "Epoch 38, Batch 220, Loss: 0.1031, Accuracy: 0.9766\n",
      "Epoch 38, Batch 230, Loss: 0.1843, Accuracy: 0.9375\n",
      "Epoch 38, Batch 240, Loss: 0.1319, Accuracy: 0.9531\n",
      "Epoch 38, Batch 250, Loss: 0.0912, Accuracy: 0.9688\n",
      "Epoch 38, Batch 260, Loss: 0.2589, Accuracy: 0.9062\n",
      "Epoch 38, Batch 270, Loss: 0.1277, Accuracy: 0.9609\n",
      "Epoch 38, Batch 280, Loss: 0.1483, Accuracy: 0.9297\n",
      "Epoch 38, Batch 290, Loss: 0.2372, Accuracy: 0.9531\n",
      "Epoch 38, Batch 300, Loss: 0.1186, Accuracy: 0.9609\n",
      "Epoch 38, Batch 310, Loss: 0.2007, Accuracy: 0.9531\n",
      "Epoch 38, Batch 320, Loss: 0.0716, Accuracy: 0.9766\n",
      "Epoch 38, Batch 330, Loss: 0.1740, Accuracy: 0.9375\n",
      "Epoch 38, Batch 340, Loss: 0.1972, Accuracy: 0.9297\n",
      "Epoch 38, Batch 350, Loss: 0.1368, Accuracy: 0.9531\n",
      "Epoch 38, Batch 360, Loss: 0.1572, Accuracy: 0.9531\n",
      "Epoch 38, Batch 370, Loss: 0.2443, Accuracy: 0.9453\n",
      "Epoch 38, Batch 380, Loss: 0.0648, Accuracy: 0.9844\n",
      "Epoch 38, Batch 390, Loss: 0.1401, Accuracy: 0.9453\n",
      "Epoch 38, Batch 400, Loss: 0.1706, Accuracy: 0.9297\n",
      "Epoch 38, Batch 410, Loss: 0.0977, Accuracy: 0.9609\n",
      "Epoch 38, Batch 420, Loss: 0.1606, Accuracy: 0.9453\n",
      "Epoch 38, Batch 430, Loss: 0.0983, Accuracy: 0.9609\n",
      "Epoch 38, Batch 440, Loss: 0.1684, Accuracy: 0.9453\n",
      "Epoch 38, Batch 450, Loss: 0.1520, Accuracy: 0.9766\n",
      "Epoch 38, Batch 460, Loss: 0.1118, Accuracy: 0.9609\n",
      "Epoch 38, Train Accuracy: 0.9569, Test Loss: 0.1419\n",
      "Epoch 38, Validation Accuracy: 0.9622, Validation Loss: 0.1343\n",
      "Epoch 39, Batch 0, Loss: 0.1308, Accuracy: 0.9609\n",
      "Epoch 39, Batch 10, Loss: 0.0772, Accuracy: 0.9844\n",
      "Epoch 39, Batch 20, Loss: 0.1462, Accuracy: 0.9531\n",
      "Epoch 39, Batch 30, Loss: 0.1844, Accuracy: 0.9609\n",
      "Epoch 39, Batch 40, Loss: 0.1050, Accuracy: 0.9688\n",
      "Epoch 39, Batch 50, Loss: 0.1364, Accuracy: 0.9453\n",
      "Epoch 39, Batch 60, Loss: 0.1204, Accuracy: 0.9609\n",
      "Epoch 39, Batch 70, Loss: 0.0766, Accuracy: 0.9844\n",
      "Epoch 39, Batch 80, Loss: 0.1340, Accuracy: 0.9609\n",
      "Epoch 39, Batch 90, Loss: 0.1358, Accuracy: 0.9453\n",
      "Epoch 39, Batch 100, Loss: 0.1564, Accuracy: 0.9531\n",
      "Epoch 39, Batch 110, Loss: 0.0835, Accuracy: 0.9766\n",
      "Epoch 39, Batch 120, Loss: 0.0860, Accuracy: 0.9844\n",
      "Epoch 39, Batch 130, Loss: 0.0803, Accuracy: 0.9922\n",
      "Epoch 39, Batch 140, Loss: 0.2700, Accuracy: 0.9219\n",
      "Epoch 39, Batch 150, Loss: 0.2014, Accuracy: 0.9531\n",
      "Epoch 39, Batch 160, Loss: 0.1401, Accuracy: 0.9453\n",
      "Epoch 39, Batch 170, Loss: 0.0879, Accuracy: 0.9609\n",
      "Epoch 39, Batch 180, Loss: 0.1123, Accuracy: 0.9531\n",
      "Epoch 39, Batch 190, Loss: 0.1792, Accuracy: 0.9609\n",
      "Epoch 39, Batch 200, Loss: 0.0827, Accuracy: 0.9766\n",
      "Epoch 39, Batch 210, Loss: 0.0885, Accuracy: 0.9766\n",
      "Epoch 39, Batch 220, Loss: 0.0727, Accuracy: 0.9766\n",
      "Epoch 39, Batch 230, Loss: 0.0914, Accuracy: 0.9766\n",
      "Epoch 39, Batch 240, Loss: 0.1201, Accuracy: 0.9609\n",
      "Epoch 39, Batch 250, Loss: 0.2783, Accuracy: 0.9375\n",
      "Epoch 39, Batch 260, Loss: 0.1330, Accuracy: 0.9688\n",
      "Epoch 39, Batch 270, Loss: 0.1114, Accuracy: 0.9531\n",
      "Epoch 39, Batch 280, Loss: 0.1044, Accuracy: 0.9531\n",
      "Epoch 39, Batch 290, Loss: 0.1208, Accuracy: 0.9453\n",
      "Epoch 39, Batch 300, Loss: 0.1342, Accuracy: 0.9375\n",
      "Epoch 39, Batch 310, Loss: 0.1869, Accuracy: 0.9219\n",
      "Epoch 39, Batch 320, Loss: 0.0777, Accuracy: 0.9766\n",
      "Epoch 39, Batch 330, Loss: 0.1975, Accuracy: 0.9297\n",
      "Epoch 39, Batch 340, Loss: 0.1391, Accuracy: 0.9688\n",
      "Epoch 39, Batch 350, Loss: 0.1212, Accuracy: 0.9688\n",
      "Epoch 39, Batch 360, Loss: 0.1198, Accuracy: 0.9453\n",
      "Epoch 39, Batch 370, Loss: 0.1115, Accuracy: 0.9688\n",
      "Epoch 39, Batch 380, Loss: 0.1059, Accuracy: 0.9531\n",
      "Epoch 39, Batch 390, Loss: 0.1063, Accuracy: 0.9609\n",
      "Epoch 39, Batch 400, Loss: 0.2444, Accuracy: 0.9609\n",
      "Epoch 39, Batch 410, Loss: 0.1054, Accuracy: 0.9688\n",
      "Epoch 39, Batch 420, Loss: 0.0831, Accuracy: 0.9766\n",
      "Epoch 39, Batch 430, Loss: 0.1412, Accuracy: 0.9453\n",
      "Epoch 39, Batch 440, Loss: 0.1351, Accuracy: 0.9609\n",
      "Epoch 39, Batch 450, Loss: 0.0880, Accuracy: 0.9609\n",
      "Epoch 39, Batch 460, Loss: 0.1859, Accuracy: 0.9297\n",
      "Epoch 39, Train Accuracy: 0.9579, Test Loss: 0.1383\n",
      "Epoch 39, Validation Accuracy: 0.9523, Validation Loss: 0.1591\n",
      "Epoch 40, Batch 0, Loss: 0.1616, Accuracy: 0.9609\n",
      "Epoch 40, Batch 10, Loss: 0.1159, Accuracy: 0.9688\n",
      "Epoch 40, Batch 20, Loss: 0.1507, Accuracy: 0.9531\n",
      "Epoch 40, Batch 30, Loss: 0.1121, Accuracy: 0.9688\n",
      "Epoch 40, Batch 40, Loss: 0.1742, Accuracy: 0.9297\n",
      "Epoch 40, Batch 50, Loss: 0.0775, Accuracy: 0.9766\n",
      "Epoch 40, Batch 60, Loss: 0.1925, Accuracy: 0.9375\n",
      "Epoch 40, Batch 70, Loss: 0.2190, Accuracy: 0.9531\n",
      "Epoch 40, Batch 80, Loss: 0.0632, Accuracy: 0.9844\n",
      "Epoch 40, Batch 90, Loss: 0.1395, Accuracy: 0.9531\n",
      "Epoch 40, Batch 100, Loss: 0.1070, Accuracy: 0.9609\n",
      "Epoch 40, Batch 110, Loss: 0.1698, Accuracy: 0.9531\n",
      "Epoch 40, Batch 120, Loss: 0.1564, Accuracy: 0.9609\n",
      "Epoch 40, Batch 130, Loss: 0.1143, Accuracy: 0.9531\n",
      "Epoch 40, Batch 140, Loss: 0.0790, Accuracy: 0.9766\n",
      "Epoch 40, Batch 150, Loss: 0.1795, Accuracy: 0.9219\n",
      "Epoch 40, Batch 160, Loss: 0.1603, Accuracy: 0.9375\n",
      "Epoch 40, Batch 170, Loss: 0.0873, Accuracy: 0.9844\n",
      "Epoch 40, Batch 180, Loss: 0.1103, Accuracy: 0.9531\n",
      "Epoch 40, Batch 190, Loss: 0.1406, Accuracy: 0.9297\n",
      "Epoch 40, Batch 200, Loss: 0.1315, Accuracy: 0.9609\n",
      "Epoch 40, Batch 210, Loss: 0.0589, Accuracy: 0.9766\n",
      "Epoch 40, Batch 220, Loss: 0.1373, Accuracy: 0.9531\n",
      "Epoch 40, Batch 230, Loss: 0.2314, Accuracy: 0.9219\n",
      "Epoch 40, Batch 240, Loss: 0.1226, Accuracy: 0.9531\n",
      "Epoch 40, Batch 250, Loss: 0.1496, Accuracy: 0.9453\n",
      "Epoch 40, Batch 260, Loss: 0.1426, Accuracy: 0.9688\n",
      "Epoch 40, Batch 270, Loss: 0.1408, Accuracy: 0.9609\n",
      "Epoch 40, Batch 280, Loss: 0.1696, Accuracy: 0.9609\n",
      "Epoch 40, Batch 290, Loss: 0.1400, Accuracy: 0.9297\n",
      "Epoch 40, Batch 300, Loss: 0.1144, Accuracy: 0.9609\n",
      "Epoch 40, Batch 310, Loss: 0.0992, Accuracy: 0.9609\n",
      "Epoch 40, Batch 320, Loss: 0.1780, Accuracy: 0.9609\n",
      "Epoch 40, Batch 330, Loss: 0.1837, Accuracy: 0.9609\n",
      "Epoch 40, Batch 340, Loss: 0.0705, Accuracy: 0.9844\n",
      "Epoch 40, Batch 350, Loss: 0.3651, Accuracy: 0.8828\n",
      "Epoch 40, Batch 360, Loss: 0.1571, Accuracy: 0.9453\n",
      "Epoch 40, Batch 370, Loss: 0.1177, Accuracy: 0.9688\n",
      "Epoch 40, Batch 380, Loss: 0.1222, Accuracy: 0.9688\n",
      "Epoch 40, Batch 390, Loss: 0.1098, Accuracy: 0.9609\n",
      "Epoch 40, Batch 400, Loss: 0.1272, Accuracy: 0.9531\n",
      "Epoch 40, Batch 410, Loss: 0.0788, Accuracy: 0.9844\n",
      "Epoch 40, Batch 420, Loss: 0.0430, Accuracy: 0.9922\n",
      "Epoch 40, Batch 430, Loss: 0.2425, Accuracy: 0.9453\n",
      "Epoch 40, Batch 440, Loss: 0.1591, Accuracy: 0.9453\n",
      "Epoch 40, Batch 450, Loss: 0.1266, Accuracy: 0.9688\n",
      "Epoch 40, Batch 460, Loss: 0.1378, Accuracy: 0.9688\n",
      "Epoch 40, Train Accuracy: 0.9577, Test Loss: 0.1380\n",
      "Epoch 40, Validation Accuracy: 0.9629, Validation Loss: 0.1327\n",
      "Epoch 41, Batch 0, Loss: 0.0744, Accuracy: 0.9844\n",
      "Epoch 41, Batch 10, Loss: 0.2049, Accuracy: 0.9375\n",
      "Epoch 41, Batch 20, Loss: 0.1255, Accuracy: 0.9531\n",
      "Epoch 41, Batch 30, Loss: 0.1613, Accuracy: 0.9609\n",
      "Epoch 41, Batch 40, Loss: 0.1922, Accuracy: 0.9453\n",
      "Epoch 41, Batch 50, Loss: 0.1496, Accuracy: 0.9609\n",
      "Epoch 41, Batch 60, Loss: 0.1410, Accuracy: 0.9688\n",
      "Epoch 41, Batch 70, Loss: 0.1401, Accuracy: 0.9688\n",
      "Epoch 41, Batch 80, Loss: 0.1178, Accuracy: 0.9609\n",
      "Epoch 41, Batch 90, Loss: 0.1408, Accuracy: 0.9609\n",
      "Epoch 41, Batch 100, Loss: 0.0705, Accuracy: 0.9766\n",
      "Epoch 41, Batch 110, Loss: 0.1471, Accuracy: 0.9531\n",
      "Epoch 41, Batch 120, Loss: 0.0651, Accuracy: 0.9844\n",
      "Epoch 41, Batch 130, Loss: 0.2571, Accuracy: 0.8984\n",
      "Epoch 41, Batch 140, Loss: 0.2294, Accuracy: 0.9141\n",
      "Epoch 41, Batch 150, Loss: 0.1108, Accuracy: 0.9688\n",
      "Epoch 41, Batch 160, Loss: 0.1941, Accuracy: 0.9453\n",
      "Epoch 41, Batch 170, Loss: 0.1638, Accuracy: 0.9609\n",
      "Epoch 41, Batch 180, Loss: 0.0938, Accuracy: 0.9766\n",
      "Epoch 41, Batch 190, Loss: 0.1321, Accuracy: 0.9688\n",
      "Epoch 41, Batch 200, Loss: 0.1248, Accuracy: 0.9688\n",
      "Epoch 41, Batch 210, Loss: 0.1884, Accuracy: 0.9375\n",
      "Epoch 41, Batch 220, Loss: 0.2133, Accuracy: 0.9375\n",
      "Epoch 41, Batch 230, Loss: 0.0661, Accuracy: 0.9766\n",
      "Epoch 41, Batch 240, Loss: 0.0890, Accuracy: 0.9609\n",
      "Epoch 41, Batch 250, Loss: 0.1091, Accuracy: 0.9688\n",
      "Epoch 41, Batch 260, Loss: 0.1862, Accuracy: 0.9297\n",
      "Epoch 41, Batch 270, Loss: 0.0849, Accuracy: 0.9688\n",
      "Epoch 41, Batch 280, Loss: 0.1116, Accuracy: 0.9688\n",
      "Epoch 41, Batch 290, Loss: 0.2347, Accuracy: 0.9375\n",
      "Epoch 41, Batch 300, Loss: 0.1908, Accuracy: 0.9609\n",
      "Epoch 41, Batch 310, Loss: 0.0975, Accuracy: 0.9609\n",
      "Epoch 41, Batch 320, Loss: 0.1794, Accuracy: 0.9297\n",
      "Epoch 41, Batch 330, Loss: 0.1506, Accuracy: 0.9375\n",
      "Epoch 41, Batch 340, Loss: 0.1063, Accuracy: 0.9688\n",
      "Epoch 41, Batch 350, Loss: 0.1528, Accuracy: 0.9531\n",
      "Epoch 41, Batch 360, Loss: 0.0830, Accuracy: 0.9688\n",
      "Epoch 41, Batch 370, Loss: 0.1341, Accuracy: 0.9688\n",
      "Epoch 41, Batch 380, Loss: 0.2423, Accuracy: 0.9062\n",
      "Epoch 41, Batch 390, Loss: 0.0935, Accuracy: 0.9609\n",
      "Epoch 41, Batch 400, Loss: 0.1414, Accuracy: 0.9297\n",
      "Epoch 41, Batch 410, Loss: 0.2610, Accuracy: 0.9375\n",
      "Epoch 41, Batch 420, Loss: 0.1004, Accuracy: 0.9609\n",
      "Epoch 41, Batch 430, Loss: 0.2552, Accuracy: 0.9297\n",
      "Epoch 41, Batch 440, Loss: 0.0639, Accuracy: 0.9844\n",
      "Epoch 41, Batch 450, Loss: 0.1007, Accuracy: 0.9531\n",
      "Epoch 41, Batch 460, Loss: 0.1070, Accuracy: 0.9766\n",
      "Epoch 41, Train Accuracy: 0.9576, Test Loss: 0.1371\n",
      "Epoch 41, Validation Accuracy: 0.9540, Validation Loss: 0.1536\n",
      "Epoch 42, Batch 0, Loss: 0.1166, Accuracy: 0.9531\n",
      "Epoch 42, Batch 10, Loss: 0.0863, Accuracy: 0.9844\n",
      "Epoch 42, Batch 20, Loss: 0.1004, Accuracy: 0.9688\n",
      "Epoch 42, Batch 30, Loss: 0.3102, Accuracy: 0.9062\n",
      "Epoch 42, Batch 40, Loss: 0.0396, Accuracy: 0.9922\n",
      "Epoch 42, Batch 50, Loss: 0.1699, Accuracy: 0.9688\n",
      "Epoch 42, Batch 60, Loss: 0.1359, Accuracy: 0.9453\n",
      "Epoch 42, Batch 70, Loss: 0.1323, Accuracy: 0.9688\n",
      "Epoch 42, Batch 80, Loss: 0.1142, Accuracy: 0.9531\n",
      "Epoch 42, Batch 90, Loss: 0.0933, Accuracy: 0.9844\n",
      "Epoch 42, Batch 100, Loss: 0.1021, Accuracy: 0.9688\n",
      "Epoch 42, Batch 110, Loss: 0.1435, Accuracy: 0.9609\n",
      "Epoch 42, Batch 120, Loss: 0.1609, Accuracy: 0.9531\n",
      "Epoch 42, Batch 130, Loss: 0.0682, Accuracy: 0.9844\n",
      "Epoch 42, Batch 140, Loss: 0.0965, Accuracy: 0.9609\n",
      "Epoch 42, Batch 150, Loss: 0.1840, Accuracy: 0.9609\n",
      "Epoch 42, Batch 160, Loss: 0.1334, Accuracy: 0.9609\n",
      "Epoch 42, Batch 170, Loss: 0.1182, Accuracy: 0.9844\n",
      "Epoch 42, Batch 180, Loss: 0.1305, Accuracy: 0.9688\n",
      "Epoch 42, Batch 190, Loss: 0.1416, Accuracy: 0.9531\n",
      "Epoch 42, Batch 200, Loss: 0.1537, Accuracy: 0.9688\n",
      "Epoch 42, Batch 210, Loss: 0.2778, Accuracy: 0.9375\n",
      "Epoch 42, Batch 220, Loss: 0.1451, Accuracy: 0.9609\n",
      "Epoch 42, Batch 230, Loss: 0.1489, Accuracy: 0.9609\n",
      "Epoch 42, Batch 240, Loss: 0.1496, Accuracy: 0.9375\n",
      "Epoch 42, Batch 250, Loss: 0.1840, Accuracy: 0.9453\n",
      "Epoch 42, Batch 260, Loss: 0.1700, Accuracy: 0.9609\n",
      "Epoch 42, Batch 270, Loss: 0.1081, Accuracy: 0.9766\n",
      "Epoch 42, Batch 280, Loss: 0.0801, Accuracy: 0.9844\n",
      "Epoch 42, Batch 290, Loss: 0.0768, Accuracy: 0.9766\n",
      "Epoch 42, Batch 300, Loss: 0.0865, Accuracy: 0.9844\n",
      "Epoch 42, Batch 310, Loss: 0.0782, Accuracy: 0.9766\n",
      "Epoch 42, Batch 320, Loss: 0.0980, Accuracy: 0.9688\n",
      "Epoch 42, Batch 330, Loss: 0.1239, Accuracy: 0.9609\n",
      "Epoch 42, Batch 340, Loss: 0.0968, Accuracy: 0.9766\n",
      "Epoch 42, Batch 350, Loss: 0.0976, Accuracy: 0.9531\n",
      "Epoch 42, Batch 360, Loss: 0.1480, Accuracy: 0.9531\n",
      "Epoch 42, Batch 370, Loss: 0.0804, Accuracy: 0.9766\n",
      "Epoch 42, Batch 380, Loss: 0.0449, Accuracy: 0.9766\n",
      "Epoch 42, Batch 390, Loss: 0.1445, Accuracy: 0.9688\n",
      "Epoch 42, Batch 400, Loss: 0.1376, Accuracy: 0.9609\n",
      "Epoch 42, Batch 410, Loss: 0.1357, Accuracy: 0.9609\n",
      "Epoch 42, Batch 420, Loss: 0.0867, Accuracy: 0.9609\n",
      "Epoch 42, Batch 430, Loss: 0.2316, Accuracy: 0.9531\n",
      "Epoch 42, Batch 440, Loss: 0.1301, Accuracy: 0.9688\n",
      "Epoch 42, Batch 450, Loss: 0.1729, Accuracy: 0.9453\n",
      "Epoch 42, Batch 460, Loss: 0.1342, Accuracy: 0.9531\n",
      "Epoch 42, Train Accuracy: 0.9586, Test Loss: 0.1350\n",
      "Epoch 42, Validation Accuracy: 0.9617, Validation Loss: 0.1343\n",
      "Epoch 43, Batch 0, Loss: 0.2261, Accuracy: 0.9375\n",
      "Epoch 43, Batch 10, Loss: 0.1098, Accuracy: 0.9609\n",
      "Epoch 43, Batch 20, Loss: 0.1572, Accuracy: 0.9609\n",
      "Epoch 43, Batch 30, Loss: 0.1033, Accuracy: 0.9688\n",
      "Epoch 43, Batch 40, Loss: 0.1122, Accuracy: 0.9609\n",
      "Epoch 43, Batch 50, Loss: 0.1114, Accuracy: 0.9688\n",
      "Epoch 43, Batch 60, Loss: 0.0671, Accuracy: 0.9766\n",
      "Epoch 43, Batch 70, Loss: 0.1998, Accuracy: 0.9297\n",
      "Epoch 43, Batch 80, Loss: 0.1385, Accuracy: 0.9531\n",
      "Epoch 43, Batch 90, Loss: 0.1520, Accuracy: 0.9297\n",
      "Epoch 43, Batch 100, Loss: 0.0973, Accuracy: 0.9688\n",
      "Epoch 43, Batch 110, Loss: 0.1473, Accuracy: 0.9688\n",
      "Epoch 43, Batch 120, Loss: 0.1628, Accuracy: 0.9453\n",
      "Epoch 43, Batch 130, Loss: 0.1681, Accuracy: 0.9609\n",
      "Epoch 43, Batch 140, Loss: 0.1487, Accuracy: 0.9531\n",
      "Epoch 43, Batch 150, Loss: 0.0978, Accuracy: 0.9766\n",
      "Epoch 43, Batch 160, Loss: 0.1068, Accuracy: 0.9766\n",
      "Epoch 43, Batch 170, Loss: 0.1879, Accuracy: 0.9609\n",
      "Epoch 43, Batch 180, Loss: 0.1402, Accuracy: 0.9531\n",
      "Epoch 43, Batch 190, Loss: 0.1120, Accuracy: 0.9766\n",
      "Epoch 43, Batch 200, Loss: 0.0903, Accuracy: 0.9766\n",
      "Epoch 43, Batch 210, Loss: 0.1084, Accuracy: 0.9766\n",
      "Epoch 43, Batch 220, Loss: 0.1669, Accuracy: 0.9688\n",
      "Epoch 43, Batch 230, Loss: 0.0930, Accuracy: 0.9844\n",
      "Epoch 43, Batch 240, Loss: 0.0514, Accuracy: 0.9844\n",
      "Epoch 43, Batch 250, Loss: 0.1053, Accuracy: 0.9688\n",
      "Epoch 43, Batch 260, Loss: 0.1322, Accuracy: 0.9609\n",
      "Epoch 43, Batch 270, Loss: 0.2450, Accuracy: 0.9297\n",
      "Epoch 43, Batch 280, Loss: 0.1599, Accuracy: 0.9531\n",
      "Epoch 43, Batch 290, Loss: 0.0959, Accuracy: 0.9688\n",
      "Epoch 43, Batch 300, Loss: 0.1982, Accuracy: 0.9531\n",
      "Epoch 43, Batch 310, Loss: 0.0914, Accuracy: 0.9766\n",
      "Epoch 43, Batch 320, Loss: 0.2002, Accuracy: 0.9531\n",
      "Epoch 43, Batch 330, Loss: 0.1645, Accuracy: 0.9688\n",
      "Epoch 43, Batch 340, Loss: 0.1820, Accuracy: 0.9531\n",
      "Epoch 43, Batch 350, Loss: 0.2084, Accuracy: 0.9375\n",
      "Epoch 43, Batch 360, Loss: 0.1168, Accuracy: 0.9531\n",
      "Epoch 43, Batch 370, Loss: 0.0912, Accuracy: 0.9766\n",
      "Epoch 43, Batch 380, Loss: 0.0930, Accuracy: 0.9766\n",
      "Epoch 43, Batch 390, Loss: 0.1356, Accuracy: 0.9766\n",
      "Epoch 43, Batch 400, Loss: 0.0959, Accuracy: 0.9766\n",
      "Epoch 43, Batch 410, Loss: 0.0962, Accuracy: 0.9688\n",
      "Epoch 43, Batch 420, Loss: 0.0929, Accuracy: 0.9766\n",
      "Epoch 43, Batch 430, Loss: 0.0935, Accuracy: 0.9688\n",
      "Epoch 43, Batch 440, Loss: 0.0627, Accuracy: 0.9688\n",
      "Epoch 43, Batch 450, Loss: 0.1148, Accuracy: 0.9453\n",
      "Epoch 43, Batch 460, Loss: 0.0618, Accuracy: 0.9766\n",
      "Epoch 43, Train Accuracy: 0.9604, Test Loss: 0.1309\n",
      "Epoch 43, Validation Accuracy: 0.9627, Validation Loss: 0.1362\n",
      "Epoch 44, Batch 0, Loss: 0.0816, Accuracy: 0.9844\n",
      "Epoch 44, Batch 10, Loss: 0.1893, Accuracy: 0.9297\n",
      "Epoch 44, Batch 20, Loss: 0.1817, Accuracy: 0.9453\n",
      "Epoch 44, Batch 30, Loss: 0.1112, Accuracy: 0.9609\n",
      "Epoch 44, Batch 40, Loss: 0.1022, Accuracy: 0.9531\n",
      "Epoch 44, Batch 50, Loss: 0.0775, Accuracy: 0.9766\n",
      "Epoch 44, Batch 60, Loss: 0.0523, Accuracy: 0.9844\n",
      "Epoch 44, Batch 70, Loss: 0.1052, Accuracy: 0.9688\n",
      "Epoch 44, Batch 80, Loss: 0.0815, Accuracy: 0.9844\n",
      "Epoch 44, Batch 90, Loss: 0.1768, Accuracy: 0.9297\n",
      "Epoch 44, Batch 100, Loss: 0.1121, Accuracy: 0.9688\n",
      "Epoch 44, Batch 110, Loss: 0.1651, Accuracy: 0.9453\n",
      "Epoch 44, Batch 120, Loss: 0.1121, Accuracy: 0.9531\n",
      "Epoch 44, Batch 130, Loss: 0.0886, Accuracy: 0.9766\n",
      "Epoch 44, Batch 140, Loss: 0.1141, Accuracy: 0.9688\n",
      "Epoch 44, Batch 150, Loss: 0.1772, Accuracy: 0.9609\n",
      "Epoch 44, Batch 160, Loss: 0.1324, Accuracy: 0.9375\n",
      "Epoch 44, Batch 170, Loss: 0.1284, Accuracy: 0.9609\n",
      "Epoch 44, Batch 180, Loss: 0.2188, Accuracy: 0.9609\n",
      "Epoch 44, Batch 190, Loss: 0.0472, Accuracy: 0.9844\n",
      "Epoch 44, Batch 200, Loss: 0.1796, Accuracy: 0.9609\n",
      "Epoch 44, Batch 210, Loss: 0.1370, Accuracy: 0.9766\n",
      "Epoch 44, Batch 220, Loss: 0.0655, Accuracy: 0.9844\n",
      "Epoch 44, Batch 230, Loss: 0.0993, Accuracy: 0.9688\n",
      "Epoch 44, Batch 240, Loss: 0.0643, Accuracy: 0.9844\n",
      "Epoch 44, Batch 250, Loss: 0.2643, Accuracy: 0.8906\n",
      "Epoch 44, Batch 260, Loss: 0.0878, Accuracy: 0.9609\n",
      "Epoch 44, Batch 270, Loss: 0.1010, Accuracy: 0.9688\n",
      "Epoch 44, Batch 280, Loss: 0.0664, Accuracy: 0.9766\n",
      "Epoch 44, Batch 290, Loss: 0.1143, Accuracy: 0.9531\n",
      "Epoch 44, Batch 300, Loss: 0.2470, Accuracy: 0.9453\n",
      "Epoch 44, Batch 310, Loss: 0.1603, Accuracy: 0.9531\n",
      "Epoch 44, Batch 320, Loss: 0.0827, Accuracy: 0.9688\n",
      "Epoch 44, Batch 330, Loss: 0.1821, Accuracy: 0.9453\n",
      "Epoch 44, Batch 340, Loss: 0.0975, Accuracy: 0.9766\n",
      "Epoch 44, Batch 350, Loss: 0.0956, Accuracy: 0.9531\n",
      "Epoch 44, Batch 360, Loss: 0.2150, Accuracy: 0.9531\n",
      "Epoch 44, Batch 370, Loss: 0.0653, Accuracy: 0.9844\n",
      "Epoch 44, Batch 380, Loss: 0.1729, Accuracy: 0.9375\n",
      "Epoch 44, Batch 390, Loss: 0.1052, Accuracy: 0.9609\n",
      "Epoch 44, Batch 400, Loss: 0.1673, Accuracy: 0.9453\n",
      "Epoch 44, Batch 410, Loss: 0.0600, Accuracy: 0.9844\n",
      "Epoch 44, Batch 420, Loss: 0.1289, Accuracy: 0.9531\n",
      "Epoch 44, Batch 430, Loss: 0.0758, Accuracy: 0.9766\n",
      "Epoch 44, Batch 440, Loss: 0.0803, Accuracy: 0.9688\n",
      "Epoch 44, Batch 450, Loss: 0.0399, Accuracy: 0.9844\n",
      "Epoch 44, Batch 460, Loss: 0.0815, Accuracy: 0.9766\n",
      "Epoch 44, Train Accuracy: 0.9606, Test Loss: 0.1303\n",
      "Epoch 44, Validation Accuracy: 0.9629, Validation Loss: 0.1295\n",
      "Epoch 45, Batch 0, Loss: 0.0827, Accuracy: 0.9766\n",
      "Epoch 45, Batch 10, Loss: 0.2681, Accuracy: 0.9219\n",
      "Epoch 45, Batch 20, Loss: 0.2207, Accuracy: 0.9531\n",
      "Epoch 45, Batch 30, Loss: 0.0867, Accuracy: 0.9609\n",
      "Epoch 45, Batch 40, Loss: 0.1110, Accuracy: 0.9688\n",
      "Epoch 45, Batch 50, Loss: 0.2035, Accuracy: 0.9375\n",
      "Epoch 45, Batch 60, Loss: 0.1482, Accuracy: 0.9375\n",
      "Epoch 45, Batch 70, Loss: 0.0757, Accuracy: 0.9688\n",
      "Epoch 45, Batch 80, Loss: 0.1493, Accuracy: 0.9375\n",
      "Epoch 45, Batch 90, Loss: 0.1105, Accuracy: 0.9609\n",
      "Epoch 45, Batch 100, Loss: 0.1137, Accuracy: 0.9766\n",
      "Epoch 45, Batch 110, Loss: 0.0882, Accuracy: 0.9688\n",
      "Epoch 45, Batch 120, Loss: 0.0722, Accuracy: 0.9688\n",
      "Epoch 45, Batch 130, Loss: 0.1485, Accuracy: 0.9531\n",
      "Epoch 45, Batch 140, Loss: 0.1922, Accuracy: 0.9375\n",
      "Epoch 45, Batch 150, Loss: 0.0936, Accuracy: 0.9688\n",
      "Epoch 45, Batch 160, Loss: 0.0627, Accuracy: 0.9844\n",
      "Epoch 45, Batch 170, Loss: 0.1597, Accuracy: 0.9375\n",
      "Epoch 45, Batch 180, Loss: 0.1859, Accuracy: 0.9609\n",
      "Epoch 45, Batch 190, Loss: 0.1539, Accuracy: 0.9688\n",
      "Epoch 45, Batch 200, Loss: 0.1560, Accuracy: 0.9531\n",
      "Epoch 45, Batch 210, Loss: 0.0655, Accuracy: 0.9844\n",
      "Epoch 45, Batch 220, Loss: 0.0656, Accuracy: 0.9844\n",
      "Epoch 45, Batch 230, Loss: 0.1554, Accuracy: 0.9609\n",
      "Epoch 45, Batch 240, Loss: 0.1390, Accuracy: 0.9688\n",
      "Epoch 45, Batch 250, Loss: 0.1251, Accuracy: 0.9766\n",
      "Epoch 45, Batch 260, Loss: 0.1542, Accuracy: 0.9609\n",
      "Epoch 45, Batch 270, Loss: 0.1209, Accuracy: 0.9609\n",
      "Epoch 45, Batch 280, Loss: 0.0957, Accuracy: 0.9609\n",
      "Epoch 45, Batch 290, Loss: 0.0747, Accuracy: 0.9766\n",
      "Epoch 45, Batch 300, Loss: 0.2664, Accuracy: 0.9219\n",
      "Epoch 45, Batch 310, Loss: 0.1872, Accuracy: 0.9609\n",
      "Epoch 45, Batch 320, Loss: 0.1684, Accuracy: 0.9375\n",
      "Epoch 45, Batch 330, Loss: 0.1080, Accuracy: 0.9609\n",
      "Epoch 45, Batch 340, Loss: 0.2735, Accuracy: 0.9297\n",
      "Epoch 45, Batch 350, Loss: 0.1476, Accuracy: 0.9688\n",
      "Epoch 45, Batch 360, Loss: 0.1401, Accuracy: 0.9688\n",
      "Epoch 45, Batch 370, Loss: 0.0560, Accuracy: 0.9844\n",
      "Epoch 45, Batch 380, Loss: 0.1750, Accuracy: 0.9453\n",
      "Epoch 45, Batch 390, Loss: 0.1146, Accuracy: 0.9609\n",
      "Epoch 45, Batch 400, Loss: 0.1260, Accuracy: 0.9766\n",
      "Epoch 45, Batch 410, Loss: 0.0535, Accuracy: 0.9922\n",
      "Epoch 45, Batch 420, Loss: 0.1288, Accuracy: 0.9453\n",
      "Epoch 45, Batch 430, Loss: 0.1846, Accuracy: 0.9453\n",
      "Epoch 45, Batch 440, Loss: 0.1150, Accuracy: 0.9766\n",
      "Epoch 45, Batch 450, Loss: 0.2069, Accuracy: 0.9531\n",
      "Epoch 45, Batch 460, Loss: 0.0754, Accuracy: 0.9688\n",
      "Epoch 45, Train Accuracy: 0.9605, Test Loss: 0.1290\n",
      "Epoch 45, Validation Accuracy: 0.9632, Validation Loss: 0.1298\n",
      "Epoch 46, Batch 0, Loss: 0.1177, Accuracy: 0.9688\n",
      "Epoch 46, Batch 10, Loss: 0.1605, Accuracy: 0.9688\n",
      "Epoch 46, Batch 20, Loss: 0.1201, Accuracy: 0.9609\n",
      "Epoch 46, Batch 30, Loss: 0.1710, Accuracy: 0.9375\n",
      "Epoch 46, Batch 40, Loss: 0.1341, Accuracy: 0.9453\n",
      "Epoch 46, Batch 50, Loss: 0.1206, Accuracy: 0.9531\n",
      "Epoch 46, Batch 60, Loss: 0.1909, Accuracy: 0.9375\n",
      "Epoch 46, Batch 70, Loss: 0.1027, Accuracy: 0.9688\n",
      "Epoch 46, Batch 80, Loss: 0.0799, Accuracy: 0.9766\n",
      "Epoch 46, Batch 90, Loss: 0.1410, Accuracy: 0.9609\n",
      "Epoch 46, Batch 100, Loss: 0.0996, Accuracy: 0.9688\n",
      "Epoch 46, Batch 110, Loss: 0.0809, Accuracy: 0.9766\n",
      "Epoch 46, Batch 120, Loss: 0.1402, Accuracy: 0.9297\n",
      "Epoch 46, Batch 130, Loss: 0.1570, Accuracy: 0.9531\n",
      "Epoch 46, Batch 140, Loss: 0.1678, Accuracy: 0.9375\n",
      "Epoch 46, Batch 150, Loss: 0.0923, Accuracy: 0.9688\n",
      "Epoch 46, Batch 160, Loss: 0.1350, Accuracy: 0.9609\n",
      "Epoch 46, Batch 170, Loss: 0.1142, Accuracy: 0.9609\n",
      "Epoch 46, Batch 180, Loss: 0.1094, Accuracy: 0.9531\n",
      "Epoch 46, Batch 190, Loss: 0.1054, Accuracy: 0.9766\n",
      "Epoch 46, Batch 200, Loss: 0.0592, Accuracy: 0.9844\n",
      "Epoch 46, Batch 210, Loss: 0.1466, Accuracy: 0.9453\n",
      "Epoch 46, Batch 220, Loss: 0.2365, Accuracy: 0.9375\n",
      "Epoch 46, Batch 230, Loss: 0.2007, Accuracy: 0.9375\n",
      "Epoch 46, Batch 240, Loss: 0.1735, Accuracy: 0.9531\n",
      "Epoch 46, Batch 250, Loss: 0.1837, Accuracy: 0.9297\n",
      "Epoch 46, Batch 260, Loss: 0.1301, Accuracy: 0.9609\n",
      "Epoch 46, Batch 270, Loss: 0.1012, Accuracy: 0.9531\n",
      "Epoch 46, Batch 280, Loss: 0.1820, Accuracy: 0.9453\n",
      "Epoch 46, Batch 290, Loss: 0.0669, Accuracy: 0.9766\n",
      "Epoch 46, Batch 300, Loss: 0.0740, Accuracy: 0.9609\n",
      "Epoch 46, Batch 310, Loss: 0.0774, Accuracy: 0.9844\n",
      "Epoch 46, Batch 320, Loss: 0.1304, Accuracy: 0.9609\n",
      "Epoch 46, Batch 330, Loss: 0.1144, Accuracy: 0.9609\n",
      "Epoch 46, Batch 340, Loss: 0.1447, Accuracy: 0.9531\n",
      "Epoch 46, Batch 350, Loss: 0.0428, Accuracy: 1.0000\n",
      "Epoch 46, Batch 360, Loss: 0.0494, Accuracy: 0.9922\n",
      "Epoch 46, Batch 370, Loss: 0.1360, Accuracy: 0.9531\n",
      "Epoch 46, Batch 380, Loss: 0.1651, Accuracy: 0.9609\n",
      "Epoch 46, Batch 390, Loss: 0.0989, Accuracy: 0.9609\n",
      "Epoch 46, Batch 400, Loss: 0.0846, Accuracy: 0.9844\n",
      "Epoch 46, Batch 410, Loss: 0.1547, Accuracy: 0.9297\n",
      "Epoch 46, Batch 420, Loss: 0.1645, Accuracy: 0.9531\n",
      "Epoch 46, Batch 430, Loss: 0.1378, Accuracy: 0.9453\n",
      "Epoch 46, Batch 440, Loss: 0.0620, Accuracy: 0.9766\n",
      "Epoch 46, Batch 450, Loss: 0.1441, Accuracy: 0.9453\n",
      "Epoch 46, Batch 460, Loss: 0.1927, Accuracy: 0.9375\n",
      "Epoch 46, Train Accuracy: 0.9604, Test Loss: 0.1267\n",
      "Epoch 46, Validation Accuracy: 0.9583, Validation Loss: 0.1381\n",
      "Epoch 47, Batch 0, Loss: 0.0560, Accuracy: 0.9844\n",
      "Epoch 47, Batch 10, Loss: 0.1319, Accuracy: 0.9688\n",
      "Epoch 47, Batch 20, Loss: 0.0568, Accuracy: 0.9922\n",
      "Epoch 47, Batch 30, Loss: 0.1747, Accuracy: 0.9609\n",
      "Epoch 47, Batch 40, Loss: 0.0976, Accuracy: 0.9766\n",
      "Epoch 47, Batch 50, Loss: 0.1751, Accuracy: 0.9453\n",
      "Epoch 47, Batch 60, Loss: 0.1236, Accuracy: 0.9531\n",
      "Epoch 47, Batch 70, Loss: 0.1275, Accuracy: 0.9531\n",
      "Epoch 47, Batch 80, Loss: 0.1767, Accuracy: 0.9375\n",
      "Epoch 47, Batch 90, Loss: 0.0494, Accuracy: 0.9922\n",
      "Epoch 47, Batch 100, Loss: 0.1055, Accuracy: 0.9688\n",
      "Epoch 47, Batch 110, Loss: 0.1466, Accuracy: 0.9688\n",
      "Epoch 47, Batch 120, Loss: 0.0975, Accuracy: 0.9688\n",
      "Epoch 47, Batch 130, Loss: 0.1802, Accuracy: 0.9219\n",
      "Epoch 47, Batch 140, Loss: 0.2207, Accuracy: 0.9453\n",
      "Epoch 47, Batch 150, Loss: 0.0966, Accuracy: 0.9609\n",
      "Epoch 47, Batch 160, Loss: 0.0721, Accuracy: 0.9844\n",
      "Epoch 47, Batch 170, Loss: 0.2164, Accuracy: 0.9531\n",
      "Epoch 47, Batch 180, Loss: 0.1488, Accuracy: 0.9609\n",
      "Epoch 47, Batch 190, Loss: 0.1403, Accuracy: 0.9531\n",
      "Epoch 47, Batch 200, Loss: 0.1529, Accuracy: 0.9609\n",
      "Epoch 47, Batch 210, Loss: 0.1639, Accuracy: 0.9688\n",
      "Epoch 47, Batch 220, Loss: 0.1576, Accuracy: 0.9609\n",
      "Epoch 47, Batch 230, Loss: 0.1004, Accuracy: 0.9844\n",
      "Epoch 47, Batch 240, Loss: 0.1000, Accuracy: 0.9688\n",
      "Epoch 47, Batch 250, Loss: 0.0908, Accuracy: 0.9688\n",
      "Epoch 47, Batch 260, Loss: 0.1972, Accuracy: 0.9453\n",
      "Epoch 47, Batch 270, Loss: 0.0822, Accuracy: 0.9688\n",
      "Epoch 47, Batch 280, Loss: 0.2242, Accuracy: 0.9453\n",
      "Epoch 47, Batch 290, Loss: 0.1877, Accuracy: 0.9375\n",
      "Epoch 47, Batch 300, Loss: 0.1336, Accuracy: 0.9609\n",
      "Epoch 47, Batch 310, Loss: 0.1229, Accuracy: 0.9609\n",
      "Epoch 47, Batch 320, Loss: 0.1040, Accuracy: 0.9766\n",
      "Epoch 47, Batch 330, Loss: 0.2491, Accuracy: 0.9219\n",
      "Epoch 47, Batch 340, Loss: 0.1466, Accuracy: 0.9531\n",
      "Epoch 47, Batch 350, Loss: 0.1021, Accuracy: 0.9766\n",
      "Epoch 47, Batch 360, Loss: 0.1219, Accuracy: 0.9766\n",
      "Epoch 47, Batch 370, Loss: 0.2019, Accuracy: 0.9531\n",
      "Epoch 47, Batch 380, Loss: 0.1143, Accuracy: 0.9531\n",
      "Epoch 47, Batch 390, Loss: 0.1180, Accuracy: 0.9688\n",
      "Epoch 47, Batch 400, Loss: 0.1489, Accuracy: 0.9375\n",
      "Epoch 47, Batch 410, Loss: 0.1604, Accuracy: 0.9609\n",
      "Epoch 47, Batch 420, Loss: 0.0999, Accuracy: 0.9688\n",
      "Epoch 47, Batch 430, Loss: 0.1777, Accuracy: 0.9375\n",
      "Epoch 47, Batch 440, Loss: 0.1625, Accuracy: 0.9609\n",
      "Epoch 47, Batch 450, Loss: 0.1369, Accuracy: 0.9688\n",
      "Epoch 47, Batch 460, Loss: 0.0904, Accuracy: 0.9688\n",
      "Epoch 47, Train Accuracy: 0.9620, Test Loss: 0.1246\n",
      "Epoch 47, Validation Accuracy: 0.9587, Validation Loss: 0.1375\n",
      "Epoch 48, Batch 0, Loss: 0.0993, Accuracy: 0.9531\n",
      "Epoch 48, Batch 10, Loss: 0.1018, Accuracy: 0.9531\n",
      "Epoch 48, Batch 20, Loss: 0.1173, Accuracy: 0.9688\n",
      "Epoch 48, Batch 30, Loss: 0.2196, Accuracy: 0.9375\n",
      "Epoch 48, Batch 40, Loss: 0.0620, Accuracy: 0.9766\n",
      "Epoch 48, Batch 50, Loss: 0.0535, Accuracy: 0.9922\n",
      "Epoch 48, Batch 60, Loss: 0.0934, Accuracy: 0.9531\n",
      "Epoch 48, Batch 70, Loss: 0.0730, Accuracy: 0.9766\n",
      "Epoch 48, Batch 80, Loss: 0.1396, Accuracy: 0.9609\n",
      "Epoch 48, Batch 90, Loss: 0.1038, Accuracy: 0.9375\n",
      "Epoch 48, Batch 100, Loss: 0.0583, Accuracy: 0.9766\n",
      "Epoch 48, Batch 110, Loss: 0.1944, Accuracy: 0.9453\n",
      "Epoch 48, Batch 120, Loss: 0.0896, Accuracy: 0.9844\n",
      "Epoch 48, Batch 130, Loss: 0.1247, Accuracy: 0.9609\n",
      "Epoch 48, Batch 140, Loss: 0.2233, Accuracy: 0.9141\n",
      "Epoch 48, Batch 150, Loss: 0.0529, Accuracy: 0.9844\n",
      "Epoch 48, Batch 160, Loss: 0.1152, Accuracy: 0.9688\n",
      "Epoch 48, Batch 170, Loss: 0.1113, Accuracy: 0.9766\n",
      "Epoch 48, Batch 180, Loss: 0.1679, Accuracy: 0.9453\n",
      "Epoch 48, Batch 190, Loss: 0.0432, Accuracy: 1.0000\n",
      "Epoch 48, Batch 200, Loss: 0.1177, Accuracy: 0.9531\n",
      "Epoch 48, Batch 210, Loss: 0.1543, Accuracy: 0.9531\n",
      "Epoch 48, Batch 220, Loss: 0.2127, Accuracy: 0.9453\n",
      "Epoch 48, Batch 230, Loss: 0.1380, Accuracy: 0.9609\n",
      "Epoch 48, Batch 240, Loss: 0.1789, Accuracy: 0.9297\n",
      "Epoch 48, Batch 250, Loss: 0.0428, Accuracy: 0.9922\n",
      "Epoch 48, Batch 260, Loss: 0.1133, Accuracy: 0.9609\n",
      "Epoch 48, Batch 270, Loss: 0.1305, Accuracy: 0.9297\n",
      "Epoch 48, Batch 280, Loss: 0.1439, Accuracy: 0.9766\n",
      "Epoch 48, Batch 290, Loss: 0.0832, Accuracy: 0.9766\n",
      "Epoch 48, Batch 300, Loss: 0.1334, Accuracy: 0.9609\n",
      "Epoch 48, Batch 310, Loss: 0.1737, Accuracy: 0.9453\n",
      "Epoch 48, Batch 320, Loss: 0.0897, Accuracy: 0.9688\n",
      "Epoch 48, Batch 330, Loss: 0.1251, Accuracy: 0.9688\n",
      "Epoch 48, Batch 340, Loss: 0.0678, Accuracy: 0.9844\n",
      "Epoch 48, Batch 350, Loss: 0.1596, Accuracy: 0.9453\n",
      "Epoch 48, Batch 360, Loss: 0.2040, Accuracy: 0.9609\n",
      "Epoch 48, Batch 370, Loss: 0.1238, Accuracy: 0.9531\n",
      "Epoch 48, Batch 380, Loss: 0.1372, Accuracy: 0.9609\n",
      "Epoch 48, Batch 390, Loss: 0.0584, Accuracy: 0.9688\n",
      "Epoch 48, Batch 400, Loss: 0.1357, Accuracy: 0.9531\n",
      "Epoch 48, Batch 410, Loss: 0.1525, Accuracy: 0.9688\n",
      "Epoch 48, Batch 420, Loss: 0.1548, Accuracy: 0.9609\n",
      "Epoch 48, Batch 430, Loss: 0.1225, Accuracy: 0.9609\n",
      "Epoch 48, Batch 440, Loss: 0.1056, Accuracy: 0.9766\n",
      "Epoch 48, Batch 450, Loss: 0.1121, Accuracy: 0.9766\n",
      "Epoch 48, Batch 460, Loss: 0.0941, Accuracy: 0.9609\n",
      "Epoch 48, Train Accuracy: 0.9620, Test Loss: 0.1233\n",
      "Epoch 48, Validation Accuracy: 0.9564, Validation Loss: 0.1415\n",
      "Epoch 49, Batch 0, Loss: 0.1689, Accuracy: 0.9453\n",
      "Epoch 49, Batch 10, Loss: 0.1444, Accuracy: 0.9531\n",
      "Epoch 49, Batch 20, Loss: 0.1213, Accuracy: 0.9766\n",
      "Epoch 49, Batch 30, Loss: 0.1537, Accuracy: 0.9531\n",
      "Epoch 49, Batch 40, Loss: 0.2138, Accuracy: 0.9219\n",
      "Epoch 49, Batch 50, Loss: 0.0569, Accuracy: 0.9844\n",
      "Epoch 49, Batch 60, Loss: 0.0347, Accuracy: 0.9922\n",
      "Epoch 49, Batch 70, Loss: 0.1424, Accuracy: 0.9609\n",
      "Epoch 49, Batch 80, Loss: 0.0872, Accuracy: 0.9766\n",
      "Epoch 49, Batch 90, Loss: 0.1455, Accuracy: 0.9531\n",
      "Epoch 49, Batch 100, Loss: 0.2251, Accuracy: 0.9453\n",
      "Epoch 49, Batch 110, Loss: 0.0821, Accuracy: 0.9766\n",
      "Epoch 49, Batch 120, Loss: 0.0639, Accuracy: 0.9844\n",
      "Epoch 49, Batch 130, Loss: 0.1778, Accuracy: 0.9297\n",
      "Epoch 49, Batch 140, Loss: 0.1053, Accuracy: 0.9766\n",
      "Epoch 49, Batch 150, Loss: 0.0439, Accuracy: 0.9922\n",
      "Epoch 49, Batch 160, Loss: 0.0883, Accuracy: 0.9766\n",
      "Epoch 49, Batch 170, Loss: 0.0547, Accuracy: 0.9844\n",
      "Epoch 49, Batch 180, Loss: 0.0926, Accuracy: 0.9609\n",
      "Epoch 49, Batch 190, Loss: 0.2222, Accuracy: 0.9297\n",
      "Epoch 49, Batch 200, Loss: 0.1165, Accuracy: 0.9609\n",
      "Epoch 49, Batch 210, Loss: 0.0437, Accuracy: 0.9766\n",
      "Epoch 49, Batch 220, Loss: 0.0635, Accuracy: 0.9844\n",
      "Epoch 49, Batch 230, Loss: 0.0648, Accuracy: 0.9844\n",
      "Epoch 49, Batch 240, Loss: 0.1587, Accuracy: 0.9531\n",
      "Epoch 49, Batch 250, Loss: 0.1234, Accuracy: 0.9688\n",
      "Epoch 49, Batch 260, Loss: 0.1574, Accuracy: 0.9688\n",
      "Epoch 49, Batch 270, Loss: 0.1050, Accuracy: 0.9609\n",
      "Epoch 49, Batch 280, Loss: 0.1900, Accuracy: 0.9531\n",
      "Epoch 49, Batch 290, Loss: 0.1551, Accuracy: 0.9766\n",
      "Epoch 49, Batch 300, Loss: 0.1415, Accuracy: 0.9609\n",
      "Epoch 49, Batch 310, Loss: 0.2524, Accuracy: 0.9141\n",
      "Epoch 49, Batch 320, Loss: 0.1075, Accuracy: 0.9766\n",
      "Epoch 49, Batch 330, Loss: 0.1227, Accuracy: 0.9609\n",
      "Epoch 49, Batch 340, Loss: 0.1786, Accuracy: 0.9297\n",
      "Epoch 49, Batch 350, Loss: 0.1094, Accuracy: 0.9609\n",
      "Epoch 49, Batch 360, Loss: 0.1985, Accuracy: 0.9609\n",
      "Epoch 49, Batch 370, Loss: 0.0861, Accuracy: 0.9688\n",
      "Epoch 49, Batch 380, Loss: 0.1084, Accuracy: 0.9766\n",
      "Epoch 49, Batch 390, Loss: 0.1204, Accuracy: 0.9688\n",
      "Epoch 49, Batch 400, Loss: 0.0878, Accuracy: 0.9688\n",
      "Epoch 49, Batch 410, Loss: 0.0905, Accuracy: 0.9844\n",
      "Epoch 49, Batch 420, Loss: 0.0987, Accuracy: 0.9766\n",
      "Epoch 49, Batch 430, Loss: 0.1980, Accuracy: 0.9375\n",
      "Epoch 49, Batch 440, Loss: 0.0556, Accuracy: 0.9844\n",
      "Epoch 49, Batch 450, Loss: 0.1471, Accuracy: 0.9609\n",
      "Epoch 49, Batch 460, Loss: 0.1754, Accuracy: 0.9453\n",
      "Epoch 49, Train Accuracy: 0.9620, Test Loss: 0.1245\n",
      "Epoch 49, Validation Accuracy: 0.9529, Validation Loss: 0.1664\n",
      "Epoch 50, Batch 0, Loss: 0.1435, Accuracy: 0.9688\n",
      "Epoch 50, Batch 10, Loss: 0.0688, Accuracy: 0.9688\n",
      "Epoch 50, Batch 20, Loss: 0.2376, Accuracy: 0.9531\n",
      "Epoch 50, Batch 30, Loss: 0.1026, Accuracy: 0.9766\n",
      "Epoch 50, Batch 40, Loss: 0.1626, Accuracy: 0.9609\n",
      "Epoch 50, Batch 50, Loss: 0.0898, Accuracy: 0.9766\n",
      "Epoch 50, Batch 60, Loss: 0.1498, Accuracy: 0.9531\n",
      "Epoch 50, Batch 70, Loss: 0.0383, Accuracy: 0.9844\n",
      "Epoch 50, Batch 80, Loss: 0.0757, Accuracy: 0.9688\n",
      "Epoch 50, Batch 90, Loss: 0.0476, Accuracy: 0.9844\n",
      "Epoch 50, Batch 100, Loss: 0.2043, Accuracy: 0.9453\n",
      "Epoch 50, Batch 110, Loss: 0.0813, Accuracy: 0.9766\n",
      "Epoch 50, Batch 120, Loss: 0.0538, Accuracy: 0.9844\n",
      "Epoch 50, Batch 130, Loss: 0.1679, Accuracy: 0.9453\n",
      "Epoch 50, Batch 140, Loss: 0.1023, Accuracy: 0.9844\n",
      "Epoch 50, Batch 150, Loss: 0.1529, Accuracy: 0.9297\n",
      "Epoch 50, Batch 160, Loss: 0.1788, Accuracy: 0.9375\n",
      "Epoch 50, Batch 170, Loss: 0.0574, Accuracy: 1.0000\n",
      "Epoch 50, Batch 180, Loss: 0.1234, Accuracy: 0.9609\n",
      "Epoch 50, Batch 190, Loss: 0.1236, Accuracy: 0.9609\n",
      "Epoch 50, Batch 200, Loss: 0.0974, Accuracy: 0.9531\n",
      "Epoch 50, Batch 210, Loss: 0.2443, Accuracy: 0.9453\n",
      "Epoch 50, Batch 220, Loss: 0.0805, Accuracy: 0.9766\n",
      "Epoch 50, Batch 230, Loss: 0.1067, Accuracy: 0.9844\n",
      "Epoch 50, Batch 240, Loss: 0.1365, Accuracy: 0.9297\n",
      "Epoch 50, Batch 250, Loss: 0.1461, Accuracy: 0.9609\n",
      "Epoch 50, Batch 260, Loss: 0.1102, Accuracy: 0.9609\n",
      "Epoch 50, Batch 270, Loss: 0.2384, Accuracy: 0.9531\n",
      "Epoch 50, Batch 280, Loss: 0.1121, Accuracy: 0.9375\n",
      "Epoch 50, Batch 290, Loss: 0.1018, Accuracy: 0.9766\n",
      "Epoch 50, Batch 300, Loss: 0.0600, Accuracy: 0.9688\n",
      "Epoch 50, Batch 310, Loss: 0.1819, Accuracy: 0.9375\n",
      "Epoch 50, Batch 320, Loss: 0.1337, Accuracy: 0.9766\n",
      "Epoch 50, Batch 330, Loss: 0.1274, Accuracy: 0.9609\n",
      "Epoch 50, Batch 340, Loss: 0.1295, Accuracy: 0.9531\n",
      "Epoch 50, Batch 350, Loss: 0.1744, Accuracy: 0.9375\n",
      "Epoch 50, Batch 360, Loss: 0.1220, Accuracy: 0.9688\n",
      "Epoch 50, Batch 370, Loss: 0.1860, Accuracy: 0.9375\n",
      "Epoch 50, Batch 380, Loss: 0.1404, Accuracy: 0.9531\n",
      "Epoch 50, Batch 390, Loss: 0.0676, Accuracy: 0.9844\n",
      "Epoch 50, Batch 400, Loss: 0.2066, Accuracy: 0.9609\n",
      "Epoch 50, Batch 410, Loss: 0.1302, Accuracy: 0.9688\n",
      "Epoch 50, Batch 420, Loss: 0.1209, Accuracy: 0.9531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[269], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(state, train_data, test_data, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[268], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(state, train_ds, test_ds, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     tr_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_ds)\n\u001b[1;32m     36\u001b[0m tr_acc \u001b[38;5;241m=\u001b[39m tr_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_ds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/array.py:295\u001b[0m, in \u001b[0;36mArrayImpl.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec):\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;66;03m# Simulates behavior of https://github.com/numpy/numpy/pull/9883\u001b[39;00m\n\u001b[1;32m    294\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value[()], format_spec)\n\u001b[1;32m    296\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value, format_spec)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/array.py:594\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_array_to_np_array()\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(state, train_data, test_data, num_epochs = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
