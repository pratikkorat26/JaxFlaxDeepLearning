{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "412a2740-72cd-4cef-bd3a-e68affe2b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "from jax import numpy as jnp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from typing import *\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "01aab412-11e5-496b-982e-1bf6eea56cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20_000\n",
    "LATENT_DIM = 16\n",
    "key = jax.random.PRNGKey(10)\n",
    "GEN_LEARNING_RATE = 1e-4\n",
    "DISC_LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "171873c8-2dcb-4c68-8c08-23a8b9cf9b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.4726117 ,  0.3460033 ],\n",
       "       [-0.75647926,  0.07138681]], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, key_generator = jax.random.split(key, 2)\n",
    "jax.random.normal(key_generator, shape = (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "563b2fe4-591b-40ba-bb58-715725af6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, buffer_size):\n",
    "    train_ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
    "    test_ds = tfds.load('mnist', split='test', shuffle_files = True)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        image, label = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "        image = (tf.cast(image, dtype = tf.float32) - 127.5) / 127.5\n",
    "\n",
    "        return {\"image\": image}\n",
    "\n",
    "    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size = batch_size, drop_remainder=True)\n",
    "    train_ds = train_ds.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.cache()\n",
    "    \n",
    "    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size = batch_size, drop_remainder=True)\n",
    "    test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache()\n",
    "    \n",
    "    train_ds = tfds.as_numpy(train_ds)\n",
    "    test_ds = tfds.as_numpy(test_ds)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c3974ecf-21a2-48df-a448-9d7f5e7f577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data(batch_size = BATCH_SIZE, buffer_size = BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "34f2e44c-42e8-48be-8df8-2974e546807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 18:06:48.812155: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-07-17 18:06:48.812400: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data:\n",
    "    print(batch[\"image\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77fec7-8c6a-4724-9a87-ab769e972d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f16bc13d-ad5f-4c2d-ae8a-b0661a07cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    in_channels: Any\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        x = nn.Dense(7 * 7 * self.in_channels)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = x.reshape((-1, 7, 7, self.in_channels))\n",
    "        x = nn.ConvTranspose(features=128, kernel_size=(4, 4), strides=(2, 2), padding='SAME')(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.ConvTranspose(features=128, kernel_size=(4, 4), strides=(2, 2), padding='SAME')(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=1, kernel_size=(7, 7), padding='SAME')(x)\n",
    "        x = nn.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=128, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = jnp.max(x, axis=(1, 2))\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6c05c91a-9b1b-4592-9d48-a0cb5a615ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(in_channels = 16)\n",
    "gen_x = jnp.ones(shape = (BATCH_SIZE, LATENT_DIM))\n",
    "gen_params = generator.init(jax.random.key(10), x = gen_x, train = False)\n",
    "\n",
    "gen_out = generator.apply(gen_params, x = gen_x, train = False)\n",
    "print(gen_out.shape)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "disc_x = jnp.ones(shape = (BATCH_SIZE, 28, 28, 1))\n",
    "disc_params = discriminator.init(jax.random.key(11), x = disc_x, train = False)\n",
    "\n",
    "disc_out = discriminator.apply(disc_params, x = disc_x, train = False)\n",
    "print(disc_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4e5b4370-924c-4b0e-b70b-2bc771a5c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                               Generator Summary                                \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams      \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
      "│               │ Generator     │ train: False  │ \u001b[2mfloat32\u001b[0m[64,2… │              │\n",
      "│               │               │ x:            │               │              │\n",
      "│               │               │ \u001b[2mfloat32\u001b[0m[64,1… │               │              │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Dense_0       │ Dense         │ \u001b[2mfloat32\u001b[0m[64,1… │ \u001b[2mfloat32\u001b[0m[64,7… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[784] │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[16,… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m13,328 \u001b[0m\u001b[1;2m(53.3\u001b[0m │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ ConvTranspos… │ ConvTranspose │ \u001b[2mfloat32\u001b[0m[64,7… │ \u001b[2mfloat32\u001b[0m[64,1… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[128] │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[4,4… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m32,896 \u001b[0m      │\n",
      "│               │               │               │               │ \u001b[1;2m(131.6 KB)\u001b[0m   │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ ConvTranspos… │ ConvTranspose │ \u001b[2mfloat32\u001b[0m[64,1… │ \u001b[2mfloat32\u001b[0m[64,2… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[128] │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[4,4… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m262,272 \u001b[0m\u001b[1;2m(1.0\u001b[0m │\n",
      "│               │               │               │               │ \u001b[1;2mMB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│ Conv_0        │ Conv          │ \u001b[2mfloat32\u001b[0m[64,2… │ \u001b[2mfloat32\u001b[0m[64,2… │ bias:        │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[1]   │\n",
      "│               │               │               │               │ kernel:      │\n",
      "│               │               │               │               │ \u001b[2mfloat32\u001b[0m[7,7… │\n",
      "│               │               │               │               │              │\n",
      "│               │               │               │               │ \u001b[1m6,273 \u001b[0m\u001b[1;2m(25.1 \u001b[0m │\n",
      "│               │               │               │               │ \u001b[1;2mKB)\u001b[0m          │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼──────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m        Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m314,769 \u001b[0m\u001b[1;2m(1.3\u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mMB)\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────────┴───────────────┴───────────────┴───────────────┴──────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 314,769 \u001b[0m\u001b[1;2m(1.3 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generator.tabulate(jax.random.key(0), x = gen_x, train = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f08b11a8-8b47-4ca9-b398-dd828929715e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                             Discriminator Summary                              \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams         \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Discriminator │ train: False    │ \u001b[2mfloat32\u001b[0m[64,1]  │                 │\n",
      "│         │               │ x:              │                │                 │\n",
      "│         │               │ \u001b[2mfloat32\u001b[0m[64,28,… │                │                 │\n",
      "├─────────┼───────────────┼─────────────────┼────────────────┼─────────────────┤\n",
      "│ Conv_0  │ Conv          │ \u001b[2mfloat32\u001b[0m[64,28,… │ \u001b[2mfloat32\u001b[0m[64,14… │ bias:           │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[64]     │\n",
      "│         │               │                 │                │ kernel:         │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[3,3,1,… │\n",
      "│         │               │                 │                │                 │\n",
      "│         │               │                 │                │ \u001b[1m640 \u001b[0m\u001b[1;2m(2.6 KB)\u001b[0m    │\n",
      "├─────────┼───────────────┼─────────────────┼────────────────┼─────────────────┤\n",
      "│ Conv_1  │ Conv          │ \u001b[2mfloat32\u001b[0m[64,14,… │ \u001b[2mfloat32\u001b[0m[64,7,… │ bias:           │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[128]    │\n",
      "│         │               │                 │                │ kernel:         │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[3,3,64… │\n",
      "│         │               │                 │                │                 │\n",
      "│         │               │                 │                │ \u001b[1m73,856 \u001b[0m\u001b[1;2m(295.4 \u001b[0m  │\n",
      "│         │               │                 │                │ \u001b[1;2mKB)\u001b[0m             │\n",
      "├─────────┼───────────────┼─────────────────┼────────────────┼─────────────────┤\n",
      "│ Dense_0 │ Dense         │ \u001b[2mfloat32\u001b[0m[64,128] │ \u001b[2mfloat32\u001b[0m[64,1]  │ bias:           │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[1]      │\n",
      "│         │               │                 │                │ kernel:         │\n",
      "│         │               │                 │                │ \u001b[2mfloat32\u001b[0m[128,1]  │\n",
      "│         │               │                 │                │                 │\n",
      "│         │               │                 │                │ \u001b[1m129 \u001b[0m\u001b[1;2m(516 B)\u001b[0m     │\n",
      "├─────────┼───────────────┼─────────────────┼────────────────┼─────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m74,625 \u001b[0m\u001b[1;2m(298.5 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m         \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m                 \u001b[0m│\u001b[1m                \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mKB)\u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴───────────────┴─────────────────┴────────────────┴─────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 74,625 \u001b[0m\u001b[1;2m(298.5 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(discriminator.tabulate(jax.random.key(0), x = disc_x, train = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e586b1-a7e7-4100-9ef8-9b4c5f56dc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "bf136906-b68a-48b8-86ea-5a3a193210aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "def loss_function(logits, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        logits : unnormalized logits output from discriminator\n",
    "        labels : labels for the images\n",
    "    \"\"\"\n",
    "    loss = optax.losses.sigmoid_binary_cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def train_step(batch, generator_state, discriminator_state, key):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    key, key_generator = jax.random.split(key)\n",
    "    \n",
    "    # We need random noise of shape -> (BATCH_SIZE, LATENT_DIM)\n",
    "    noise = jax.random.normal(key = key, shape = (BATCH_SIZE, LATENT_DIM))\n",
    "    real_data = batch[\"image\"]\n",
    "    \n",
    "    # define loss function for generator model\n",
    "    def gen_loss_fn(params):\n",
    "        generated_data = generator_state.apply_fn(\n",
    "            params,\n",
    "            x = noise, train = True\n",
    "        )\n",
    "        logits = discriminator_state.apply_fn(\n",
    "            discriminator_state.params,\n",
    "            x = generated_data, train = True\n",
    "        )\n",
    "\n",
    "        loss = -jnp.mean(jnp.log(nn.sigmoid(logits)))\n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "    grads_gen_fn = jax.value_and_grad(fun = gen_loss_fn, has_aux = True)\n",
    "    (gen_loss, gen_logits), grads_gen = grads_gen_fn(generator_state.params)\n",
    "\n",
    "    new_generator_state = generator_state.apply_gradients(\n",
    "        grads=grads_gen\n",
    "    )\n",
    "\n",
    "    noise = jax.random.normal(key, shape = (BATCH_SIZE, LATENT_DIM))\n",
    "    generated_data = generator_state.apply_fn(\n",
    "            generator_state.params,\n",
    "            x = noise, train = True\n",
    "        )\n",
    "    \n",
    "    def disc_loss_fn(params):\n",
    "        \n",
    "\n",
    "        all_data = jnp.concatenate([real_data, generated_data], axis = 0)\n",
    "        all_labels = jnp.concatenate([\n",
    "            jnp.ones(shape = (BATCH_SIZE, 1)),\n",
    "            jnp.zeros(shape = (BATCH_SIZE, 1))\n",
    "        ], axis = 0)\n",
    "        \n",
    "        logits = discriminator_state.apply_fn(\n",
    "            params,\n",
    "            x = all_data, train = True\n",
    "        )\n",
    "\n",
    "        loss = jnp.mean(loss_function(logits, all_labels))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    # This two line will calculate the loss value for given generated data\n",
    "    grads_disc_fn = jax.value_and_grad(fun = disc_loss_fn, has_aux = True)\n",
    "    (disc_loss, disc_logits), grads_disc = grads_disc_fn(discriminator_state.params)\n",
    "\n",
    "    new_discriminator_state = discriminator_state.apply_gradients(\n",
    "        grads = grads_disc\n",
    "    )\n",
    "    \n",
    "    return new_generator_state, new_discriminator_state, gen_loss, disc_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "2055aade-6211-4aa8-8a7b-273977c96a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_train_state(model, parameters, tx):\n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        params = parameters,\n",
    "        tx = tx,\n",
    "    )\n",
    "\n",
    "    return state\n",
    "\n",
    "# Generator\n",
    "generator = Generator(in_channels = 16)\n",
    "generator_parameters = generator.init(jax.random.key(0),\n",
    "                                      x = jnp.ones(shape = (BATCH_SIZE, LATENT_DIM)),\n",
    "                                     train = True)\n",
    "gen_tx = optax.adam(learning_rate = GEN_LEARNING_RATE)\n",
    "generator_state = create_train_state(model=generator, parameters=generator_parameters, tx=gen_tx)\n",
    "\n",
    "# Discriminator\n",
    "discriminator = Discriminator(name = \"Discriminator\")\n",
    "discriminator_parameters = discriminator.init(jax.random.key(0),\n",
    "                                             x = jnp.ones(shape = (BATCH_SIZE, 28, 28, 1)),\n",
    "                                             train = True)\n",
    "disc_tx = optax.adam(learning_rate = DISC_LEARNING_RATE)\n",
    "discriminator_state = create_train_state(model=discriminator, parameters=discriminator_parameters, tx=disc_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "8a3a5818-77e0-4c4a-b845-721092d3e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_model(generator_state,\n",
    "                discriminator_state,\n",
    "                train_dataloader,\n",
    "                test_dataloader,\n",
    "                epochs,\n",
    "                key):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0\n",
    "        for i,batch in enumerate(train_dataloader):\n",
    "            key, key_generator = jax.random.split(key, 2)\n",
    "            generator_state, discriminator_state, gen_loss, disc_loss = train_step(\n",
    "                batch = batch,\n",
    "                generator_state = generator_state,\n",
    "                discriminator_state = discriminator_state,\n",
    "                key = key_generator\n",
    "            )\n",
    "            train_loss += (gen_loss + disc_loss) / 2\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch: {epoch} | Batch : {i} | Generator Loss: {gen_loss:.04f} | Discriminator Loss: {disc_loss:.04f}\")\n",
    "        print(f\"Epoch : {epoch} | Total Trainig Loss : {train_loss / len(train_dataloader)}\")\n",
    "\n",
    "    return generator_state, discriminator_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "00a9dd4a-1fc4-42db-9c8c-738aa0c1b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9612e14504844df28d963941e126436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch : 0 | Generator Loss: 0.6533 | Discriminator Loss: 0.5938\n",
      "Epoch: 0 | Batch : 100 | Generator Loss: 1.3398 | Discriminator Loss: 0.1831\n",
      "Epoch: 0 | Batch : 200 | Generator Loss: 3.2310 | Discriminator Loss: 0.0480\n",
      "Epoch: 0 | Batch : 300 | Generator Loss: 0.5694 | Discriminator Loss: 0.4780\n",
      "Epoch: 0 | Batch : 400 | Generator Loss: 0.7766 | Discriminator Loss: 0.3852\n",
      "Epoch: 0 | Batch : 500 | Generator Loss: 0.9910 | Discriminator Loss: 0.4208\n",
      "Epoch: 0 | Batch : 600 | Generator Loss: 1.0434 | Discriminator Loss: 0.4262\n",
      "Epoch: 0 | Batch : 700 | Generator Loss: 1.2343 | Discriminator Loss: 0.4143\n",
      "Epoch: 0 | Batch : 800 | Generator Loss: 1.2009 | Discriminator Loss: 0.4355\n",
      "Epoch: 0 | Batch : 900 | Generator Loss: 1.1609 | Discriminator Loss: 0.4467\n",
      "Epoch : 0 | Total Trainig Loss : 0.7544053196907043\n",
      "Epoch: 1 | Batch : 0 | Generator Loss: 1.3158 | Discriminator Loss: 0.3516\n",
      "Epoch: 1 | Batch : 100 | Generator Loss: 1.3964 | Discriminator Loss: 0.3409\n",
      "Epoch: 1 | Batch : 200 | Generator Loss: 1.3466 | Discriminator Loss: 0.3429\n",
      "Epoch: 1 | Batch : 300 | Generator Loss: 3.1380 | Discriminator Loss: 0.0564\n",
      "Epoch: 1 | Batch : 400 | Generator Loss: 4.1070 | Discriminator Loss: 0.0133\n",
      "Epoch: 1 | Batch : 500 | Generator Loss: 4.5863 | Discriminator Loss: 0.0090\n",
      "Epoch: 1 | Batch : 600 | Generator Loss: 4.8028 | Discriminator Loss: 0.0102\n",
      "Epoch: 1 | Batch : 700 | Generator Loss: 1.9703 | Discriminator Loss: 0.3525\n",
      "Epoch: 1 | Batch : 800 | Generator Loss: 1.3616 | Discriminator Loss: 0.4309\n",
      "Epoch: 1 | Batch : 900 | Generator Loss: 1.4456 | Discriminator Loss: 0.3839\n",
      "Epoch : 1 | Total Trainig Loss : 1.4076474905014038\n",
      "Epoch: 2 | Batch : 0 | Generator Loss: 1.4488 | Discriminator Loss: 0.3525\n",
      "Epoch: 2 | Batch : 100 | Generator Loss: 1.1738 | Discriminator Loss: 0.4248\n",
      "Epoch: 2 | Batch : 200 | Generator Loss: 1.1844 | Discriminator Loss: 0.4500\n",
      "Epoch: 2 | Batch : 300 | Generator Loss: 1.0859 | Discriminator Loss: 0.5978\n",
      "Epoch: 2 | Batch : 400 | Generator Loss: 0.9678 | Discriminator Loss: 0.5099\n",
      "Epoch: 2 | Batch : 500 | Generator Loss: 0.9976 | Discriminator Loss: 0.5108\n",
      "Epoch: 2 | Batch : 600 | Generator Loss: 1.0846 | Discriminator Loss: 0.5101\n",
      "Epoch: 2 | Batch : 700 | Generator Loss: 1.3276 | Discriminator Loss: 0.3826\n",
      "Epoch: 2 | Batch : 800 | Generator Loss: 1.1427 | Discriminator Loss: 0.4243\n",
      "Epoch: 2 | Batch : 900 | Generator Loss: 0.9801 | Discriminator Loss: 0.5468\n",
      "Epoch : 2 | Total Trainig Loss : 0.8216564059257507\n",
      "Epoch: 3 | Batch : 0 | Generator Loss: 1.0760 | Discriminator Loss: 0.4420\n",
      "Epoch: 3 | Batch : 100 | Generator Loss: 1.1300 | Discriminator Loss: 0.4191\n",
      "Epoch: 3 | Batch : 200 | Generator Loss: 1.0820 | Discriminator Loss: 0.5452\n",
      "Epoch: 3 | Batch : 300 | Generator Loss: 1.1333 | Discriminator Loss: 0.4682\n",
      "Epoch: 3 | Batch : 400 | Generator Loss: 1.1065 | Discriminator Loss: 0.4992\n",
      "Epoch: 3 | Batch : 500 | Generator Loss: 1.3050 | Discriminator Loss: 0.4107\n",
      "Epoch: 3 | Batch : 600 | Generator Loss: 1.1010 | Discriminator Loss: 0.5591\n",
      "Epoch: 3 | Batch : 700 | Generator Loss: 1.1097 | Discriminator Loss: 0.4280\n",
      "Epoch: 3 | Batch : 800 | Generator Loss: 1.1200 | Discriminator Loss: 0.4121\n",
      "Epoch: 3 | Batch : 900 | Generator Loss: 1.0326 | Discriminator Loss: 0.5710\n",
      "Epoch : 3 | Total Trainig Loss : 0.8300143480300903\n",
      "Epoch: 4 | Batch : 0 | Generator Loss: 1.1560 | Discriminator Loss: 0.4801\n",
      "Epoch: 4 | Batch : 100 | Generator Loss: 1.2670 | Discriminator Loss: 0.4497\n",
      "Epoch: 4 | Batch : 200 | Generator Loss: 1.0173 | Discriminator Loss: 0.5954\n",
      "Epoch: 4 | Batch : 300 | Generator Loss: 1.2957 | Discriminator Loss: 0.4181\n",
      "Epoch: 4 | Batch : 400 | Generator Loss: 1.0463 | Discriminator Loss: 0.5377\n",
      "Epoch: 4 | Batch : 500 | Generator Loss: 1.5863 | Discriminator Loss: 0.2959\n",
      "Epoch: 4 | Batch : 600 | Generator Loss: 1.1849 | Discriminator Loss: 0.5260\n",
      "Epoch: 4 | Batch : 700 | Generator Loss: 1.0841 | Discriminator Loss: 0.5123\n",
      "Epoch: 4 | Batch : 800 | Generator Loss: 1.4294 | Discriminator Loss: 0.4264\n",
      "Epoch: 4 | Batch : 900 | Generator Loss: 0.9444 | Discriminator Loss: 0.5410\n",
      "Epoch : 4 | Total Trainig Loss : 0.8214937448501587\n",
      "Epoch: 5 | Batch : 0 | Generator Loss: 1.0628 | Discriminator Loss: 0.4988\n",
      "Epoch: 5 | Batch : 100 | Generator Loss: 1.1626 | Discriminator Loss: 0.4640\n",
      "Epoch: 5 | Batch : 200 | Generator Loss: 1.0823 | Discriminator Loss: 0.5041\n",
      "Epoch: 5 | Batch : 300 | Generator Loss: 0.9917 | Discriminator Loss: 0.5251\n",
      "Epoch: 5 | Batch : 400 | Generator Loss: 1.3145 | Discriminator Loss: 0.3599\n",
      "Epoch: 5 | Batch : 500 | Generator Loss: 1.2067 | Discriminator Loss: 0.3963\n",
      "Epoch: 5 | Batch : 600 | Generator Loss: 1.1453 | Discriminator Loss: 0.5590\n",
      "Epoch: 5 | Batch : 700 | Generator Loss: 1.4425 | Discriminator Loss: 0.3310\n",
      "Epoch: 5 | Batch : 800 | Generator Loss: 1.2197 | Discriminator Loss: 0.4441\n",
      "Epoch: 5 | Batch : 900 | Generator Loss: 1.1476 | Discriminator Loss: 0.4782\n",
      "Epoch : 5 | Total Trainig Loss : 0.8314064741134644\n",
      "Epoch: 6 | Batch : 0 | Generator Loss: 1.2661 | Discriminator Loss: 0.4746\n",
      "Epoch: 6 | Batch : 100 | Generator Loss: 1.2308 | Discriminator Loss: 0.5010\n",
      "Epoch: 6 | Batch : 200 | Generator Loss: 1.1404 | Discriminator Loss: 0.4977\n",
      "Epoch: 6 | Batch : 300 | Generator Loss: 0.9645 | Discriminator Loss: 0.5663\n",
      "Epoch: 6 | Batch : 400 | Generator Loss: 1.1895 | Discriminator Loss: 0.5204\n",
      "Epoch: 6 | Batch : 500 | Generator Loss: 1.5537 | Discriminator Loss: 0.2772\n",
      "Epoch: 6 | Batch : 600 | Generator Loss: 1.1807 | Discriminator Loss: 0.4836\n",
      "Epoch: 6 | Batch : 700 | Generator Loss: 1.0956 | Discriminator Loss: 0.4953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[409], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator_state, discriminator_parameters \u001b[38;5;241m=\u001b[39m train_model(generator_state, discriminator_state, train_data, test_data, \u001b[38;5;241m50\u001b[39m, key)\n",
      "Cell \u001b[0;32mIn[408], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(generator_state, discriminator_state, train_dataloader, test_dataloader, epochs, key)\u001b[0m\n\u001b[1;32m     14\u001b[0m key, key_generator \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m generator_state, discriminator_state, gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m train_step(\n\u001b[1;32m     16\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch,\n\u001b[1;32m     17\u001b[0m     generator_state \u001b[38;5;241m=\u001b[39m generator_state,\n\u001b[1;32m     18\u001b[0m     discriminator_state \u001b[38;5;241m=\u001b[39m discriminator_state,\n\u001b[1;32m     19\u001b[0m     key \u001b[38;5;241m=\u001b[39m key_generator\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (gen_loss \u001b[38;5;241m+\u001b[39m disc_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisc_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:265\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m binary_op(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator_state, discriminator_parameters = train_model(generator_state, discriminator_state, train_data, test_data, 50, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "001ffc9b-e03b-48d9-8536-b52225a53544",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = jax.random.normal(jax.random.key(0), shape = (BATCH_SIZE, LATENT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e5be76ea-7d52-4e57-aad4-8db28f037a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generator.apply(generator_parameters, x = noise, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "9c5c8e88-e595-434a-9545-9d4964a30d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc49ca33150>"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(jnp.clip((out[0] * 127.5) + 127.5, min = 0, max = 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f924e-73b7-4dbd-8f9d-a9a5cd039857",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tx.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
